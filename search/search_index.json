{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","tags":false},"docs":[{"location":"","text":"<p>Welcome to The Internals of Kafka Streams online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark, Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Kafka Streams as much as I have.</p>  <p>Flannery O'Connor</p> <p>I write to discover what I know.</p>  \"The Internals Of\" series<p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p>  <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Kafka Streams \ud83d\udd25</p>  <p>Last update: 2021-10-20</p>","title":"The Internals of Kafka Streams 3.0.0"},{"location":"AssignorConfiguration/","text":"","title":"AssignorConfiguration"},{"location":"AssignorConfiguration/#creating-instance","text":"<p><code>AssignorConfiguration</code> takes the following to be created:</p> <ul> <li> Configuration Properties  <p><code>AssignorConfiguration</code> is created\u00a0when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to configure</li> </ul>","title":"Creating Instance"},{"location":"AssignorConfiguration/#rebalanceprotocol","text":"","title":"rebalanceProtocol <pre><code>RebalanceProtocol rebalanceProtocol()\n</code></pre> <p><code>rebalanceProtocol</code> takes the value of upgrade.from configuration property (from the StreamsConfig).</p> <p>Unless <code>upgrade.from</code> is defined, <code>rebalanceProtocol</code> prints out the following INFO message to the logs and returns <code>RebalanceProtocol.COOPERATIVE</code>.</p> <pre><code>Cooperative rebalancing enabled now\n</code></pre> <p>With <code>upgrade.from</code> defined, <code>rebalanceProtocol</code>...FIXME</p> <p><code>rebalanceProtocol</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to configure</li> </ul>"},{"location":"AssignorConfiguration/#copartitionedtopicsenforcer","text":"","title":"CopartitionedTopicsEnforcer <pre><code>CopartitionedTopicsEnforcer copartitionedTopicsEnforcer()\n</code></pre> <p><code>copartitionedTopicsEnforcer</code> creates a new CopartitionedTopicsEnforcer (with the logPrefix).</p> <p><code>copartitionedTopicsEnforcer</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to configure</li> </ul>"},{"location":"AssignorConfiguration/#internaltopicmanager","text":"","title":"InternalTopicManager <pre><code>InternalTopicManager internalTopicManager()\n</code></pre> <p><code>internalTopicManager</code> creates a new InternalTopicManager.</p> <p><code>internalTopicManager</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to configure</li> </ul>"},{"location":"ChangelogTopics/","text":"","title":"ChangelogTopics"},{"location":"ChangelogTopics/#creating-instance","text":"<p><code>ChangelogTopics</code> takes the following to be created:</p> <ul> <li> InternalTopicManager <li> Topic Groups (<code>Map&lt;Subtopology, TopicsInfo&gt;</code>) <li> Tasks for Topic Groups (<code>Map&lt;Subtopology, Set&lt;TaskId&gt;&gt;</code>) <li> Log Prefix  <p><code>ChangelogTopics</code> is created\u00a0when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to assignTasksToClients</li> </ul>","title":"Creating Instance"},{"location":"CopartitionedTopicsEnforcer/","text":"","title":"CopartitionedTopicsEnforcer"},{"location":"CopartitionedTopicsEnforcer/#creating-instance","text":"<p><code>CopartitionedTopicsEnforcer</code> takes the following to be created:</p> <ul> <li> Log Prefix  <p><code>CopartitionedTopicsEnforcer</code> is created\u00a0when:</p> <ul> <li><code>AssignorConfiguration</code> is requested to copartitionedTopicsEnforcer</li> </ul>","title":"Creating Instance"},{"location":"InternalTopicManager/","text":"","title":"InternalTopicManager"},{"location":"InternalTopicManager/#creating-instance","text":"<p><code>InternalTopicManager</code> takes the following to be created:</p> <ul> <li> <code>Time</code> <li> <code>Admin</code> (Apache Kafka) <li> StreamsConfig  <p><code>InternalTopicManager</code> is created\u00a0when:</p> <ul> <li><code>AssignorConfiguration</code> is requested for an InternalTopicManager</li> </ul>","title":"Creating Instance"},{"location":"InternalTopicManager/#makeready","text":"","title":"makeReady <pre><code>Set&lt;String&gt; makeReady(\n  Map&lt;String, InternalTopicConfig&gt; topics)\n</code></pre> <p><code>makeReady</code>...FIXME</p> <p><code>makeReady</code>\u00a0is used when:</p> <ul> <li><code>ChangelogTopics</code> is requested to setup</li> <li><code>RepartitionTopics</code> is requested to setup</li> </ul>"},{"location":"InternalTopologyBuilder/","text":"","title":"InternalTopologyBuilder"},{"location":"InternalTopologyBuilder/#creating-instance","text":"<p><code>InternalTopologyBuilder</code> takes the following to be created:</p> <ul> <li> Topology Name  <p><code>InternalTopologyBuilder</code> is created\u00a0when:</p> <ul> <li><code>Topology</code> is created</li> </ul>","title":"Creating Instance"},{"location":"InternalTopologyBuilder/#node-groups","text":"","title":"Node Groups <pre><code>Map&lt;Integer, Set&lt;String&gt;&gt; nodeGroups\n</code></pre> <p><code>InternalTopologyBuilder</code> defines <code>nodeGroups</code> internal registry of subtopologies and an associated group of (source) topics.</p> <p><code>nodeGroups</code> is initially undefined (<code>null</code>) and is built on demand when undefined that happens after <code>InternalTopologyBuilder</code> is requested for the following:</p> <ul> <li>addSource</li> <li>addSink</li> <li>addProcessor</li> <li>addStateStore</li> <li>addGlobalStore</li> <li>connectProcessorAndStateStores</li> </ul> <p>Node groups are uniquely identified by node group ID (starting from <code>0</code>).</p> <p>Used when:</p> <ul> <li>Building a topology</li> <li>Building a sub-topology</li> <li>globalNodeGroups</li> <li>topicGroups</li> <li>Describing a topology</li> </ul>"},{"location":"InternalTopologyBuilder/#makenodegroups","text":"","title":"makeNodeGroups <pre><code>Map&lt;Integer, Set&lt;String&gt;&gt; makeNodeGroups()\n</code></pre> <p>For every node (in the nodeFactories registry) <code>makeNodeGroups</code> putNodeGroupName.</p> <p><code>makeNodeGroups</code> uses local mutable <code>nodeGroups</code> and <code>nodeGroupId</code> values that can be modified every putNodeGroupName.</p> <p>In the end, <code>makeNodeGroups</code> returns the <code>nodeGroups</code> local collection.</p>"},{"location":"InternalTopologyBuilder/#putnodegroupname","text":"","title":"putNodeGroupName <pre><code>int putNodeGroupName(\n  String nodeName,\n  int nodeGroupId,\n  Map&lt;Integer, Set&lt;String&gt;&gt; nodeGroups,\n  Map&lt;String, Set&lt;String&gt;&gt; rootToNodeGroup)\n</code></pre> <p><code>putNodeGroupName</code> requests the nodeGrouper for the name of the root node of the given <code>nodeName</code>.</p> <p><code>putNodeGroupName</code> looks up the name of the root node in the given <code>rootToNodeGroup</code>.</p> <p>If the node group is found (by the name of the root node), <code>putNodeGroupName</code> simply adds the given <code>nodeName</code> and returns the given <code>nodeGroupId</code> (unchanged).</p> <p>Otherwise, if the name of the root node is not among the available node groups (in the given <code>rootToNodeGroup</code>), <code>putNodeGroupName</code> adds the root name to the given <code>rootToNodeGroup</code> and <code>nodeGroups</code> (with an empty node group and a new node group ID).</p> <p>In the end, <code>putNodeGroupName</code> returns a new or the given node group ID (based on availability of the root node).</p>"},{"location":"InternalTopologyBuilder/#node-grouper","text":"","title":"Node Grouper <p><code>InternalTopologyBuilder</code> creates a node grouper (<code>QuickUnion&lt;String&gt;</code>) when created.</p> <p>The node grouper is requested to add a node name for the following:</p> <ul> <li>addSource</li> <li>addSink</li> <li>addProcessor</li> <li>addGlobalStore</li> </ul> <p>The node grouper is requested to unite names (of a node and predecessors) for the following:</p> <ul> <li>addSink</li> <li>addProcessor</li> <li>addGlobalStore</li> <li>connectProcessorAndStateStore</li> </ul> <p>In the end, the node grouper is requested for a root node in putNodeGroupName.</p>"},{"location":"InternalTopologyBuilder/#describing-topology","text":"","title":"Describing Topology <pre><code>TopologyDescription describe()\n</code></pre> <p><code>describe</code> creates a new <code>TopologyDescription</code> (that is going to be the returned value in the end).</p> <p>For every node group <code>describe</code> checks if the group contains a global (state) source.</p> <p>If so, <code>describe</code> describeGlobalStore. Otherwise, <code>describe</code> describeSubtopology.</p> <p><code>describe</code> is used when:</p> <ul> <li><code>Topology</code> is requested to describe</li> </ul>"},{"location":"InternalTopologyBuilder/#copartitionsourcegroups","text":"","title":"copartitionSourceGroups <pre><code>List&lt;Set&lt;String&gt;&gt; copartitionSourceGroups\n</code></pre> <p><code>InternalTopologyBuilder</code> defines <code>copartitionSourceGroups</code> internal registry for groups of source processors that need to be co-partitioned.</p> <p>A new entry is added when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to copartitionSources</li> </ul> <p>The registry is used when <code>InternalTopologyBuilder</code> is requested for the following:</p> <ul> <li>maybeUpdateCopartitionSourceGroups</li> <li>validateCopartition</li> <li>copartitionGroups</li> </ul>"},{"location":"InternalTopologyBuilder/#copartitionsources","text":"","title":"copartitionSources <pre><code>void copartitionSources(\n  Collection&lt;String&gt; sourceNodes)\n</code></pre> <p><code>copartitionSources</code> simply adds the given <code>sourceNodes</code> to the copartitionSourceGroups internal registry.</p> <p><code>copartitionSources</code>\u00a0is used when:</p> <ul> <li><code>AbstractStream</code> is requested to ensureCopartitionWith</li> <li><code>KTableImpl</code> is requested to doJoinOnForeignKey</li> </ul>"},{"location":"InternalTopologyBuilder/#internaltopicnameswithproperties","text":"","title":"internalTopicNamesWithProperties <pre><code>Map&lt;String, InternalTopicProperties&gt; internalTopicNamesWithProperties\n</code></pre> <p><code>InternalTopologyBuilder</code> defines <code>internalTopicNamesWithProperties</code> internal registry of all the internal topics with their corresponding properties.</p> <p>A new internal topic is added when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to addInternalTopic</li> </ul> <p>The registry is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to validateCopartition, buildSinkNode, buildSourceNode, topicGroups, maybeDecorateInternalSourceTopics</li> <li><code>SinkNodeFactory</code> is requested to build a processor node</li> </ul>"},{"location":"InternalTopologyBuilder/#addinternaltopic","text":"","title":"addInternalTopic <pre><code>void addInternalTopic(\n  String topicName,\n  InternalTopicProperties internalTopicProperties)\n</code></pre> <p><code>addInternalTopic</code>...FIXME</p> <p><code>addInternalTopic</code>\u00a0is used when:</p> <ul> <li><code>KTableImpl</code> is requested to doJoinOnForeignKey</li> <li><code>GroupedTableOperationRepartitionNode</code> is requested to writeToTopology</li> <li><code>OptimizableRepartitionNode</code> is requested to writeToTopology</li> <li><code>UnoptimizableRepartitionNode</code> is requested to writeToTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#validatecopartition","text":"","title":"validateCopartition <pre><code>void validateCopartition()\n</code></pre> <p><code>validateCopartition</code>...FIXME</p> <p><code>validateCopartition</code>\u00a0is used when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to buildAndOptimizeTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#global-topics","text":"","title":"Global Topics <pre><code>Set&lt;String&gt; globalTopics\n</code></pre> <p><code>InternalTopologyBuilder</code> tracks global topics (names) in a <code>globalTopics</code> internal registry.</p> <p>A new topic name is added in addGlobalStore.</p>"},{"location":"InternalTopologyBuilder/#building-processor-topology","text":"","title":"Building Processor Topology <pre><code>ProcessorTopology build(\n  Set&lt;String&gt; nodeGroup)\n</code></pre> <p>For every NodeFactory (in the nodeFactories internal registry), if the name of the factory is in the given node group if defined or simply all node factories go through, <code>build</code> does the following:</p> <ol> <li>Requests the <code>NodeFactory</code> to build a ProcessorNode (and registers it in a local registry of processors by name)</li> <li>For <code>ProcessorNodeFactory</code>s, buildProcessorNode</li> <li>For <code>SourceNodeFactory</code>s, buildSourceNode</li> <li>For <code>SinkNodeFactory</code>s, buildSinkNode</li> </ol> <p>In the end, <code>build</code> creates a new ProcessorTopology.</p> <p><code>build</code>\u00a0is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to build a topology, a subtopology and a global state topology</li> </ul>"},{"location":"InternalTopologyBuilder/#buildprocessornode","text":"","title":"buildProcessorNode <pre><code>void buildProcessorNode(\n  Map&lt;String, ProcessorNode&lt;?, ?, ?, ?&gt;&gt; processorMap,\n  Map&lt;String, StateStore&gt; stateStoreMap,\n  ProcessorNodeFactory&lt;?, ?, ?, ?&gt; factory,\n  ProcessorNode&lt;Object, Object, Object, Object&gt; node)\n</code></pre> <p><code>buildProcessorNode</code>...FIXME</p>"},{"location":"InternalTopologyBuilder/#building-source-node","text":"","title":"Building Source Node <pre><code>void buildSourceNode(\n  Map&lt;String, SourceNode&lt;?, ?&gt;&gt; topicSourceMap,\n  Set&lt;String&gt; repartitionTopics,\n  SourceNodeFactory&lt;?, ?&gt; sourceNodeFactory,\n  SourceNode&lt;?, ?&gt; node)\n</code></pre> <p><code>buildSourceNode</code> mutates (changes) the given <code>SourceNode</code> by topic name (<code>topicSourceMap</code>) and repartition topic names (<code>repartitionTopics</code>) collections.</p>  <p>When the pattern (of the given SourceNodeFactory) is defined, <code>buildSourceNode</code> subscriptionUpdates and requests the <code>SourceNodeFactory</code> to get the topics. Otherwise, <code>buildSourceNode</code> requests the <code>SourceNodeFactory</code> for the topics.</p> <p><code>buildSourceNode</code> adds the topic to the given <code>topicSourceMap</code> collection.</p> <p>For internal topics (in internalTopicNamesWithProperties registry), <code>buildSourceNode</code> decorates the name before adding to the given <code>topicSourceMap</code> collection and adds them to the given <code>repartitionTopics</code> collection.</p>"},{"location":"InternalTopologyBuilder/#buildsinknode","text":"","title":"buildSinkNode <pre><code>void buildSinkNode(\n  Map&lt;String, ProcessorNode&lt;?, ?, ?, ?&gt;&gt; processorMap,\n  Map&lt;String, SinkNode&lt;?, ?&gt;&gt; topicSinkMap,\n  Set&lt;String&gt; repartitionTopics,\n  SinkNodeFactory&lt;?, ?&gt; sinkNodeFactory,\n  SinkNode&lt;?, ?&gt; node)\n</code></pre> <p><code>buildSinkNode</code>...FIXME</p>"},{"location":"InternalTopologyBuilder/#building-local-processor-topology","text":"","title":"Building (Local) Processor Topology <pre><code>ProcessorTopology buildTopology()\n</code></pre> <p><code>buildTopology</code> initializes subscription and then builds a topology (of the node groups without the global node groups).</p> <p><code>buildTopology</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> <li><code>TopologyTestDriver</code> is requested to setupTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#building-processor-subtopology","text":"","title":"Building Processor SubTopology <pre><code>ProcessorTopology buildSubtopology(\n  int topicGroupId)\n</code></pre> <p><code>buildSubtopology</code> takes the <code>topicGroupId</code> node group (from the nodeGroups) and builds a topology.</p> <p><code>buildSubtopology</code>\u00a0is used when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createTasks and createActiveTaskFromStandby</li> <li><code>StandbyTaskCreator</code> is requested to createTasks and createStandbyTaskFromActive</li> </ul>"},{"location":"InternalTopologyBuilder/#building-global-state-processor-topology","text":"","title":"Building Global State Processor Topology <pre><code>ProcessorTopology buildGlobalStateTopology()\n</code></pre> <p><code>buildGlobalStateTopology</code> builds a topology of the global node groups if there are any.</p> <p><code>buildGlobalStateTopology</code> assumes that the applicationId has already been set or throws a <code>NullPointerException</code>:</p> <pre><code>topology has not completed optimization\n</code></pre> <p><code>buildGlobalStateTopology</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> <li><code>TopologyTestDriver</code> is requested to setupTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#rewriting-topology","text":"","title":"Rewriting Topology <pre><code>InternalTopologyBuilder rewriteTopology(\n  StreamsConfig config)\n</code></pre> <p><code>rewriteTopology</code> setApplicationId to the value of application.id configuration property.</p> <p>With cache.max.bytes.buffering enabled, <code>rewriteTopology</code>...FIXME</p> <p><code>rewriteTopology</code> requests the global StoreBuilders to build StateStores.</p> <p>In the end, <code>rewriteTopology</code> saves the StreamsConfig (and returns itself).</p> <p><code>rewriteTopology</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> <li><code>TopologyTestDriver</code> is requested to setupTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#globalnodegroups","text":"","title":"globalNodeGroups <pre><code>Set&lt;String&gt; globalNodeGroups()\n</code></pre> <p><code>globalNodeGroups</code> collects global source nodes from all the node groups.</p> <p><code>globalNodeGroups</code>\u00a0is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to build a local (excluding global state nodes) and global state topologies</li> </ul>"},{"location":"InternalTopologyBuilder/#isglobalsource","text":"","title":"isGlobalSource <pre><code>boolean isGlobalSource(\n  String nodeName)\n</code></pre> <p><code>isGlobalSource</code> finds a NodeFactory (by given <code>nodeName</code>) in nodeFactories registry.</p> <p><code>isGlobalSource</code> is positive (<code>true</code>) when the <code>NodeFactory</code> is a SourceNodeFactory with one topic only that is global. Otherwise, <code>isGlobalSource</code> is negative (<code>false</code>).</p> <p><code>isGlobalSource</code>\u00a0is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to globalNodeGroups, describeGlobalStore and nodeGroupContainsGlobalSourceNode</li> </ul>"},{"location":"InternalTopologyBuilder/#registering-global-store","text":"","title":"Registering Global Store <pre><code>&lt;KIn, VIn&gt; void addGlobalStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  String sourceName,\n  TimestampExtractor timestampExtractor,\n  Deserializer&lt;KIn&gt; keyDeserializer,\n  Deserializer&lt;VIn&gt; valueDeserializer,\n  String topic,\n  String processorName,\n  ProcessorSupplier&lt;KIn, VIn, Void, Void&gt; stateUpdateSupplier)\n</code></pre> <p><code>addGlobalStore</code>...FIXME</p> <p><code>addGlobalStore</code> is used when:</p> <ul> <li><code>Topology</code> is requested to addGlobalStore</li> <li><code>GlobalStoreNode</code> is requested to <code>writeToTopology</code></li> <li><code>TableSourceNode</code> is requested to <code>writeToTopology</code></li> </ul>"},{"location":"InternalTopologyBuilder/#registering-processor","text":"","title":"Registering Processor <pre><code>void addProcessor(\n  String name,\n  ProcessorSupplier&lt;KIn, VIn, KOut, VOut&gt; supplier,\n  String... predecessorNames)\n</code></pre> <p><code>addProcessor</code> creates a ProcessorNodeFactory (that is then added to nodeFactories registry).</p> <p><code>addProcessor</code> adds the name to nodeGrouper to unite the name with the given <code>predecessorNames</code>.</p> <p><code>addProcessor</code> is used when:</p> <ul> <li><code>Topology</code> is requested to addProcessor</li> <li>Some <code>GraphNode</code>s are requested to writeToTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#registering-statestore","text":"","title":"Registering StateStore <pre><code>void addStateStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  String... processorNames) // (1)\nvoid addStateStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  boolean allowOverride,\n  String... processorNames)\n</code></pre> <ol> <li>Uses <code>allowOverride</code> flag disabled (<code>false</code>)</li> </ol> <p><code>addStateStore</code>...FIXME</p> <p><code>addStateStore</code>\u00a0is used when:</p> <ul> <li><code>Topology</code> is requested to addProcessor and addStateStore</li> <li><code>KTableKTableJoinNode</code> is requested to <code>writeToTopology</code></li> <li><code>StatefulProcessorNode</code> is requested to <code>writeToTopology</code></li> <li><code>StateStoreNode</code> is requested to <code>writeToTopology</code></li> <li><code>StreamStreamJoinNode</code> is requested to <code>writeToTopology</code></li> <li><code>StreamToTableNode</code> is requested to <code>writeToTopology</code></li> <li><code>TableProcessorNode</code> is requested to <code>writeToTopology</code></li> <li><code>TableSourceNode</code> is requested to <code>writeToTopology</code></li> </ul>"},{"location":"InternalTopologyBuilder/#topicgroups","text":"","title":"topicGroups <pre><code>Map&lt;Subtopology, TopicsInfo&gt; topicGroups()\n</code></pre> <p><code>topicGroups</code>...FIXME</p> <p><code>topicGroups</code>\u00a0is used when:</p> <ul> <li><code>RepartitionTopics</code> is requested to setup</li> <li><code>StreamsPartitionAssignor</code> is requested for consumer group assignment</li> </ul>"},{"location":"InternalTopologyBuilder/#addsource","text":"","title":"addSource <pre><code>void addSource(\n  Topology.AutoOffsetReset offsetReset,\n  String name,\n  TimestampExtractor timestampExtractor,\n  Deserializer&lt;?&gt; keyDeserializer,\n  Deserializer&lt;?&gt; valDeserializer,\n  Pattern topicPattern)\nvoid addSource(\n  Topology.AutoOffsetReset offsetReset,\n  String name,\n  TimestampExtractor timestampExtractor,\n  Deserializer&lt;?&gt; keyDeserializer,\n  Deserializer&lt;?&gt; valDeserializer,\n  String... topics)\n</code></pre> <p><code>addSource</code> adds the topics to the sourceTopicNames internal registry.</p> <p><code>addSource</code> creates a new SourceNodeFactory and adds the factory to the nodeFactories registry (under the given <code>name</code>).</p> <p><code>addSource</code> adds the given <code>name</code> and the <code>topics</code> to the nodeToSourceTopics registry.</p> <p><code>addSource</code> adds the given <code>name</code> to nodeGrouper and clears out the nodeGroups (so it has to be rebuilt next time it is requested).</p> <p><code>addSource</code>\u00a0is used when:</p> <ul> <li><code>GroupedTableOperationRepartitionNode</code> is requested to <code>writeToTopology</code></li> <li><code>OptimizableRepartitionNode</code> is requested to <code>writeToTopology</code></li> <li><code>StreamSourceNode</code> is requested to writeToTopology</li> <li><code>TableSourceNode</code> is requested to <code>writeToTopology</code></li> <li><code>Topology</code> is requested to addSource</li> <li><code>UnoptimizableRepartitionNode</code> is requested to <code>writeToTopology</code></li> </ul>"},{"location":"KafkaClientSupplier/","text":"<p><code>KafkaClientSupplier</code> is...FIXME</p>","title":"KafkaClientSupplier"},{"location":"KafkaStreams/","text":"<p><code>KafkaStreams</code> is the execution environment of a Kafka Streams application.</p> <p><code>KafkaStreams</code> is a Kafka client for continuous stream processing (on input coming from one or more input topics and sending output to zero, one, or more output topics).</p>","title":"KafkaStreams"},{"location":"KafkaStreams/#creating-instance","text":"<p><code>KafkaStreams</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder (or Topology) <li> StreamsConfig <li> KafkaClientSupplier (default: <code>DefaultKafkaClientSupplier</code>) <li> <code>Time</code>  <p>When created, <code>KafkaStreams</code> requests the given InternalTopologyBuilder to rewriteTopology followed by building a task and global task topologies.</p> <p><code>KafkaStreams</code> then...FIXME</p>","title":"Creating Instance"},{"location":"KafkaStreams/#defaultstreamsuncaughtexceptionhandler","text":"","title":"defaultStreamsUncaughtExceptionHandler <pre><code>void defaultStreamsUncaughtExceptionHandler(\n  Throwable throwable)\n</code></pre> <p><code>defaultStreamsUncaughtExceptionHandler</code>...FIXME</p>"},{"location":"KafkaStreams/#task-topology","text":"","title":"Task Topology <p><code>KafkaStreams</code> requests the InternalTopologyBuilder to build a task topology when created.</p> <p>The ProcessorTopology can have persistent local stores.</p>"},{"location":"KafkaStreams/#global-task-topology","text":"","title":"Global Task Topology <p>When created <code>KafkaStreams</code> requests the InternalTopologyBuilder to build a global task topology.</p>"},{"location":"KafkaStreams/#streamthreads","text":"","title":"StreamThreads <p><code>KafkaStreams</code> manages StreamThreads in a <code>threads</code> internal registry.</p> <p>The <code>threads</code> collection starts empty when <code>KafkaStreams</code> is created.</p> <p><code>KafkaStreams</code> adds a new <code>StreamThread</code> when requested to createAndAddStreamThread.</p> <p>A <code>StreamThread</code> is removed when <code>KafkaStreams</code> is requested for the following:</p> <ul> <li>defaultStreamsUncaughtExceptionHandler</li> <li>addStreamThread</li> <li>removeStreamThread</li> <li>getNumLiveStreamThreads</li> <li>getNextThreadIndex</li> </ul> <p><code>KafkaStreams</code> uses processStreamThread to work with the <code>StreamThread</code>s.</p>"},{"location":"KafkaStreams/#processstreamthread","text":"","title":"processStreamThread <pre><code>void processStreamThread(\n  java.util.function.Consumer&lt;StreamThread&gt; consumer)\n</code></pre> <p><code>processStreamThread</code>...FIXME</p>"},{"location":"KafkaStreams/#getnumlivestreamthreads","text":"","title":"getNumLiveStreamThreads <pre><code>int getNumLiveStreamThreads()\n</code></pre> <p><code>getNumLiveStreamThreads</code>...FIXME</p>"},{"location":"KafkaStreams/#globalstreamthread","text":"","title":"GlobalStreamThread <p><code>KafkaStreams</code> can use a GlobalStreamThread if...FIXME</p>"},{"location":"KafkaStreams/#starting-streams-client","text":"","title":"Starting Streams Client <pre><code>void start()\n</code></pre> <p><code>start</code> attempts to enter <code>REBALANCING</code> state and, if successful, prints out the following INFO message to the logs:</p> <pre><code>State transition from [oldState] to REBALANCING\n</code></pre> <p><code>start</code> prints out the following DEBUG message to the logs:</p> <pre><code>Starting Streams client\n</code></pre> <p><code>start</code> requests the GlobalStreamThread to start (if defined).</p> <p><code>start</code> requests all the StreamThreads to start.</p> <p><code>start</code>...FIXME</p>"},{"location":"KafkaStreams/#setuncaughtexceptionhandler","text":"","title":"setUncaughtExceptionHandler <pre><code>void setUncaughtExceptionHandler(\n  StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler)\n</code></pre> <p><code>setUncaughtExceptionHandler</code>...FIXME</p> <p><code>setUncaughtExceptionHandler</code>\u00a0is part of the public API.</p>"},{"location":"KafkaStreams/#handlestreamsuncaughtexception","text":"","title":"handleStreamsUncaughtException <pre><code>void handleStreamsUncaughtException(\n  Throwable throwable,\n  StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler)\n</code></pre> <p><code>handleStreamsUncaughtException</code>...FIXME</p> <p><code>handleStreamsUncaughtException</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is requested to setUncaughtExceptionHandler and defaultStreamsUncaughtExceptionHandler</li> </ul>"},{"location":"KafkaStreams/#replacestreamthread","text":"","title":"replaceStreamThread <pre><code>void replaceStreamThread(\n  Throwable throwable)\n</code></pre> <p><code>replaceStreamThread</code>...FIXME</p>"},{"location":"KafkaStreams/#addstreamthread","text":"","title":"addStreamThread <pre><code>Optional&lt;String&gt; addStreamThread()\n</code></pre> <p><code>addStreamThread</code>...FIXME</p> <p><code>addStreamThread</code> is part of the public API.</p>"},{"location":"KafkaStreams/#createandaddstreamthread","text":"","title":"createAndAddStreamThread <pre><code>StreamThread createAndAddStreamThread(\n  long cacheSizePerThread,\n  int threadIdx)\n</code></pre> <p><code>createAndAddStreamThread</code> creates a StreamThread and requests it to setStateListener with the StreamStateListener.</p> <p><code>createAndAddStreamThread</code> registers the <code>StreamThread</code> (in the threads and threadState internal registries).</p> <p><code>createAndAddStreamThread</code> requests the QueryableStoreProvider to addStoreProviderForThread (with the name of the <code>StreamThread</code> and a new <code>StreamThreadStateStoreProvider</code>).</p> <p><code>createAndAddStreamThread</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created and requested to addStreamThread</li> </ul>"},{"location":"KafkaStreams/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.KafkaStreams</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.KafkaStreams=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"PartitionGrouper/","text":"","title":"PartitionGrouper"},{"location":"PartitionGrouper/#creating-instance","text":"<p><code>PartitionGrouper</code> takes no arguments to be created.</p> <p><code>PartitionGrouper</code> is created\u00a0when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to configure</li> </ul>","title":"Creating Instance"},{"location":"PartitionGrouper/#partitiongroups","text":"","title":"partitionGroups <pre><code>Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; partitionGroups(\n  Map&lt;Subtopology, Set&lt;String&gt;&gt; topicGroups, \n  Cluster metadata)\n</code></pre> <p><code>partitionGroups</code>...FIXME</p> <p><code>partitionGroups</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to assign</li> </ul>"},{"location":"PartitionGrouper/#maximum-number-of-partitions","text":"","title":"Maximum Number of Partitions <pre><code>int maxNumPartitions(\n  Cluster metadata, \n  Set&lt;String&gt; topics)\n</code></pre> <p><code>maxNumPartitions</code> finds the maximum number of partitions across all the given <code>topics</code> (using <code>Cluster</code> metadata).</p>"},{"location":"RepartitionTopics/","text":"<p><code>RepartitionTopics</code> is a helper class of StreamsPartitionAssignor (to prepare repartition topics).</p>","title":"RepartitionTopics"},{"location":"RepartitionTopics/#creating-instance","text":"<p><code>RepartitionTopics</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder <li> InternalTopicManager <li> CopartitionedTopicsEnforcer <li> <code>Cluster</code> metadata <li> Log prefix","title":"Creating Instance"},{"location":"RepartitionTopics/#topicpartitioninfos","text":"","title":"topicPartitionInfos <pre><code>Map&lt;TopicPartition, PartitionInfo&gt; topicPartitionInfos\n</code></pre> <p><code>RepartitionTopics</code> defines a <code>topicPartitionInfos</code> internal registry of <code>TopicPartition</code>s and the associated <code>PartitionInfo</code>.</p> <p><code>topicPartitionInfos</code> is initially empty and filled up when requested to setup.</p>"},{"location":"RepartitionTopics/#setup","text":"","title":"setup <pre><code>void setup()\n</code></pre> <p><code>setup</code> requests the InternalTopologyBuilder for the topic groups (that gives TopicsInfos by <code>Subtopology</code>).</p> <p><code>setup</code> computeRepartitionTopicConfig for the topic groups and the cluster metadata (that gives a <code>Map&lt;String, InternalTopicConfig&gt;</code>).</p> <p><code>setup</code> ensureCopartitioning of the copartitionGroups (from InternalTopologyBuilder).</p> <p><code>setup</code> requests the InternalTopicManager to make the repartition source topics ready (exist and have proper number of partitions, creating if necessary).</p>"},{"location":"RepartitionTopics/#computerepartitiontopicconfig","text":"","title":"computeRepartitionTopicConfig <pre><code>Map&lt;String, InternalTopicConfig&gt; computeRepartitionTopicConfig(\n  Map&lt;Subtopology, TopicsInfo&gt; topicGroups,\n  Cluster clusterMetadata)\n</code></pre> <p><code>computeRepartitionTopicConfig</code>...FIXME</p>"},{"location":"RepartitionTopics/#ensurecopartitioning","text":"","title":"ensureCopartitioning <pre><code>void ensureCopartitioning(\n  Collection&lt;Set&lt;String&gt;&gt; copartitionGroups,\n  Map&lt;String, InternalTopicConfig&gt; repartitionTopicMetadata,\n  Cluster clusterMetadata)\n</code></pre> <p><code>ensureCopartitioning</code>...FIXME</p>"},{"location":"StreamsConfig/","text":"","title":"StreamsConfig"},{"location":"StreamsConfig/#applicationid","text":"","title":"application.id"},{"location":"StreamsConfig/#cachemaxbytesbuffering","text":"","title":"cache.max.bytes.buffering"},{"location":"StreamsConfig/#commitintervalms","text":"","title":"commit.interval.ms <p>How often to save (commit and flush) the position of a processor</p> <p>Default: <code>30000L</code> (or <code>100L</code> for processing.guarantee being <code>exactly_once_v2</code> or deprecated <code>exactly_once</code>)</p> <p>Must be at least <code>0</code></p> <p><code>commit.interval.ms</code> has to be lower than <code>transaction.timeout.ms</code> (or ongoing transaction always time out due to inactivity caused by long commit interval)</p> <p>Used when:</p> <ul> <li><code>GlobalStreamThread</code> is requested to initialize (and create a <code>StateConsumer</code>)</li> <li><code>StoreChangelogReader</code> is created</li> <li><code>StreamThread</code> is created</li> </ul>"},{"location":"StreamsConfig/#pollms","text":"","title":"poll.ms <p>Time (in millis) to block waiting for input</p> <p>Default: <code>100L</code></p> <p>Used when:</p> <ul> <li><code>GlobalStateManagerImpl</code> is created</li> <li><code>GlobalStreamThread</code> is requested to initialize</li> <li><code>StoreChangelogReader</code> is created</li> <li><code>StreamThread</code> is created</li> </ul>"},{"location":"StreamsConfig/#tasktimeoutms","text":"","title":"task.timeout.ms"},{"location":"StreamsConfig/#upgradefrom","text":"","title":"upgrade.from <p>Default: (undefined)</p>"},{"location":"StreamsPartitionAssignor/","text":"<p><code>StreamsPartitionAssignor</code> is a <code>ConsumerPartitionAssignor</code> (Apache Kafka) and a <code>Configurable</code> (Apache Kafka).</p>","title":"StreamsPartitionAssignor"},{"location":"StreamsPartitionAssignor/#supported-rebalance-protocols","text":"","title":"Supported Rebalance Protocols <pre><code>List&lt;RebalanceProtocol&gt; supportedProtocols()\n</code></pre> <p><code>supportedProtocols</code> returns the following <code>RebalanceProtocol</code>s:</p> <ol> <li><code>RebalanceProtocol.EAGER</code></li> <li><code>RebalanceProtocol.COOPERATIVE</code> (based on upgrade.from)</li> </ol> <p><code>supportedProtocols</code>\u00a0is part of the <code>ConsumerPartitionAssignor</code> (Apache Kafka) abstraction.</p>"},{"location":"StreamsPartitionAssignor/#name","text":"","title":"Name <pre><code>String name()\n</code></pre> <p><code>name</code> is stream.</p> <p><code>name</code>\u00a0is part of the <code>ConsumerPartitionAssignor</code> (Apache Kafka) abstraction.</p>"},{"location":"StreamsPartitionAssignor/#configure","text":"","title":"configure <pre><code>void configure(\n  Map&lt;String, ?&gt; configs)\n</code></pre> <p><code>configure</code> creates a new AssignorConfiguration (with the given <code>configs</code>).</p> <p><code>configure</code>...FIXME</p> <p><code>configure</code>\u00a0is part of the <code>Configurable</code> (Apache Kafka) abstraction.</p>"},{"location":"StreamsPartitionAssignor/#consumer-group-assignment","text":"","title":"Consumer Group Assignment <pre><code>GroupAssignment assign(\n  Cluster metadata,\n  GroupSubscription groupSubscription)\n</code></pre> <p><code>assign</code>...FIXME</p> <p><code>assign</code> prints out the following DEBUG message to the logs:</p> <pre><code>Constructed client metadata [clientMetadata] from the member subscriptions.\n</code></pre> <p><code>assign</code> prepareRepartitionTopics with the given cluster metadata (that gives a <code>Map&lt;TopicPartition, PartitionInfo&gt;</code> as <code>allRepartitionTopicPartitions</code>).</p> <p><code>assign</code> prints out the following DEBUG message to the logs:</p> <pre><code>Created repartition topics [allRepartitionTopicPartitions] from the parsed topology.\n</code></pre> <p><code>assign</code>...FIXME</p> <p><code>assign</code>\u00a0is part of the <code>ConsumerPartitionAssignor</code> (Apache Kafka) abstraction.</p>"},{"location":"StreamsPartitionAssignor/#preparerepartitiontopics","text":"","title":"prepareRepartitionTopics <pre><code>Map&lt;TopicPartition, PartitionInfo&gt; prepareRepartitionTopics(\n  Cluster metadata)\n</code></pre> <p><code>prepareRepartitionTopics</code> creates a new RepartitionTopics that is requested to setup and then for the topicPartitionsInfo.</p>"},{"location":"StreamsPartitionAssignor/#assigntaskstoclients","text":"","title":"assignTasksToClients <pre><code>boolean assignTasksToClients(\n  Cluster fullMetadata,\n  Set&lt;String&gt; allSourceTopics,\n  Map&lt;Subtopology, TopicsInfo&gt; topicGroups,\n  Map&lt;UUID, ClientMetadata&gt; clientMetadataMap,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; partitionsForTask,\n  Set&lt;TaskId&gt; statefulTasks)\n</code></pre> <p><code>assignTasksToClients</code>...FIXME</p>"},{"location":"StreamsPartitionAssignor/#onassignment","text":"","title":"onAssignment <pre><code>void onAssignment(\n  Assignment assignment,\n  ConsumerGroupMetadata metadata)\n</code></pre> <p><code>onAssignment</code>...FIXME</p> <p><code>onAssignment</code>\u00a0is part of the <code>ConsumerPartitionAssignor</code> (Apache Kafka) abstraction.</p>"},{"location":"StreamsPartitionAssignor/#subscriptionuserdata","text":"","title":"subscriptionUserData <pre><code>ByteBuffer subscriptionUserData(\n  Set&lt;String&gt; topics)\n</code></pre> <p><code>subscriptionUserData</code>...FIXME</p> <p><code>subscriptionUserData</code>\u00a0is part of the <code>ConsumerPartitionAssignor</code> (Apache Kafka) abstraction.</p>"},{"location":"StreamsPartitionAssignor/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"TopicsInfo/","text":"","title":"TopicsInfo"},{"location":"TopicsInfo/#creating-instance","text":"<p><code>TopicsInfo</code> takes the following to be created:</p> <ul> <li> Names of the Sink Topics <li> Names of the Source Topics <li> Repartition Source Topics (<code>Map&lt;String, InternalTopicConfig&gt;</code>) <li> State Changelog Topics (<code>Map&lt;String, InternalTopicConfig&gt;</code>)  <p><code>TopicsInfo</code> is created\u00a0when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested for topic groups</li> </ul>","title":"Creating Instance"},{"location":"Topology/","text":"<p><code>Topology</code> is a logical representation of a ProcessorTopology.</p> <p><code>Topology</code> is a facade to InternalTopologyBuilder (with all methods delegating to it).</p>","title":"Topology"},{"location":"Topology/#creating-instance","text":"<p><code>Topology</code> takes no arguments to be created.</p> <p><code>Topology</code> is a part of the public API of Kafka Streams and can be created directly or indirectly for StreamsBuilder.</p>","title":"Creating Instance"},{"location":"Topology/#internaltopologybuilder","text":"","title":"InternalTopologyBuilder <p><code>Topology</code> creates an InternalTopologyBuilder when created.</p>"},{"location":"Topology/#addglobalstore","text":"","title":"addGlobalStore <pre><code>&lt;KIn, VIn&gt; Topology addGlobalStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  String sourceName,\n  Deserializer&lt;KIn&gt; keyDeserializer,\n  Deserializer&lt;VIn&gt; valueDeserializer,\n  String topic,\n  String processorName,\n  ProcessorSupplier&lt;KIn, VIn, Void, Void&gt; stateUpdateSupplier) // (1)\n&lt;KIn, VIn&gt; Topology addGlobalStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  String sourceName,\n  TimestampExtractor timestampExtractor,\n  Deserializer&lt;KIn&gt; keyDeserializer,\n  Deserializer&lt;VIn&gt; valueDeserializer,\n  String topic,\n  String processorName,\n  ProcessorSupplier&lt;KIn, VIn, Void, Void&gt; stateUpdateSupplier)\n</code></pre> <ol> <li>Uses no TimestampExtractor</li> </ol> <p><code>addGlobalStore</code> requests the InternalTopologyBuilder to add a global store.</p>"},{"location":"Topology/#addprocessor","text":"","title":"addProcessor <pre><code>Topology addProcessor(\n  String name,\n  ProcessorSupplier&lt;KIn, VIn, KOut, VOut&gt; supplier,\n  String... parentNames)\n</code></pre> <p><code>addProcessor</code> requests the InternalTopologyBuilder to add a processor.</p> <p>If there are any state stores associated with the processor, <code>addProcessor</code> requests the InternalTopologyBuilder to add them.</p>"},{"location":"Topology/#addsource","text":"","title":"addSource <pre><code>Topology addSource(...) // (1)\n</code></pre> <ol> <li>There are over 10 different <code>addSource</code>s</li> </ol> <p><code>addSource</code> requests the InternalTopologyBuilder to add a new source (node) (with the given arguments).</p>"},{"location":"Topology/#demo","text":"","title":"Demo <pre><code>import org.apache.kafka.streams.Topology\nval topology = new Topology\n</code></pre>"},{"location":"TopologyTestDriver/","text":"<p><code>TopologyTestDriver</code> helps writing tests to verify behavior of topologies (created with Topology or StreamsBuilder).</p> <pre><code>import org.apache.kafka.streams.TopologyTestDriver\n</code></pre>","title":"TopologyTestDriver"},{"location":"TopologyTestDriver/#library-dependency","text":"","title":"Library Dependency <p><code>TopologyTestDriver</code> belongs to a separate module and has to be defined as a dependency in a build configuration (e.g. <code>build.sbt</code>).</p> <pre><code>val kafkaVersion = \"3.0.0\"\nlibraryDependencies += \"org.apache.kafka\" % \"kafka-streams-test-utils\" % kafkaVersion % Test\n</code></pre>"},{"location":"global-stores/","text":"<p>StreamsBuilder.addGlobalStore adds a global StateStore to a topology.</p> <p>Such a <code>StateStore</code> sources its data from all partitions of the provided input topic. This store uses the source topic as changelog (and during restore will insert records directly from the source).</p> <p>Global stores should not be added to <code>Processor</code>, <code>Transformer</code>, or <code>ValueTransformer</code> (unlike regular stores). They have read-only access to all global stores by default.</p> <p>There will be exactly one instance of this <code>StateStore</code> per Kafka Streams instance.</p> <p>A SourceNode will be added to consume the data arriving from the partitions of the input topic.</p>","title":"Global Stores"},{"location":"logging/","text":"<p>Kafka Streams uses Simple Logging Facade for Java (SLF4J) for logging.</p> <p>Among the logging frameworks supported by slf4j is Apache Log4j that is used by Apache Kafka by default.</p>","title":"Logging"},{"location":"logging/#library-dependencies","text":"<p>Use <code>slf4j-api</code> and <code>slf4j-log4j12</code> library dependencies in a Kafka Streams application (in <code>build.sbt</code>) for logging.</p> <pre><code>val slf4jVersion = \"2.0.0-alpha5\"\nlibraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4jVersion\nlibraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4jVersion\n</code></pre>","title":"Library Dependencies"},{"location":"logging/#log4jproperties","text":"","title":"log4j.properties <p>Use the following <code>log4j.properties</code> in <code>src/main/resources</code> in your Kafka Streams application's project.</p> <pre><code>log4j.rootLogger=INFO, stdout\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.target=System.out\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.logger.org.apache.kafka.streams.processor.internals.StreamThread=ALL\n</code></pre>"},{"location":"overview/","text":"<p>Kafka Streams is a library for developing applications for processing records from topics in Apache Kafka.</p> <p>Kafka Streams comes with high-level Streams DSL and low-level Processor API to describe a Topology that eventually is built as a ProcessorTopology.</p> <p>The execution environment of <code>ProcessorTopology</code> is KafkaStreams. Once created, the <code>KafkaStreams</code> instance is supposed to be started to start stream processing.</p>","title":"Kafka Streams\u2009\u2014\u2009Stream Processing Library on Apache Kafka"},{"location":"scala/","text":"<p>Scala API for Kafka Streams is a separate Kafka Streams module (a Scala library) that acts as a wrapper over the existing Java API for Kafka Streams.</p> <p>The Scala API for Kafka Streams defines Scala-friendly types that wrap the corresponding Kafka Streams types and simply delegate all method calls to the underlying Java object with the purpose of making it much more expressive, with less boilerplate and more succinct.</p>","title":"Scala API for Kafka Streams"},{"location":"scala/#scala-package","text":"","title":"scala Package <p>The Scala API is available in the <code>org.apache.kafka.streams.scala</code> package.</p> <pre><code>import org.apache.kafka.streams.scala._\nimport org.apache.kafka.streams.scala.kstream._\n</code></pre>"},{"location":"scala/#library-dependency","text":"","title":"Library Dependency <p>As a separate Scala library Scala API for Kafka Streams has to be defined as a dependency in a build configuration (e.g. <code>build.sbt</code>).</p> <pre><code>val kafkaVersion = \"3.0.0\"\nlibraryDependencies += \"org.apache.kafka\" %% \"kafka-streams-scala\" % kafkaVersion\n</code></pre>"},{"location":"scala/#implicit-conversions","text":"","title":"Implicit Conversions <p>The Scala API for Kafka Streams defines implicit conversions, i.e. <code>Serdes</code>, and <code>ImplicitConversions</code>.</p> <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n</code></pre>"},{"location":"scala/#consumed","text":"","title":"Consumed <p>The Scala API for Kafka Streams comes with Scala objects for creating Consumed, Produced, <code>Materialized</code> and other metadata-related instances with Serdes objects for the key and value types available in implicit scope.</p> <pre><code>import org.apache.kafka.streams.scala.kstream._\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/","text":"","title":"Demo: Developing Kafka Streams Application"},{"location":"demo/developing-kafka-streams-application/#build-topology-using-streamsbuilder","text":"","title":"Build Topology using StreamsBuilder <p>A Kafka Streams application requires a Topology that can be created directly or described (and built) indirectly using StreamsBuilder.</p> <pre><code>import org.apache.kafka.streams.scala.StreamsBuilder\nval streamBuilder = new StreamsBuilder\n</code></pre> <pre><code>import org.apache.kafka.streams.scala.ImplicitConversions._\nimport org.apache.kafka.streams.scala.serialization.Serdes._\n</code></pre> <pre><code>val records = streamBuilder.stream[String, String](topic = \"streams-demo-input\")\nrecords.to(topic = \"streams-demo-output\")\n</code></pre> <pre><code>import org.apache.kafka.streams.Topology\nval topology = streamBuilder.build()\n</code></pre> <p>A topology can be described.</p> <pre><code>println(topology.describe)\n</code></pre> <pre><code>Topologies:\n   Sub-topology: 0\n    Source: KSTREAM-SOURCE-0000000000 (topics: [streams-demo-input])\n      --&gt; KSTREAM-SINK-0000000001\n    Sink: KSTREAM-SINK-0000000001 (topic: streams-demo-output)\n      &lt;-- KSTREAM-SOURCE-0000000000\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#create-kafka-topics","text":"","title":"Create Kafka Topics <p>Kafka Streams requires that all input topics are available before it can be started (or <code>MissingSourceTopicException</code> is thrown).</p> <pre><code>./bin/kafka-topics.sh \\\n  --bootstrap-server :9092 \\\n  --create \\\n  --topic streams-demo-input \\\n  --partitions 1 \\\n  --replication-factor 1\n</code></pre> <pre><code>./bin/kafka-topics.sh \\\n  --bootstrap-server :9092 \\\n  --create \\\n  --topic streams-demo-output \\\n  --partitions 1 \\\n  --replication-factor 1\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#streamsconfig","text":"","title":"StreamsConfig <p>An execution environment of a Kafka Streams application is configured using StreamsConfig.</p> <pre><code>import org.apache.kafka.streams.StreamsConfig\nimport scala.jdk.CollectionConverters._\n// Only required configuration properties\n// And one more for demo purposes to slow processing to 15 secs\n// import java.util.concurrent.TimeUnit\nimport scala.concurrent.duration._\nval props = Map(\n  StreamsConfig.APPLICATION_ID_CONFIG -&gt; \"kafka-streams-demo\",\n  StreamsConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; \":9092\",\n  StreamsConfig.POLL_MS_CONFIG -&gt; 15.seconds.toMillis).asJava\nval config = new StreamsConfig(props)\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#kafkastreams","text":"","title":"KafkaStreams <p>The execution environment of a Kafka Stream application is KafkaStreams.</p> <pre><code>import org.apache.kafka.streams.KafkaStreams\nval streams = new KafkaStreams(topology, config)\n</code></pre> <p>Eventually, <code>KafkaStreams</code> should be started for the stream processing to be executed.</p> <pre><code>streams.start\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#kcat","text":"","title":"kcat <pre><code>kcat -P -b localhost -t streams-demo-input\n</code></pre> <pre><code>kcat -C -b localhost -t streams-demo-output\n</code></pre>"},{"location":"kstream/","text":"<p>Streams DSL (KStream DSL) is a high-level API for developers to define topologies in Kafka Streams.</p> <p>The entry point to the KStream DSL is StreamsBuilder.</p> <p>Main abstractions (for Kafka Streams developers):</p> <ul> <li>Consumed</li> <li>GlobalKTable</li> <li>KStream</li> <li>KTable</li> <li>Materialized</li> <li>Produced</li> <li>others</li> </ul> <p>A typical Kafka Streams application (that uses this Streams DSL and Scala API for Kafka Streams) looks as follows:</p> <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nval builder = new StreamsBuilder\n\n// Add a KStream if needed\n// K and V are the types of keys and values, accordingly\nbuilder.stream[K, V](...)\n\n// Add a KTable if needed\nbuilder.table[K, V](...)\n\n// Add a global store if needed\nbuilder.addGlobalStore(...)\n\n// Add a global store if needed\nbuilder.globalTable[K, V](...)\n\n// In the end, build a topology\nval topology = builder.build\n</code></pre>","title":"Streams DSL"},{"location":"kstream/AbstractStream/","text":"","title":"AbstractStream"},{"location":"kstream/AbstractStream/#ensurecopartitionwith","text":"","title":"ensureCopartitionWith <pre><code>Set&lt;String&gt; ensureCopartitionWith(\n  Collection&lt;? extends AbstractStream&lt;K, ?&gt;&gt; otherStreams)\n</code></pre> <p><code>ensureCopartitionWith</code>...FIXME</p> <p><code>ensureCopartitionWith</code>\u00a0is used when:</p> <ul> <li><code>CogroupedStreamAggregateBuilder</code> is requested to <code>processRepartitions</code></li> <li><code>KStreamImpl</code> is requested to doJoin and doStreamTableJoin</li> <li><code>KTableImpl</code> is requested to doJoin</li> </ul>"},{"location":"kstream/Aggregator/","text":"<p><code>Aggregator</code> is...FIXME</p>","title":"Aggregator"},{"location":"kstream/Consumed/","text":"<p><code>Consumed&lt;K, V&gt;</code> describes how to consume records in a topology in the High-Level KStream DSL for the following StreamsBuilder operators:</p> <ul> <li>StreamsBuilder.stream</li> <li>StreamsBuilder.table</li> <li>StreamsBuilder.globalTable</li> <li>StreamsBuilder.addGlobalStore</li> </ul> <p><code>Consumed&lt;K, V&gt;</code> is a NamedOperation.</p>","title":"Consumed \u2014 Metadata for Consuming Records"},{"location":"kstream/Consumed/#demo","text":"<pre><code>import org.apache.kafka.common.serialization.Serdes\nimport org.apache.kafka.streams.kstream.Consumed\nval consumed = Consumed.`with`(Serdes.Long, Serdes.String)\n</code></pre> <pre><code>scala&gt; :type consumed\norg.apache.kafka.streams.kstream.Consumed[Long,String]\n</code></pre>","title":"Demo"},{"location":"kstream/Consumed/#creating-instance","text":"<p><code>Consumed</code> takes the following to be created:</p> <ul> <li> <code>Serde&lt;K&gt;</code> of keys (Apache Kafka) <li> <code>Serde&lt;V&gt;</code> of values (Apache Kafka) <li> <code>TimestampExtractor</code> <li> Reset Policy (<code>Topology.AutoOffsetReset</code>) <li> Processor Name  <p><code>Consumed</code> is created\u00a0using the factories.</p>","title":"Creating Instance"},{"location":"kstream/Consumed/#creating-consumed","text":"","title":"Creating Consumed"},{"location":"kstream/Consumed/#as","text":"","title":"as <pre><code>Consumed&lt;K, V&gt; as(\n  String processorName)\n</code></pre>"},{"location":"kstream/Consumed/#with","text":"","title":"with <pre><code>Consumed&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde)\nConsumed&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde,\n  TimestampExtractor timestampExtractor,\n  Topology.AutoOffsetReset resetPolicy)\nConsumed&lt;K, V&gt; with(\n  TimestampExtractor timestampExtractor)\nConsumed&lt;K, V&gt; with(\n  Topology.AutoOffsetReset resetPolicy)\n</code></pre>"},{"location":"kstream/Consumed/#scala-api","text":"","title":"Scala API <p>Scala API for Kafka Streams makes the optional <code>Consumed</code> metadata an implicit parameter in the StreamsBuilder API.</p> <p>Moreover, <code>ImplicitConversions</code> object defines <code>consumedFromSerde</code> implicit method that creates a <code>Consumed</code> instance with the key and value <code>Serde</code> objects available in implicit scope.</p> <p>And the last but not least, Scala API for Kafka Streams defines <code>Consumed</code> object with <code>with</code> factory methods that use implicit key and value <code>Serde</code> objects.</p>"},{"location":"kstream/GlobalKTable/","text":"<p><code>GlobalKTable</code> is...FIXME</p>","title":"GlobalKTable"},{"location":"kstream/GraphNode/","text":"<p><code>GraphNode</code> is an abstraction of graph nodes (for InternalStreamsBuilder to build a topology for StreamsBuilder).</p>","title":"GraphNode"},{"location":"kstream/GraphNode/#contract","text":"","title":"Contract"},{"location":"kstream/GraphNode/#writetotopology","text":"","title":"writeToTopology <pre><code>void writeToTopology(\n  InternalTopologyBuilder topologyBuilder,\n  Properties props)\n</code></pre> <p>Used when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to build and optimize a topology</li> </ul>"},{"location":"kstream/GraphNode/#implementations","text":"<ul> <li>ProcessorGraphNode</li> <li>StreamToTableNode</li> <li>BaseJoinProcessorNode</li> <li>SourceGraphNode</li> <li>StreamSinkNode</li> <li>StateStoreNode</li> <li>TableProcessorNode</li> <li>BaseRepartitionNode</li> <li>StreamTableJoinNode</li> </ul>","title":"Implementations"},{"location":"kstream/GroupedStreamAggregateBuilder/","text":"<p><code>GroupedStreamAggregateBuilder</code> is...FIXME</p>","title":"GroupedStreamAggregateBuilder"},{"location":"kstream/GroupedTableOperationRepartitionNode/","text":"<p><code>GroupedTableOperationRepartitionNode</code> is...FIXME</p>","title":"GroupedTableOperationRepartitionNode"},{"location":"kstream/Initializer/","text":"<p><code>Initializer</code> is...FIXME</p>","title":"Initializer"},{"location":"kstream/InternalStreamsBuilder/","text":"","title":"InternalStreamsBuilder"},{"location":"kstream/InternalStreamsBuilder/#creating-instance","text":"<p><code>InternalStreamsBuilder</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder  <p><code>InternalStreamsBuilder</code> is created\u00a0when:</p> <ul> <li><code>StreamsBuilder</code> is created</li> </ul>","title":"Creating Instance"},{"location":"kstream/InternalStreamsBuilder/#root-node","text":"","title":"Root Node <p><code>InternalStreamsBuilder</code> creates a root GraphNode when created.</p> <p>This root node is used to addGraphNode in the following high-level operators:</p> <ul> <li>stream</li> <li>table</li> <li>globalTable</li> <li>addStateStore</li> <li>addGlobalStore</li> </ul> <p>This root node is then used to build and optimize a topology (for StreamsBuilder).</p>"},{"location":"kstream/InternalStreamsBuilder/#buildandoptimizetopology","text":"","title":"buildAndOptimizeTopology <pre><code>void buildAndOptimizeTopology(\n  Properties props)\n</code></pre> <p><code>buildAndOptimizeTopology</code>...FIXME</p> <p><code>buildAndOptimizeTopology</code>\u00a0is used when:</p> <ul> <li><code>StreamsBuilder</code> is requested to build a topology</li> </ul>"},{"location":"kstream/InternalStreamsBuilder/#mergeduplicatesourcenodes","text":"","title":"mergeDuplicateSourceNodes <pre><code>void mergeDuplicateSourceNodes()\n</code></pre> <p><code>mergeDuplicateSourceNodes</code>...FIXME</p>"},{"location":"kstream/InternalStreamsBuilder/#adding-statestore-to-topology","text":"","title":"Adding StateStore to Topology <pre><code>void addStateStore(\n  StoreBuilder&lt;?&gt; builder)\n</code></pre> <p><code>addStateStore</code> adds a new StateStoreNode to the root node.</p> <p><code>addStateStore</code> is used when:</p> <ul> <li><code>StreamsBuilder</code> is requested to addStateStore</li> <li><code>KTableImpl</code> is requested to <code>doJoinOnForeignKey</code></li> </ul>"},{"location":"kstream/InternalStreamsBuilder/#stream","text":"","title":"stream <pre><code>KStream&lt;K, V&gt; stream(\n  Collection&lt;String&gt; topics,\n  ConsumedInternal&lt;K, V&gt; consumed)\nKStream&lt;K, V&gt; stream(\n  Pattern topicPattern,\n  ConsumedInternal&lt;K, V&gt; consumed)\n</code></pre> <p><code>stream</code>...FIXME</p> <p><code>stream</code>\u00a0is used when:</p> <ul> <li><code>StreamsBuilder</code> is requested to stream</li> </ul>"},{"location":"kstream/KGroupedStream/","text":"<p><code>KGroupedStream</code> is...FIXME</p>","title":"KGroupedStream"},{"location":"kstream/KGroupedStreamImpl/","text":"<p><code>KGroupedStreamImpl</code> is a KGroupedStream (and an AbstractStream).</p>","title":"KGroupedStreamImpl"},{"location":"kstream/KGroupedStreamImpl/#creating-instance","text":"<p><code>KGroupedStreamImpl</code> takes the following to be created:</p> <ul> <li> Name <li> Sub-Topology Source Nodes (Names) <li> <code>GroupedInternal&lt;K, V&gt;</code> <li>repartitionRequired flag</li> <li> GraphNode <li> InternalStreamsBuilder  <p><code>KGroupedStreamImpl</code> is created\u00a0when:</p> <ul> <li><code>KStreamImpl</code> is requested to groupBy and groupByKey</li> </ul>","title":"Creating Instance"},{"location":"kstream/KGroupedStreamImpl/#groupedstreamaggregatebuilder","text":"","title":"GroupedStreamAggregateBuilder <p><code>KGroupedStreamImpl</code> creates a GroupedStreamAggregateBuilder when created.</p>"},{"location":"kstream/KGroupedStreamImpl/#repartitionrequired-flag","text":"","title":"repartitionRequired Flag <p><code>KGroupedStreamImpl</code> is given a <code>repartitionRequired</code> flag when created.</p> <p>The <code>repartitionRequired</code> flag is always <code>true</code> for groupBy.</p>"},{"location":"kstream/KStream/","text":"<p><code>KStream&lt;K, V&gt;</code> is an abstraction of a stream of records (of key-value pairs).</p> <p><code>KStream</code> can be created directly from one or many Kafka topics (using StreamsBuilder.stream operators) or as a result of transformations on an existing <code>KStream</code> instance.</p> <p><code>KStream</code> offers a rich set of operators (KStream API) for building topologies to consume, process and produce (key-value) records.</p>","title":"KStream API \u2014 Stream of Records"},{"location":"kstream/KStream/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"kstream/KStream/#flatmap","text":"","title":"flatMap <pre><code>KStream&lt;KR, VR&gt; flatMap(\n  KeyValueMapper&lt;\n    ? super K,\n    ? super V,\n    ? extends Iterable&lt;? extends KeyValue&lt;? extends KR, ? extends VR&gt;&gt;&gt; mapper)\nKStream&lt;KR, VR&gt; flatMap(\n  KeyValueMapper&lt;\n    ? super K,\n    ? super V,\n    ? extends Iterable&lt;? extends KeyValue&lt;? extends KR, ? extends VR&gt;&gt;&gt; mapper,\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#foreach","text":"","title":"foreach <pre><code>void foreach(\n  ForeachAction&lt;? super K, ? super V&gt; action)\nvoid foreach(\n  ForeachAction&lt;? super K, ? super V&gt; action,\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#groupby","text":"","title":"groupBy <pre><code>KGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector)\nKGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector,\n  Grouped&lt;KR, V&gt; grouped)\n</code></pre>"},{"location":"kstream/KStream/#groupbykey","text":"","title":"groupByKey <pre><code>KGroupedStream&lt;K, V&gt; groupByKey()\nKGroupedStream&lt;K, V&gt; groupByKey(\n  Grouped&lt;K, V&gt; grouped)\n</code></pre>"},{"location":"kstream/KStream/#join","text":"","title":"join <pre><code>KStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoiner&lt;? super V, ? super GV, ? extends RV&gt; joiner)\nKStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoiner&lt;? super V, ? super GV, ? extends RV&gt; joiner,\n  Named named)\nKStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super GV, ? extends RV&gt; joiner)\nKStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super GV, ? extends RV&gt; joiner,\n  Named named)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoiner&lt;? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoiner&lt;? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K, V, VO&gt; streamJoined)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K, V, VO&gt; streamJoined)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoiner&lt;? super V, ? super VT, ? extends VR&gt; joiner)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoiner&lt;? super V, ? super VT, ? extends VR&gt; joiner,\n  Joined&lt;K, V, VT&gt; joined)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VT, ? extends VR&gt; joiner)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VT, ? extends VR&gt; joiner,\n  Joined&lt;K, V, VT&gt; joined)\n</code></pre>"},{"location":"kstream/KStream/#merge","text":"","title":"merge <pre><code>KStream&lt;K, V&gt; merge(\n  KStream&lt;K, V&gt; stream)\nKStream&lt;K, V&gt; merge(\n  KStream&lt;K, V&gt; stream,\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#peek","text":"","title":"peek <pre><code>KStream&lt;K, V&gt; peek(\n  ForeachAction&lt;? super K, ? super V&gt; action)\nKStream&lt;K, V&gt; peek(\n  ForeachAction&lt;? super K, ? super V&gt; action,\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#print","text":"","title":"print <pre><code>void print(\n  Printed&lt;K, V&gt; printed)\n</code></pre>"},{"location":"kstream/KStream/#process","text":"","title":"process <pre><code>void process(\n  ProcessorSupplier&lt;? super K, ? super V, Void, Void&gt; processorSupplier,\n  Named named,\n  String... stateStoreNames)\nvoid process(\n  ProcessorSupplier&lt;? super K, ? super V, Void, Void&gt; processorSupplier,\n  String... stateStoreNames)\n</code></pre>"},{"location":"kstream/KStream/#repartition","text":"","title":"repartition <pre><code>KStream&lt;K, V&gt; repartition()\nKStream&lt;K, V&gt; repartition(\n  Repartitioned&lt;K, V&gt; repartitioned)\n</code></pre>"},{"location":"kstream/KStream/#split","text":"","title":"split <pre><code>BranchedKStream&lt;K, V&gt; split()\nBranchedKStream&lt;K, V&gt; split(\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#to","text":"","title":"to <pre><code>void to(\n  String topic)\nvoid to(\n  String topic,\n  Produced&lt;K, V&gt; produced)\nvoid to(\n  TopicNameExtractor&lt;K, V&gt; topicExtractor)\nvoid to(\n  TopicNameExtractor&lt;K, V&gt; topicExtractor,\n  Produced&lt;K, V&gt; produced)\n</code></pre>"},{"location":"kstream/KStream/#totable","text":"","title":"toTable <pre><code>KTable&lt;K, V&gt; toTable()\nKTable&lt;K, V&gt; toTable(\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\nKTable&lt;K, V&gt; toTable(\n  Named named)\nKTable&lt;K, V&gt; toTable(\n  Named named,\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\n</code></pre>"},{"location":"kstream/KStream/#transform","text":"","title":"transform <pre><code>KStream&lt;K1, V1&gt; transform(\n  TransformerSupplier&lt;? super K, ? super V, KeyValue&lt;K1, V1&gt;&gt; transformerSupplier,\n  Named named,\n  String... stateStoreNames)\nKStream&lt;K1, V1&gt; transform(\n  TransformerSupplier&lt;? super K, ? super V, KeyValue&lt;K1, V1&gt;&gt; transformerSupplier,\n  String... stateStoreNames)\n</code></pre>"},{"location":"kstream/KStream/#implementations","text":"<ul> <li>KStreamImpl</li> </ul>","title":"Implementations"},{"location":"kstream/KStream/#demo","text":"<pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nval builder = new StreamsBuilder\n\n// Use type annotation to describe the stream, i.e. stream[String, String]\n// Else...Scala type inferencer gives us a stream of \"nothing\", i.e. KStream[Nothing, Nothing]\nval input = builder.stream[String, String](\"input\")\n</code></pre> <pre><code>scala&gt; :type input\norg.apache.kafka.streams.scala.kstream.KStream[String,String]\n</code></pre>","title":"Demo"},{"location":"kstream/KStreamAggProcessorSupplier/","text":"<p><code>KStreamAggProcessorSupplier</code> is...FIXME</p>","title":"KStreamAggProcessorSupplier"},{"location":"kstream/KStreamAggregate/","text":"<p><code>KStreamAggregate&lt;K, V, T&gt;</code> is a KStreamAggProcessorSupplier.</p>","title":"KStreamAggregate"},{"location":"kstream/KStreamAggregate/#creating-instance","text":"<p><code>KStreamAggregate</code> takes the following to be created:</p> <ul> <li> Name of a State Store <li> Initializer (of <code>T</code> values) <li> <code>Aggregator&lt;? super K, ? super V, T&gt;</code>  <p><code>KStreamAggregate</code> is created\u00a0when:</p> <ul> <li><code>CogroupedStreamAggregateBuilder</code> is requested to <code>build</code> (a KTable)</li> <li><code>KGroupedStreamImpl</code> is requested to aggregate and doCount</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamAggregateProcessor/","text":"<p><code>KStreamAggregateProcessor</code> is an AbstractProcessor of KStreamAggregate.</p>","title":"KStreamAggregateProcessor"},{"location":"kstream/KStreamAggregateProcessor/#creating-instance","text":"<p><code>KStreamAggregateProcessor</code> takes no arguments to be created.</p> <p><code>KStreamAggregateProcessor</code> is created\u00a0when:</p> <ul> <li><code>KStreamAggregate</code> is requested for a Processor</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamAggregateProcessor/#kstreamaggregate","text":"","title":"KStreamAggregate <p><code>KStreamAggregateProcessor</code> is a <code>private class</code> of KStreamAggregate and so have access to the internal properties (e.g. state name) thereof.</p>"},{"location":"kstream/KStreamAggregateProcessor/#timestampedkeyvaluestore","text":"","title":"TimestampedKeyValueStore <p><code>KStreamAggregateProcessor</code> looks up a TimestampedKeyValueStore by the name given when the owning KStreamAggregate was created.</p> <p>The <code>TimestampedKeyValueStore</code> is used for the following:</p> <ul> <li>Create a TimestampedTupleForwarder (in init)</li> <li>Process a key-value record (using a ValueAndTimestamp)</li> </ul>"},{"location":"kstream/KStreamAggregateProcessor/#timestampedtupleforwarder","text":"","title":"TimestampedTupleForwarder <p><code>KStreamAggregateProcessor</code> creates a new TimestampedTupleForwarder when created.</p> <p>The <code>TimestampedTupleForwarder</code> is used when processing a record.</p>"},{"location":"kstream/KStreamAggregateProcessor/#initializing","text":"","title":"Initializing <pre><code>void init(\n  ProcessorContext context)\n</code></pre> <p><code>init</code>...FIXME</p> <p><code>init</code>\u00a0is part of the AbstractProcessor abstraction.</p>"},{"location":"kstream/KStreamAggregateProcessor/#processing-record","text":"","title":"Processing Record <pre><code>void process(\n  K key, \n  V value)\n</code></pre> <p><code>process</code> requests the TimestampedKeyValueStore for the value for the input key (that gives a ValueAndTimestamp if found).</p> <p>With no previous value found, <code>process</code> requests the parent's Initializer for the initial value and the ProcessorContext for the timestamp.</p> <p><code>process</code> requests the parent's Aggregator for a new aggregate for the input key and value (and the previous or newly-created aggregation).</p> <p><code>process</code> creates a new ValueAndTimestamp with the new aggregate and the timestamp and requests the TimestampedKeyValueStore to store it (for the key).</p> <p>In the end, <code>process</code> requests the TimestampedTupleForwarder to maybeForward.</p>  <p><code>process</code>\u00a0is part of the AbstractProcessor abstraction.</p>"},{"location":"kstream/KStreamImpl/","text":"<p><code>KStreamImpl</code> is a KStream.</p>","title":"KStreamImpl"},{"location":"kstream/KStreamImpl/#creating-instance","text":"<p><code>KStreamImpl</code> takes the following to be created:</p> <ul> <li> Name <li> <code>Serde&lt;K&gt;</code> <li> <code>Serde&lt;V&gt;</code> <li> Sub-Topology Source Nodes (Names) <li>repartitionRequired flag</li> <li> GraphNode <li> InternalStreamsBuilder  <p><code>KStreamImpl</code> is created\u00a0when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to stream</li> <li>others</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamImpl/#repartitionrequired-flag","text":"","title":"repartitionRequired Flag <p><code>KStreamImpl</code> is given a <code>repartitionRequired</code> flag when created.</p>"},{"location":"kstream/KStreamImpl/#dojoin","text":"","title":"doJoin <pre><code>KStream&lt;K, VR&gt; doJoin(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K, V, VO&gt; streamJoined,\n  KStreamImplJoin join)\n</code></pre> <p>In the end, <code>doJoin</code> requests the given KStreamImplJoin to join.</p> <p><code>doJoin</code>\u00a0is used when:</p> <ul> <li><code>KStreamImpl</code> is requested to join, leftJoin and outerJoin</li> </ul>"},{"location":"kstream/KStreamImpl/#groupby","text":"","title":"groupBy <pre><code>KGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector)\nKGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector,\n  Grouped&lt;KR, V&gt; grouped)\n</code></pre> <p><code>groupBy</code>...FIXME</p> <p>In the end, <code>groupBy</code> creates a KGroupedStreamImpl (with the repartitionRequired flag enabled).</p> <p><code>groupBy</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImpl/#groupbykey","text":"","title":"groupByKey <pre><code>KGroupedStream&lt;K, V&gt; groupByKey()\nKGroupedStream&lt;K, V&gt; groupByKey(\n  Grouped&lt;K, V&gt; grouped)\n</code></pre> <p><code>groupByKey</code> creates a KGroupedStreamImpl.</p> <p><code>groupByKey</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImplJoin/","text":"","title":"KStreamImplJoin"},{"location":"kstream/KStreamImplJoin/#creating-instance","text":"<p><code>KStreamImplJoin</code> takes the following to be created:</p> <ul> <li> InternalStreamsBuilder <li> <code>leftOuter</code> flag <li> <code>rightOuter</code> flag  <p><code>KStreamImplJoin</code> is created\u00a0when:</p> <ul> <li><code>KStreamImpl</code> is requested to join, leftJoin and outerJoin</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamImplJoin/#join","text":"","title":"join <pre><code>KStream&lt;K1, R&gt; join(\n  KStream&lt;K1, V1&gt; lhs,\n  KStream&lt;K1, V2&gt; other,\n  ValueJoinerWithKey&lt;? super K1, ? super V1, ? super V2, ? extends R&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K1, V1, V2&gt; streamJoined)\n</code></pre> <p><code>join</code>...FIXME</p> <p><code>join</code>\u00a0is used when:</p> <ul> <li><code>KStreamImpl</code> is requested to doJoin</li> </ul>"},{"location":"kstream/KStreamSlidingWindowAggregateProcessor/","text":"<p><code>KStreamSlidingWindowAggregateProcessor</code> is...FIXME</p>","title":"KStreamSlidingWindowAggregateProcessor"},{"location":"kstream/KStreamWindowAggregateProcessor/","text":"<p><code>KStreamWindowAggregateProcessor</code> is...FIXME</p>","title":"KStreamWindowAggregateProcessor"},{"location":"kstream/KTable/","text":"<p><code>KTable</code> is...FIXME</p>","title":"KTable"},{"location":"kstream/KTableImpl/","text":"<p><code>KTableImpl</code> is...FIXME</p>","title":"KTableImpl"},{"location":"kstream/KTableKTableJoinMergeProcessor/","text":"<p><code>KTableKTableJoinMergeProcessor</code> is...FIXME</p>","title":"KTableKTableJoinMergeProcessor"},{"location":"kstream/KTableSource/","text":"<p><code>KTableSource</code> is...FIXME</p>","title":"KTableSource"},{"location":"kstream/Materialized/","text":"<p><code>Materialized</code> is...FIXME</p>","title":"Materialized"},{"location":"kstream/NamedOperation/","text":"<p><code>NamedOperation</code> is an abstraction of metadata with the name of the associated processors (and in turn the names of operations, internal topics and stores).</p>","title":"NamedOperation"},{"location":"kstream/NamedOperation/#contract","text":"","title":"Contract"},{"location":"kstream/NamedOperation/#withname","text":"","title":"withName <pre><code>T withName(\n  String name)\n</code></pre> <p>Processor name</p>"},{"location":"kstream/NamedOperation/#implementations","text":"<ul> <li>Branched</li> <li>Consumed</li> <li>Grouped</li> <li>Joined</li> <li>Named</li> <li>Printed</li> <li>Produced</li> <li>Repartitioned</li> <li>StreamJoined</li> <li>Suppressed</li> </ul>","title":"Implementations"},{"location":"kstream/OptimizableRepartitionNode/","text":"<p><code>OptimizableRepartitionNode</code> is...FIXME</p>","title":"OptimizableRepartitionNode"},{"location":"kstream/Produced/","text":"<p><code>Produced&lt;K, V&gt;</code> describes how to produce records in a topology in the High-Level KStream DSL for the following high-level operators:</p> <ul> <li>KStream.to</li> </ul> <p><code>Produced&lt;K, V&gt;</code> is a NamedOperation.</p>","title":"Produced \u2014 Metadata for Producing Records"},{"location":"kstream/Produced/#demo","text":"<pre><code>import org.apache.kafka.common.serialization.Serdes\nimport org.apache.kafka.streams.kstream.Produced\nval produced = Produced.`with`(Serdes.Long, Serdes.String)\n</code></pre> <pre><code>scala&gt; :type produced\norg.apache.kafka.streams.kstream.Produced[Long,String]\n</code></pre>","title":"Demo"},{"location":"kstream/Produced/#creating-instance","text":"<p><code>Produced</code> takes the following to be created:</p> <ul> <li> <code>Serde&lt;K&gt;</code> of keys (Apache Kafka) <li> <code>Serde&lt;V&gt;</code> of values (Apache Kafka) <li> <code>StreamPartitioner&lt;? super K, ? super V&gt;</code> <li> Processor Name  <p><code>Produced</code> is created\u00a0using the factories.</p>","title":"Creating Instance"},{"location":"kstream/Produced/#creating-consumed","text":"","title":"Creating Consumed"},{"location":"kstream/Produced/#as","text":"","title":"as <pre><code>Produced&lt;K, V&gt; as(\n  String processorName)\n</code></pre>"},{"location":"kstream/Produced/#with","text":"","title":"with <pre><code>Produced&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde)\nProduced&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde,\n  StreamPartitioner&lt;? super K, ? super V&gt; partitioner)\n</code></pre>"},{"location":"kstream/Produced/#keyserde","text":"","title":"keySerde <pre><code>Produced&lt;K, V&gt; keySerde(\n  Serde&lt;K&gt; keySerde)\n</code></pre>"},{"location":"kstream/Produced/#valueserde","text":"","title":"valueSerde <pre><code>Produced&lt;K, V&gt; valueSerde(\n  Serde&lt;V&gt; valueSerde)\n</code></pre>"},{"location":"kstream/Produced/#streampartitioner","text":"","title":"streamPartitioner <pre><code>Produced&lt;K, V&gt; streamPartitioner(\n  StreamPartitioner&lt;? super K, ? super V&gt; partitioner)\n</code></pre>"},{"location":"kstream/Produced/#scala-api","text":"","title":"Scala API <p>Scala API for Kafka Streams makes the optional <code>Produced</code> metadata an implicit parameter in the KStream API.</p> <p>Moreover, <code>ImplicitConversions</code> object defines <code>producedFromSerde</code> implicit method that creates a <code>Produced</code> instance with the key and value <code>Serde</code> objects available in implicit scope.</p> <p>And the last but not least, Scala API for Kafka Streams defines <code>Produced</code> object with <code>with</code> factory methods that use implicit key and value <code>Serde</code> objects.</p>"},{"location":"kstream/StateStoreNode/","text":"<p><code>StateStoreNode</code> is a GraphNode.</p>","title":"StateStoreNode"},{"location":"kstream/StateStoreNode/#creating-instance","text":"<p><code>StateStoreNode</code> takes the following to be created:</p> <ul> <li> StoreBuilder  <p><code>StateStoreNode</code> is created\u00a0when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to add a StoreBuilder</li> </ul>","title":"Creating Instance"},{"location":"kstream/StateStoreNode/#writetotopology","text":"","title":"writeToTopology <pre><code>void writeToTopology(\n  InternalTopologyBuilder topologyBuilder,\n  Properties props)\n</code></pre> <p><code>writeToTopology</code> merely requests the given InternalTopologyBuilder to add the storeBuilder.</p> <p><code>writeToTopology</code>\u00a0is part of the GraphNode abstraction.</p>"},{"location":"kstream/StreamSourceNode/","text":"<p><code>StreamSourceNode</code> is...FIXME</p>","title":"StreamSourceNode"},{"location":"kstream/StreamsBuilder/","text":"<p><code>StreamsBuilder</code> is the entry point to the High-Level Streams DSL to define and build a stream processing topology.</p> <p>All of the high-level operators use the InternalStreamsBuilder behind the scenes. In other words, <code>StreamsBuilder</code> offers a more developer-friendly high-level API for developing Kafka Streams applications than using the <code>InternalStreamsBuilder</code> API directly (and is a fa\u00e7ade of <code>InternalStreamsBuilder</code>).</p>  <p>Scala API for Kafka Streams</p> <p>Use Scala API for Kafka Streams to make your Kafka Streams development more pleasant with Scala.</p>","title":"StreamsBuilder"},{"location":"kstream/StreamsBuilder/#creating-instance","text":"<p><code>StreamsBuilder</code> takes no arguments to be created.</p> <pre><code>import org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <p>While being created, <code>StreamsBuilder</code> creates an empty Topology that in turn is requested for an InternalTopologyBuilder. In the end, <code>StreamsBuilder</code> creates an InternalStreamsBuilder.</p> <p></p>","title":"Creating Instance"},{"location":"kstream/StreamsBuilder/#topology","text":"","title":"Topology <p><code>StreamsBuilder</code> creates a Topology when created.</p> <p><code>StreamsBuilder</code> uses the <code>Topology</code> to create an InternalTopologyBuilder.</p> <p>The <code>Topology</code> is then optimized and returned when <code>StreamsBuilder</code> is requested to build a topology.</p>"},{"location":"kstream/StreamsBuilder/#building-and-optimizing-topology","text":"","title":"Building and Optimizing Topology <pre><code>Topology build() // (1)\nTopology build(\n  Properties props)\n</code></pre> <ol> <li>Uses undefined properties (<code>null</code>)</li> </ol> <p><code>build</code> requests the InternalStreamsBuilder to build and optimize a topology. In the end, <code>build</code> returns the Topology.</p>"},{"location":"kstream/StreamsBuilder/#globaltable","text":"","title":"globalTable <pre><code>GlobalKTable&lt;K, V&gt; globalTable(\n  String topic)\nGlobalKTable&lt;K, V&gt; globalTable(\n  String topic,\n  Consumed&lt;K, V&gt; consumed)\nGlobalKTable&lt;K, V&gt; globalTable(\n  String topic,\n  Consumed&lt;K, V&gt; consumed,\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\nGlobalKTable&lt;K, V&gt; globalTable(\n  String topic,\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\n</code></pre> <p><code>globalTable</code> adds a GlobalKTable to a topology.</p>"},{"location":"kstream/StreamsBuilder/#demo-non-queryable-globalktable","text":"","title":"Demo: Non-queryable GlobalKTable <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nimport org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <pre><code>val globalTable = builder.globalTable[String, String](topic = \"demo-global-table\")\n</code></pre> <pre><code>scala&gt; :type globalTable\norg.apache.kafka.streams.kstream.GlobalKTable[String,String]\n</code></pre> <pre><code>assert(globalTable.queryableStoreName == null)\n</code></pre> <pre><code>val topology = builder.build()\n</code></pre> <pre><code>scala&gt; println(topology.describe)\nTopologies:\n   Sub-topology: 0 for global store (will not generate tasks)\n    Source: KSTREAM-SOURCE-0000000001 (topics: [demo-global-table])\n      --&gt; KTABLE-SOURCE-0000000002\n    Processor: KTABLE-SOURCE-0000000002 (stores: [demo-global-table-STATE-STORE-0000000000])\n      --&gt; none\n      &lt;-- KSTREAM-SOURCE-0000000001\n</code></pre>"},{"location":"kstream/StreamsBuilder/#demo-queryable-globalktable","text":"","title":"Demo: Queryable GlobalKTable <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nimport org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <pre><code>import org.apache.kafka.streams.state.Stores\nval supplier = Stores.inMemoryKeyValueStore(\"queryable-store-name\")\n\nimport org.apache.kafka.streams.scala.kstream.Materialized\nval materialized = Materialized.as[String, String](supplier)\nval zipCodes = builder.globalTable[String, String](topic = \"zip-codes\", materialized)\n</code></pre> <pre><code>scala&gt; :type zipCodes\norg.apache.kafka.streams.kstream.GlobalKTable[String,String]\n</code></pre> <pre><code>assert(zipCodes.queryableStoreName == \"queryable-store-name\")\n</code></pre> <pre><code>val topology = builder.build()\n</code></pre> <pre><code>scala&gt; println(topology.describe)\nTopologies:\n   Sub-topology: 0 for global store (will not generate tasks)\n    Source: KSTREAM-SOURCE-0000000000 (topics: [zip-codes])\n      --&gt; KTABLE-SOURCE-0000000001\n    Processor: KTABLE-SOURCE-0000000001 (stores: [queryable-store-name])\n      --&gt; none\n      &lt;-- KSTREAM-SOURCE-0000000000\n</code></pre>"},{"location":"kstream/StreamsBuilder/#stream","text":"","title":"stream <pre><code>KStream&lt;K, V&gt; stream(\n  Collection&lt;String&gt; topics)\nKStream&lt;K, V&gt; stream(\n  Collection&lt;String&gt; topics,\n  Consumed&lt;K, V&gt; consumed)\nKStream&lt;K, V&gt; stream(\n  Pattern topicPattern)\nKStream&lt;K, V&gt; stream(\n  Pattern topicPattern,\n  Consumed&lt;K, V&gt; consumed)\nKStream&lt;K, V&gt; stream(\n  String topic)\nKStream&lt;K, V&gt; stream(\n  String topic,\n  Consumed&lt;K, V&gt; consumed)\n</code></pre> <p><code>stream</code> requests the InternalStreamsBuilder to stream.</p>"},{"location":"kstream/StreamsBuilder/#demo-custom-processor-name","text":"","title":"Demo: Custom Processor Name <pre><code>import org.apache.kafka.streams.scala._\nimport org.apache.kafka.streams.scala.kstream._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nimport org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <pre><code>implicit val consumed = Consumed.`with`[String, String].withName(\"processorName\")\nval demo = builder.stream[String, String](\"demo\")\n</code></pre> <pre><code>scala&gt; println(builder.build().describe)\nTopologies:\n   Sub-topology: 0\n    Source: processorName (topics: [demo])\n      --&gt; none\n</code></pre>"},{"location":"kstream/TimestampedTupleForwarder/","text":"<p><code>TimestampedTupleForwarder</code> is used by processors to determine whether or not to forward records to child nodes (downstream processors) (that happens only with no caching).</p>","title":"TimestampedTupleForwarder"},{"location":"kstream/TimestampedTupleForwarder/#creating-instance","text":"<p><code>TimestampedTupleForwarder</code> takes the following to be created:</p> <ul> <li> StateStore <li> ProcessorContext <li> <code>TimestampedCacheFlushListener</code> <li> <code>sendOldValues</code> flag  <p><code>TimestampedTupleForwarder</code> is created\u00a0when:</p> <ul> <li><code>KStreamAggregateProcessor</code> is requested to initialize</li> <li><code>KStreamSlidingWindowAggregateProcessor</code> is requested to initialize</li> <li><code>KStreamWindowAggregateProcessor</code> is requested to initialize</li> <li><code>KTableSource</code> is requested to initialize</li> <li>others</li> </ul>","title":"Creating Instance"},{"location":"kstream/TimestampedTupleForwarder/#cachingenabled-flag","text":"","title":"cachingEnabled Flag <p><code>TimestampedTupleForwarder</code> requests the StateStore to setFlushListener when created. The returned value is used to initialize <code>cachingEnabled</code> internal flag for maybeForward.</p>"},{"location":"kstream/TimestampedTupleForwarder/#maybeforward","text":"","title":"maybeForward <pre><code>void maybeForward(\n  K key,\n  V newValue,\n  V oldValue) // (1)\nvoid maybeForward(\n  K key,\n  V newValue,\n  V oldValue,\n  long timestamp)\nvoid maybeForward(\n  Record&lt;K, Change&lt;V&gt;&gt; record)\n</code></pre> <p><code>maybeForward</code> requests the InternalProcessorContext to forward a record only with the cachingEnabled flag disabled.</p>"},{"location":"kstream/UnoptimizableRepartitionNode/","text":"<p><code>UnoptimizableRepartitionNode</code> is...FIXME</p>","title":"UnoptimizableRepartitionNode"},{"location":"processor/","text":"<p>Processor API is a low-level API for developers to define topologies in Kafka Streams (mostly when High-Level Streams DSL would not meet expectations).</p> <p>Processor API comes with the following low-level stream processing abstractions:</p> <ul> <li>Processor</li> <li>ProcessorContext</li> <li>ProcessorSupplier</li> <li>Punctuator</li> </ul>","title":"Processor API"},{"location":"processor/AbstractProcessor/","text":"<p><code>AbstractProcessor</code> is...FIXME</p>","title":"AbstractProcessor"},{"location":"processor/AbstractProcessorContext/","text":"<p><code>AbstractProcessorContext</code> is...FIXME</p>","title":"AbstractProcessorContext"},{"location":"processor/AbstractTask/","text":"<p><code>AbstractTask</code>\u00a0is a base abstraction of the Task abstraction for tasks.</p>","title":"AbstractTask"},{"location":"processor/AbstractTask/#implementations","text":"<ul> <li>StandbyTask</li> <li>StreamTask</li> </ul>","title":"Implementations"},{"location":"processor/AbstractTask/#creating-instance","text":"<p><code>AbstractTask</code> takes the following to be created:</p> <ul> <li> <code>TaskId</code> <li> ProcessorTopology <li> StateDirectory <li> ProcessorStateManager <li> Input <code>TopicPartition</code>s <li> task.timeout.ms configuration property <li> Task Type <li> <code>AbstractTask</code> Class  Abstract Class<p><code>AbstractTask</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete AbstractTasks.</p>","title":"Creating Instance"},{"location":"processor/ActiveTaskCreator/","text":"","title":"ActiveTaskCreator"},{"location":"processor/ActiveTaskCreator/#createtasks","text":"","title":"createTasks <pre><code>Collection&lt;Task&gt; createTasks(\n  Consumer&lt;byte[], byte[]&gt; consumer,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; tasksToBeCreated)\n</code></pre> <p><code>createTasks</code>...FIXME</p> <p><code>createTasks</code>\u00a0is used when:</p> <ul> <li><code>Tasks</code> is requested to createTasks</li> </ul>"},{"location":"processor/ActiveTaskCreator/#createactivetaskfromstandby","text":"","title":"createActiveTaskFromStandby <pre><code>StreamTask createActiveTaskFromStandby(\n  StandbyTask standbyTask,\n  Set&lt;TopicPartition&gt; inputPartitions,\n  Consumer&lt;byte[], byte[]&gt; consumer)\n</code></pre> <p><code>createActiveTaskFromStandby</code>...FIXME</p> <p><code>createActiveTaskFromStandby</code>\u00a0is used when:</p> <ul> <li><code>Tasks</code> is requested to convertStandbyToActive</li> </ul>"},{"location":"processor/ActiveTaskCreator/#createactivetask","text":"","title":"createActiveTask <pre><code>StreamTask createActiveTask(\n  TaskId taskId,\n  Set&lt;TopicPartition&gt; inputPartitions,\n  Consumer&lt;byte[], byte[]&gt; consumer,\n  LogContext logContext,\n  ProcessorTopology topology,\n  ProcessorStateManager stateManager,\n  InternalProcessorContext context)\n</code></pre> <p><code>createActiveTask</code>...FIXME</p> <p><code>createActiveTask</code>\u00a0is used when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createTasks and createActiveTaskFromStandby</li> </ul>"},{"location":"processor/GlobalProcessorContextImpl/","text":"<p><code>GlobalProcessorContextImpl</code> is...FIXME</p>","title":"GlobalProcessorContextImpl"},{"location":"processor/GlobalStateManager/","text":"<p><code>GlobalStateManager</code> is...FIXME</p>","title":"GlobalStateManager"},{"location":"processor/GlobalStateManagerImpl/","text":"<p><code>GlobalStateManagerImpl</code> is...FIXME</p>","title":"GlobalStateManagerImpl"},{"location":"processor/GlobalStateUpdateTask/","text":"<p><code>GlobalStateUpdateTask</code> is...FIXME</p>","title":"GlobalStateUpdateTask"},{"location":"processor/GlobalStreamThread/","text":"<p><code>GlobalStreamThread</code> is...FIXME</p>","title":"GlobalStreamThread"},{"location":"processor/NodeFactory/","text":"<p><code>NodeFactory</code> is...FIXME</p>","title":"NodeFactory"},{"location":"processor/Processor/","text":"<p><code>Processor&lt;KIn, VIn, KOut, VOut&gt;</code> is an abstraction of processing nodes (in a stream processing topology).</p>","title":"Processor"},{"location":"processor/Processor/#contract","text":"","title":"Contract"},{"location":"processor/Processor/#close","text":"","title":"close <pre><code>void close()\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorNode</code> is requested to close</li> </ul>"},{"location":"processor/Processor/#init","text":"","title":"init <pre><code>void init(\n  ProcessorContext&lt;KOut, VOut&gt; context)\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorNode</code> is requested to init</li> </ul>"},{"location":"processor/Processor/#process","text":"","title":"process <pre><code>void process(\n  Record&lt;KIn, VIn&gt; record)\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorNode</code> is requested to process</li> </ul>"},{"location":"processor/Processor/#implementations","text":"<ul> <li><code>ContextualProcessor</code></li> <li><code>ForeachProcessor</code></li> </ul>","title":"Implementations"},{"location":"processor/ProcessorContext/","text":"<p><code>ProcessorContext</code> is...FIXME</p>","title":"ProcessorContext"},{"location":"processor/ProcessorContextImpl/","text":"<p><code>ProcessorContextImpl</code> is...FIXME</p>","title":"ProcessorContextImpl"},{"location":"processor/ProcessorNode/","text":"<p><code>ProcessorNode</code> is a \"hosting environment\" of a Processor in a processor topology.</p>","title":"ProcessorNode"},{"location":"processor/ProcessorNode/#creating-instance","text":"<p><code>ProcessorNode</code> takes the following to be created:</p> <ul> <li> Name <li> Processor <li> Names of the state stores  <p><code>ProcessorNode</code> is created\u00a0when:</p> <ul> <li><code>ProcessorNodeFactory</code> is requested to build a processor</li> </ul>","title":"Creating Instance"},{"location":"processor/ProcessorNode/#specialized-processornodes","text":"<p>SourceNode and SinkNode are specialized <code>ProcessorNode</code>s.</p>","title":"Specialized ProcessorNodes"},{"location":"processor/ProcessorNode/#terminal-node","text":"","title":"Terminal Node <p><code>ProcessorNode</code> is considered terminal when there are no children.</p>"},{"location":"processor/ProcessorNodeFactory/","text":"<p><code>ProcessorNodeFactory</code> is a NodeFactory.</p>","title":"ProcessorNodeFactory"},{"location":"processor/ProcessorNodeFactory/#creating-instance","text":"<p><code>ProcessorNodeFactory</code> takes the following to be created:</p> <ul> <li> Name <li> Predecessor Nodes <li> ProcessorSupplier  <p><code>ProcessorNodeFactory</code> is created\u00a0when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to addProcessor and addGlobalStore</li> </ul>","title":"Creating Instance"},{"location":"processor/ProcessorStateManager/","text":"<p><code>ProcessorStateManager</code> is a StateManager.</p>","title":"ProcessorStateManager"},{"location":"processor/ProcessorStateManager/#creating-instance","text":"<p><code>ProcessorStateManager</code> takes the following to be created:</p> <ul> <li> TaskId <li> <code>TaskType</code> <li>eosEnabled flag</li> <li> <code>LogContext</code> <li> StateDirectory <li> <code>ChangelogRegister</code> <li> <code>storeToChangelogTopic</code> collection <li> Source <code>TopicPartition</code>s  <p><code>ProcessorStateManager</code> is created\u00a0when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createTasks</li> <li><code>StandbyTaskCreator</code> is requested to createTasks</li> <li><code>TopologyTestDriver</code> is requested to setupTask</li> </ul>","title":"Creating Instance"},{"location":"processor/ProcessorStateManager/#eosenabled-flag","text":"","title":"eosEnabled Flag <p><code>ProcessorStateManager</code> is given <code>eosEnabled</code> flag when created.</p>"},{"location":"processor/ProcessorStateManager/#offset-checkpoint-file","text":"","title":"Offset Checkpoint File <p>When created, <code>ProcessorStateManager</code> requests the given StateDirectory for a checkpoint file for the given TaskId and creates a new <code>OffsetCheckpoint</code>.</p> <p><code>ProcessorStateManager</code> uses the <code>OffsetCheckpoint</code> for the following:</p> <ul> <li>initializeStoreOffsetsFromCheckpoint (to read offsets and then delete it with eosEnabled)</li> <li>Checkpoint</li> <li>deleteCheckPointFileIfEOSEnabled</li> </ul>"},{"location":"processor/ProcessorStateManager/#flushing-store-caches","text":"","title":"Flushing Store Caches <pre><code>void flushCache()\n</code></pre> <p><code>flushCache</code>...FIXME</p> <p><code>flushCache</code>\u00a0is used when:</p> <ul> <li><code>StreamTask</code> is requested to prepareCommit</li> </ul>"},{"location":"processor/ProcessorStateManager/#checkpointing","text":"","title":"Checkpointing <pre><code>void checkpoint()\n</code></pre> <p><code>checkpoint</code> finds all the persistent state stores (in the stores registry) that are logged (with a <code>changelogPartition</code>) and are not corrupted. For every state store, <code>checkpoint</code> records the <code>changelogPartition</code> and the offset (in a local <code>checkpointingOffsets</code> collection).</p> <p><code>checkpoint</code> prints out the following DEBUG message to the logs:</p> <pre><code>Writing checkpoint: [checkpointingOffsets]\n</code></pre> <p><code>checkpoint</code> requests the OffsetCheckpoint file to write out the offsets.</p>  <p>In case of any IO exceptions, <code>checkpoint</code> prints out the following WARN message to the logs:</p> <pre><code>Failed to write offset checkpoint file to [checkpointFile].\nThis may occur if OS cleaned the state.dir in case when it located in ${java.io.tmpdir} directory.\nThis may also occur due to running multiple instances on the same machine using the same state dir.\nChanging the location of state.dir may resolve the problem.\n</code></pre>  <p><code>checkpoint</code>\u00a0is part of the StateManager abstraction.</p>"},{"location":"processor/ProcessorStateManager/#registerstore","text":"","title":"registerStore <pre><code>void registerStore(\n  StateStore store, \n  StateRestoreCallback stateRestoreCallback)\n</code></pre> <p><code>registerStore</code>...FIXME</p> <p><code>registerStore</code>\u00a0is part of the StateManager abstraction.</p>"},{"location":"processor/ProcessorStateManager/#registerstatestores","text":"","title":"registerStateStores <pre><code>void registerStateStores(\n  List&lt;StateStore&gt; allStores, \n  InternalProcessorContext processorContext)\n</code></pre> <p><code>registerStateStores</code>...FIXME</p> <p><code>registerStateStores</code>\u00a0is used when:</p> <ul> <li><code>StateManagerUtil</code> is requested to registerStateStores</li> </ul>"},{"location":"processor/ProcessorStateManager/#mayberegisterstorewithchangelogreader","text":"","title":"maybeRegisterStoreWithChangelogReader <pre><code>void maybeRegisterStoreWithChangelogReader(\n  String storeName)\n</code></pre> <p><code>maybeRegisterStoreWithChangelogReader</code>...FIXME</p> <p><code>maybeRegisterStoreWithChangelogReader</code>\u00a0is used when:</p> <ul> <li><code>ProcessorStateManager</code> is requested to registerStateStores and registerStore</li> </ul>"},{"location":"processor/ProcessorStateManager/#getstorepartition","text":"","title":"getStorePartition <pre><code>TopicPartition getStorePartition(\n  String storeName)\n</code></pre> <p><code>getStorePartition</code> creates a <code>TopicPartition</code> with the following:</p> <ul> <li>changelogFor with the given <code>storeName</code> for the name of the (changelog) topic</li> <li>The partition of the TaskId for the partition (of the changelog topic)</li> </ul> <p><code>getStorePartition</code>\u00a0is used when:</p> <ul> <li><code>ProcessorStateManager</code> is requested to maybeRegisterStoreWithChangelogReader and registerStore</li> </ul>"},{"location":"processor/ProcessorStateManager/#initializestoreoffsetsfromcheckpoint","text":"","title":"initializeStoreOffsetsFromCheckpoint <pre><code>void initializeStoreOffsetsFromCheckpoint(\n  boolean storeDirIsEmpty)\n</code></pre> <p><code>initializeStoreOffsetsFromCheckpoint</code>...FIXME</p> <p><code>initializeStoreOffsetsFromCheckpoint</code>\u00a0is used when:</p> <ul> <li><code>StateManagerUtil</code> is requested to registerStateStores</li> </ul>"},{"location":"processor/ProcessorStateManager/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.processor.internals.ProcessorStateManager</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.processor.internals.ProcessorStateManager=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"processor/ProcessorSupplier/","text":"<p><code>ProcessorSupplier</code>\u00a0is an extension of the <code>ConnectedStoreProvider</code> and <code>Supplier</code> (Java) abstractions for processor suppliers.</p> <p><code>ProcessorSupplier</code>\u00a0is marked with <code>FunctionalInterface</code> (Java) annotation.</p>","title":"ProcessorSupplier"},{"location":"processor/ProcessorSupplier/#contract","text":"","title":"Contract"},{"location":"processor/ProcessorSupplier/#creating-processor","text":"","title":"Creating Processor <pre><code>Processor&lt;KIn, VIn, KOut, VOut&gt; get()\n</code></pre> <p>Creates a Processor</p>"},{"location":"processor/ProcessorTopology/","text":"","title":"ProcessorTopology"},{"location":"processor/ProcessorTopology/#creating-instance","text":"<p><code>ProcessorTopology</code> takes the following to be created:</p> <ul> <li> <code>ProcessorNode</code>s <li> <code>SourceNode</code>s by topic <li> <code>SinkNode</code> by topic <li> <code>StateStore</code>s <li> Global <code>StateStore</code>s <li> Store names by topic <li> Repartition topics  <p><code>ProcessorTopology</code> is created\u00a0when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to build a ProcessorTopology</li> </ul>","title":"Creating Instance"},{"location":"processor/Punctuator/","text":"<p><code>Punctuator</code> is...FIXME</p>","title":"Punctuator"},{"location":"processor/SinkNode/","text":"<p><code>SinkNode</code> is...FIXME</p>","title":"SinkNode"},{"location":"processor/SinkNodeFactory/","text":"<p><code>SinkNodeFactory</code> is...FIXME</p>","title":"SinkNodeFactory"},{"location":"processor/SourceNode/","text":"<p><code>SourceNode</code> is...FIXME</p>","title":"SourceNode"},{"location":"processor/SourceNodeFactory/","text":"<p><code>SourceNodeFactory</code> is...FIXME</p>","title":"SourceNodeFactory"},{"location":"processor/StandbyTask/","text":"<p><code>StandbyTask</code> is a Task (and AbstractTask).</p>","title":"StandbyTask"},{"location":"processor/StandbyTask/#creating-instance","text":"<p><code>StandbyTask</code> takes the following to be created:</p> <ul> <li> <code>TaskId</code> <li> Input <code>TopicPartition</code>s <li> ProcessorTopology <li> StreamsConfig <li> <code>StreamsMetricsImpl</code> <li> ProcessorStateManager <li> StateDirectory <li> <code>ThreadCache</code> <li> <code>InternalProcessorContext</code>  <p>When created, <code>StandbyTask</code> requests the InternalProcessorContext to <code>transitionToStandby</code> with the ThreadCache.</p> <p><code>StandbyTask</code> is created\u00a0when:</p> <ul> <li><code>StandbyTaskCreator</code> is requested to createStandbyTask</li> </ul>","title":"Creating Instance"},{"location":"processor/StandbyTask/#abstracttask","text":"","title":"AbstractTask <p><code>StandbyTask</code> is an AbstractTask.</p>"},{"location":"processor/StandbyTask/#task-type","text":"","title":"Task Type <p><code>StandbyTask</code> uses standby-task for task type.</p>"},{"location":"processor/StandbyTask/#class","text":"","title":"Class <p><code>StandbyTask</code> uses <code>StandbyTask.class</code> for clazz.</p>"},{"location":"processor/StandbyTaskCreator/","text":"","title":"StandbyTaskCreator"},{"location":"processor/StandbyTaskCreator/#creating-instance","text":"<p><code>StandbyTaskCreator</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder <li> StreamsConfig <li> <code>StreamsMetricsImpl</code> <li> StateDirectory <li> <code>ChangelogReader</code> <li> Thread ID <li> <code>Logger</code>  <p>When created, <code>StandbyTaskCreator</code> initializes a task sensor and a ThreadCache.</p> <p><code>StandbyTaskCreator</code> is created\u00a0when:</p> <ul> <li><code>StreamThread</code> utility is used to create a StreamThread</li> </ul>","title":"Creating Instance"},{"location":"processor/StateDirectory/","text":"<p><code>StateDirectory</code> is...FIXME</p>","title":"StateDirectory"},{"location":"processor/StateManager/","text":"<p><code>StateManager</code> is an abstraction of state managers.</p>","title":"StateManager"},{"location":"processor/StateManager/#contract","text":"","title":"Contract"},{"location":"processor/StateManager/#basedir","text":"","title":"baseDir <pre><code>File baseDir()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractProcessorContext</code> is requested for the stateDir</li> <li><code>GlobalStateUpdateTask</code> is requested to close</li> <li><code>StateManagerUtil</code> is requested to closeStateManager</li> </ul>"},{"location":"processor/StateManager/#changelogfor","text":"","title":"changelogFor <pre><code>String changelogFor(\n  String storeName)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractProcessorContext</code> is requested for the changelogFor</li> <li><code>GlobalStateManagerImpl</code> is created</li> <li><code>ProcessorStateManager</code> is requested to getStorePartition and isLoggingEnabled</li> </ul>"},{"location":"processor/StateManager/#changelogoffsets","text":"","title":"changelogOffsets <pre><code>Map&lt;TopicPartition, Long&gt; changelogOffsets()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractTask</code> is requested to maybeWriteCheckpoint</li> <li><code>GlobalStateUpdateTask</code> is requested to initialize</li> <li><code>ProcessorStateManager</code> is requested to changelogPartitions</li> <li><code>StandbyTask</code> is requested to commitNeeded and changelogOffsets</li> <li><code>StoreChangelogReader</code> is requested to getPositionString</li> <li><code>StreamTask</code> is requested to changelogOffsets</li> </ul>"},{"location":"processor/StateManager/#checkpoint","text":"","title":"checkpoint <pre><code>void checkpoint()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractTask</code> is requested to maybeWriteCheckpoint</li> <li><code>GlobalStateUpdateTask</code> is requested to flushState</li> </ul>"},{"location":"processor/StateManager/#close","text":"","title":"close <pre><code>void close()\n</code></pre> <p>Used when:</p> <ul> <li><code>GlobalStateUpdateTask</code> is requested to close</li> <li><code>StateManagerUtil</code> is requested to closeStateManager</li> </ul>"},{"location":"processor/StateManager/#flush","text":"","title":"flush <pre><code>void flush()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractTask</code> is requested to maybeWriteCheckpoint</li> <li><code>GlobalStateUpdateTask</code> is requested to flushState</li> </ul>"},{"location":"processor/StateManager/#getglobalstore","text":"","title":"getGlobalStore <pre><code>StateStore getGlobalStore(\n  String name)\n</code></pre> <p>Used when:</p> <ul> <li><code>GlobalProcessorContextImpl</code> is requested to getStateStore</li> <li><code>GlobalStateManagerImpl</code> is requested to getStore</li> <li><code>ProcessorContextImpl</code> is requested to getStateStore</li> </ul>"},{"location":"processor/StateManager/#getstore","text":"","title":"getStore <pre><code>StateStore getStore(\n  String name)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractTask</code> is requested to getStore</li> <li><code>ProcessorContextImpl</code> is requested to getStateStore</li> <li><code>TopologyTestDriver</code> is requested to getStateStore</li> </ul>"},{"location":"processor/StateManager/#registerstore","text":"","title":"registerStore <pre><code>void registerStore(\n  StateStore store, \n  StateRestoreCallback stateRestoreCallback)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractProcessorContext</code> is requested to register</li> </ul>"},{"location":"processor/StateManager/#tasktype","text":"","title":"taskType <pre><code>TaskType taskType()\n</code></pre>"},{"location":"processor/StateManager/#updatechangelogoffsets","text":"","title":"updateChangelogOffsets <pre><code>void updateChangelogOffsets(\n  Map&lt;TopicPartition, Long&gt; writtenOffsets)\n</code></pre> <p>Used when:</p> <ul> <li><code>GlobalStateUpdateTask</code> is requested to flushState</li> <li><code>StreamTask</code> is requested to maybeWriteCheckpoint</li> </ul>"},{"location":"processor/StateManager/#implementations","text":"<ul> <li>GlobalStateManager</li> <li>ProcessorStateManager</li> </ul>","title":"Implementations"},{"location":"processor/StateManagerUtil/","text":"","title":"StateManagerUtil"},{"location":"processor/StateManagerUtil/#registerstatestores","text":"","title":"registerStateStores <pre><code>void registerStateStores(\n  Logger log,\n  String logPrefix,\n  ProcessorTopology topology,\n  ProcessorStateManager stateMgr,\n  StateDirectory stateDirectory,\n  InternalProcessorContext processorContext)\n</code></pre> <p><code>registerStateStores</code>...FIXME</p> <p><code>registerStateStores</code>\u00a0is used when:</p> <ul> <li><code>StandbyTask</code> is requested to initializeIfNeeded</li> <li><code>StreamTask</code> is requested to initializeIfNeeded</li> </ul>"},{"location":"processor/StateStore/","text":"<p><code>StateStore</code> is an abstraction of storage engines (for the state of a stream processor).</p>","title":"StateStore"},{"location":"processor/StateStore/#contract","text":"","title":"Contract"},{"location":"processor/StateStore/#closing","text":"","title":"Closing <pre><code>void close()\n</code></pre>"},{"location":"processor/StateStore/#flushing","text":"","title":"Flushing <pre><code>void flush()\n</code></pre>"},{"location":"processor/StateStore/#initializing","text":"","title":"Initializing <pre><code>void init(\n  StateStoreContext context,\n  StateStore root)\n</code></pre>"},{"location":"processor/StateStore/#isopen","text":"","title":"isOpen <pre><code>boolean isOpen()\n</code></pre>"},{"location":"processor/StateStore/#name","text":"","title":"Name <pre><code>String name()\n</code></pre>"},{"location":"processor/StateStore/#persistent","text":"","title":"persistent <pre><code>boolean persistent()\n</code></pre> <p>Used when:</p> <ul> <li><code>GlobalStateManagerImpl</code> is created (and finds global non-persistent state stores)</li> <li><code>ProcessorStateManager</code> is requested to initializeStoreOffsetsFromCheckpoint and checkpoint</li> <li><code>ProcessorTopology</code> is requested to hasPersistentLocalStore and hasPersistentGlobalStore</li> <li><code>TimestampedKeyValueStoreBuilder</code> is requested to build a <code>TimestampedKeyValueStore</code></li> <li><code>TimestampedWindowStoreBuilder</code> is requested to build a <code>TimestampedWindowStore</code></li> <li>others</li> </ul>"},{"location":"processor/StateStore/#implementations","text":"<ul> <li>KeyValueStore</li> <li><code>SegmentedBytesStore</code></li> <li><code>SessionStore</code></li> <li><code>TimeOrderedKeyValueBuffer</code></li> <li>WindowStore</li> <li>WrappedStateStore</li> </ul>","title":"Implementations"},{"location":"processor/StateStoreFactory/","text":"<p><code>StateStoreFactory</code> is a factory of StateStores.</p> <pre><code>StateStoreFactory&lt;S extends StateStore&gt;\n</code></pre> <p><code>StateStoreFactory</code> is a <code>public static class</code> of InternalTopologyBuilder.</p>","title":"StateStoreFactory"},{"location":"processor/StateStoreFactory/#creating-instance","text":"<p><code>StateStoreFactory</code> takes the following to be created:</p> <ul> <li> StoreBuilder (of <code>S</code> StateStores)  <p><code>StateStoreFactory</code> is created\u00a0when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to addStateStore</li> </ul>","title":"Creating Instance"},{"location":"processor/StoreChangelogReader/","text":"<p><code>StoreChangelogReader</code> is...FIXME</p>","title":"StoreChangelogReader"},{"location":"processor/StreamTask/","text":"<p><code>StreamTask</code> is a concrete AbstractTask.</p>","title":"StreamTask"},{"location":"processor/StreamTask/#creating-instance","text":"<p><code>StreamTask</code> takes the following to be created:</p> <ul> <li> <code>TaskId</code> <li> Input <code>TopicPartition</code>s <li> <code>ProcessorTopology</code> <li> Main <code>Consumer&lt;byte[], byte[]&gt;</code> <li> StreamsConfig <li> <code>StreamsMetricsImpl</code> <li> <code>StateDirectory</code> <li> <code>ThreadCache</code> <li> <code>Time</code> <li> <code>ProcessorStateManager</code> <li> <code>RecordCollector</code> <li> <code>InternalProcessorContext</code> <li> <code>LogContext</code>  <p><code>StreamTask</code> is created\u00a0when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createActiveTask</li> <li><code>TopologyTestDriver</code> is requested to <code>setupTask</code></li> </ul>","title":"Creating Instance"},{"location":"processor/StreamTask/#preparecommit","text":"","title":"prepareCommit <pre><code>Map&lt;TopicPartition, OffsetAndMetadata&gt; prepareCommit()\n</code></pre> <p><code>prepareCommit</code>...FIXME</p> <p><code>prepareCommit</code>\u00a0is part of the Task abstraction.</p>"},{"location":"processor/StreamThread/","text":"<p><code>StreamThread</code> is a <code>Thread</code> (Java).</p>","title":"StreamThread"},{"location":"processor/StreamThread/#creating-instance","text":"<p><code>StreamThread</code> takes the following to be created:</p> <ul> <li> <code>Time</code> <li> StreamsConfig <li> <code>Admin</code> <li> Main <code>Consumer&lt;byte[], byte[]&gt;</code> <li> Restore <code>Consumer&lt;byte[], byte[]&gt;</code> <li> <code>ChangelogReader</code> <li> <code>originalReset</code> <li> TaskManager <li> <code>StreamsMetricsImpl</code> <li> <code>InternalTopologyBuilder</code> <li> Thread ID <li> <code>LogContext</code> <li> <code>assignmentErrorCode</code> <li> <code>nextProbingRebalanceMs</code> <li> Shutdown Error Hook <li> <code>java.util.function.Consumer&lt;Throwable&gt;</code> <li> <code>java.util.function.Consumer&lt;Long&gt;</code>  <p><code>StreamThread</code> is created\u00a0using create utility.</p>","title":"Creating Instance"},{"location":"processor/StreamThread/#commitintervalms","text":"","title":"commit.interval.ms <p><code>StreamThread</code> uses commit.interval.ms configuration property to control whether to commit tasks or not.</p>"},{"location":"processor/StreamThread/#creating-streamthread","text":"","title":"Creating StreamThread <pre><code>StreamThread create(\n  InternalTopologyBuilder builder,\n  StreamsConfig config,\n  KafkaClientSupplier clientSupplier,\n  Admin adminClient,\n  UUID processId,\n  String clientId,\n  StreamsMetricsImpl streamsMetrics,\n  Time time,\n  StreamsMetadataState streamsMetadataState,\n  long cacheSizeBytes,\n  StateDirectory stateDirectory,\n  StateRestoreListener userStateRestoreListener,\n  int threadIdx,\n  Runnable shutdownErrorHook,\n  java.util.function.Consumer&lt;Throwable&gt; streamsUncaughtExceptionHandler)\n</code></pre> <p><code>create</code> prints out the following INFO message to the logs:</p> <pre><code>Creating restore consumer client\n</code></pre> <p><code>create</code> requests the given <code>StreamsConfig</code> for the restore consumer configs (with getRestoreConsumerClientId) and requests the given KafkaClientSupplier for a restore consumer.</p> <p><code>create</code> creates a StoreChangelogReader.</p> <p><code>create</code> creates a ThreadCache.</p> <p><code>create</code> creates a ActiveTaskCreator, a StandbyTaskCreator and a TaskManager.</p> <p><code>create</code> prints out the following INFO message to the logs:</p> <pre><code>Creating consumer client\n</code></pre> <p><code>create</code>...FIXME</p> <p><code>create</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is requested to createAndAddStreamThread</li> </ul>"},{"location":"processor/StreamThread/#starting-execution","text":"","title":"Starting Execution <pre><code>void run()\n</code></pre> <p><code>run</code>...FIXME</p> <p><code>run</code>\u00a0is part of the <code>Thread</code> (Java) abstraction.</p>"},{"location":"processor/StreamThread/#runloop","text":"","title":"runLoop <pre><code>void runLoop()\n</code></pre> <p><code>runLoop</code>...FIXME</p>"},{"location":"processor/StreamThread/#runonce","text":"","title":"runOnce <pre><code>void runOnce()\n</code></pre> <p><code>runOnce</code>...FIXME</p>"},{"location":"processor/StreamThread/#maybecommit","text":"","title":"maybeCommit <pre><code>int maybeCommit()\n</code></pre> <p><code>maybeCommit</code> checks out whether to commit active and standby tasks (based on the last commit time and commit.interval.ms).</p> <p>If the last commit happened enough long ago, <code>maybeCommit</code> prints out the following DEBUG message to the logs:</p> <pre><code>Committing all active tasks [ids] and standby tasks [ids] since [time]ms has elapsed (commit interval is [time]ms)\n</code></pre> <p><code>maybeCommit</code> requests the TaskManager to commit the tasks that are <code>RUNNING</code> or <code>RESTORING</code>.</p> <p>If there were offsets committed, <code>maybeCommit</code> requests the TaskManager to maybePurgeCommittedRecords. Otherwise, <code>maybeCommit</code> prints out the following DEBUG message to the logs:</p> <pre><code>Unable to commit as we are in the middle of a rebalance, will try again when it completes.\n</code></pre> <p>If the last commit happened fairly recently, <code>maybeCommit</code> merely requests the TaskManager to maybeCommitActiveTasksPerUserRequested</p> <p>Either way, in the end, <code>maybeCommit</code> returns the number of committed offsets.</p>"},{"location":"processor/StreamThread/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.processor.internals.StreamThread</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.processor.internals.StreamThread=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"processor/Task/","text":"<p><code>Task</code> is an abstraction of tasks.</p>","title":"Task"},{"location":"processor/Task/#contract","text":"","title":"Contract"},{"location":"processor/Task/#addrecords","text":"","title":"addRecords <pre><code>void addRecords(\n  TopicPartition partition,\n  Iterable&lt;ConsumerRecord&lt;byte[], byte[]&gt;&gt; records)\n</code></pre> <p>Used when:</p> <ul> <li><code>TaskManager</code> is requested to addRecordsToTasks</li> <li><code>TopologyTestDriver</code> is requested to enqueueTaskRecord</li> </ul>"},{"location":"processor/Task/#changelogoffsets","text":"","title":"changelogOffsets <pre><code>Map&lt;TopicPartition, Long&gt; changelogOffsets()\n</code></pre>"},{"location":"processor/Task/#changelogpartitions","text":"","title":"changelogPartitions <pre><code>Collection&lt;TopicPartition&gt; changelogPartitions()\n</code></pre>"},{"location":"processor/Task/#cleartasktimeout","text":"","title":"clearTaskTimeout <pre><code>void clearTaskTimeout()\n</code></pre>"},{"location":"processor/Task/#closeclean","text":"","title":"closeClean <pre><code>void closeClean()\n</code></pre>"},{"location":"processor/Task/#closecleanandrecyclestate","text":"","title":"closeCleanAndRecycleState <pre><code>void closeCleanAndRecycleState()\n</code></pre>"},{"location":"processor/Task/#closedirty","text":"","title":"closeDirty <pre><code>void closeDirty()\n</code></pre>"},{"location":"processor/Task/#commitneeded","text":"","title":"commitNeeded <pre><code>boolean commitNeeded()\n</code></pre>"},{"location":"processor/Task/#committedoffsets","text":"","title":"committedOffsets <pre><code>Map&lt;TopicPartition, Long&gt; committedOffsets()\n</code></pre>"},{"location":"processor/Task/#completerestoration","text":"","title":"completeRestoration <pre><code>void completeRestoration(\n  java.util.function.Consumer&lt;Set&lt;TopicPartition&gt;&gt; offsetResetter)\n</code></pre>"},{"location":"processor/Task/#getstore","text":"","title":"getStore <pre><code>StateStore getStore(\n  String name)\n</code></pre> <p>Used when:</p> <ul> <li><code>StreamThreadStateStoreProvider</code> is requested for stores</li> </ul>"},{"location":"processor/Task/#highwatermark","text":"","title":"highWaterMark <pre><code>Map&lt;TopicPartition, Long&gt; highWaterMark()\n</code></pre>"},{"location":"processor/Task/#taskid","text":"","title":"TaskId <pre><code>TaskId id()\n</code></pre>"},{"location":"processor/Task/#initializeifneeded","text":"","title":"initializeIfNeeded <pre><code>void initializeIfNeeded()\n</code></pre>"},{"location":"processor/Task/#inputpartitions","text":"","title":"inputPartitions <pre><code>Set&lt;TopicPartition&gt; inputPartitions()\n</code></pre>"},{"location":"processor/Task/#isactive","text":"","title":"isActive <pre><code>boolean isActive()\n</code></pre>"},{"location":"processor/Task/#markchangelogascorrupted","text":"","title":"markChangelogAsCorrupted <pre><code>void markChangelogAsCorrupted(\n  Collection&lt;TopicPartition&gt; partitions)\n</code></pre>"},{"location":"processor/Task/#markchangelogascorrupted_1","text":"","title":"markChangelogAsCorrupted <pre><code>void maybeInitTaskTimeoutOrThrow(\n  long currentWallClockMs,\n  Exception cause)\n</code></pre>"},{"location":"processor/Task/#postcommit","text":"","title":"postCommit <pre><code>void postCommit(\n  boolean enforceCheckpoint)\n</code></pre>"},{"location":"processor/Task/#preparecommit","text":"","title":"prepareCommit <pre><code>Map&lt;TopicPartition, OffsetAndMetadata&gt; prepareCommit()\n</code></pre> <p>Used when:</p> <ul> <li><code>TaskManager</code> is requested to closeDirtyAndRevive, handleCloseAndRecycle, prepareCommitAndAddOffsetsToMap, closeTaskDirty, tryCloseCleanAllActiveTasks, tryCloseCleanAllStandbyTasks and commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</li> <li><code>TopologyTestDriver</code> is requested to completeAllProcessableWork, advanceWallClockTime and close</li> </ul>"},{"location":"processor/Task/#resume","text":"","title":"resume <pre><code>void resume()\n</code></pre>"},{"location":"processor/Task/#revive","text":"","title":"revive <pre><code>void revive()\n</code></pre>"},{"location":"processor/Task/#state","text":"","title":"state <pre><code>State state()\n</code></pre>"},{"location":"processor/Task/#suspend","text":"","title":"suspend <pre><code>void suspend()\n</code></pre>"},{"location":"processor/Task/#timecurrentidlingstarted","text":"","title":"timeCurrentIdlingStarted <pre><code>Optional&lt;Long&gt; timeCurrentIdlingStarted()\n</code></pre>"},{"location":"processor/Task/#updateinputpartitions","text":"","title":"updateInputPartitions <pre><code>void updateInputPartitions(\n  Set&lt;TopicPartition&gt; topicPartitions,\n  Map&lt;String, List&lt;String&gt;&gt; allTopologyNodesToSourceTopics)\n</code></pre>"},{"location":"processor/Task/#implementations","text":"<ul> <li>AbstractTask</li> <li>StandbyTask</li> <li>StreamTask</li> </ul>","title":"Implementations"},{"location":"processor/TaskId/","text":"<p><code>TaskId</code> is...FIXME</p>","title":"TaskId"},{"location":"processor/TaskManager/","text":"","title":"TaskManager"},{"location":"processor/TaskManager/#creating-instance","text":"<p><code>TaskManager</code> takes the following to be created:</p> <ul> <li> <code>Time</code> <li> <code>ChangelogReader</code> <li> Process UUID <li> Log Prefix <li> <code>StreamsMetricsImpl</code> <li> ActiveTaskCreator <li> StandbyTaskCreator <li> <code>InternalTopologyBuilder</code> <li> <code>Admin</code> <li> <code>StateDirectory</code> <li> <code>StreamThread.ProcessingMode</code>  <p><code>TaskManager</code> is created\u00a0when:</p> <ul> <li><code>StreamThread</code> utility is used to create a StreamThread</li> </ul>","title":"Creating Instance"},{"location":"processor/TaskManager/#committing-active-tasks","text":"","title":"Committing (Active) Tasks <pre><code>int commit(\n  Collection&lt;Task&gt; tasksToCommit)\n</code></pre> <p><code>commit</code> commitAndFillInConsumedOffsetsAndMetadataPerTaskMap the given tasks.</p> <p>In the end, <code>commit</code> returns consumed offsets and metadata per every committed task (<code>Map&lt;Task, Map&lt;TopicPartition, OffsetAndMetadata&gt;&gt;</code>).</p> <p><code>commit</code>\u00a0is used when:</p> <ul> <li><code>StreamThread</code> is requested to maybeCommit</li> <li><code>TaskManager</code> is requested to maybeCommitActiveTasksPerUserRequested</li> </ul>"},{"location":"processor/TaskManager/#handleassignment","text":"","title":"handleAssignment <pre><code>void handleAssignment(\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; activeTasks,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; standbyTasks)\n</code></pre> <p><code>handleAssignment</code>...FIXME</p> <p><code>handleAssignment</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to onAssignment</li> </ul>"},{"location":"processor/TaskManager/#handlecloseandrecycle","text":"","title":"handleCloseAndRecycle <pre><code>void handleCloseAndRecycle(\n  Set&lt;Task&gt; tasksToRecycle,\n  Set&lt;Task&gt; tasksToCloseClean,\n  Set&lt;Task&gt; tasksToCloseDirty,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; activeTasksToCreate,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; standbyTasksToCreate,\n  LinkedHashMap&lt;TaskId, RuntimeException&gt; taskCloseExceptions)\n</code></pre> <p><code>handleCloseAndRecycle</code>...FIXME</p>"},{"location":"processor/TaskManager/#handling-taskcorruptedexception","text":"","title":"Handling TaskCorruptedException <pre><code>void handleCorruption(\n  Set&lt;TaskId&gt; corruptedTasks)\n</code></pre> <p><code>handleCorruption</code>...FIXME</p> <p><code>handleCorruption</code> is used when:</p> <ul> <li><code>StreamThread</code> is requested to runLoop (and caught a <code>TaskCorruptedException</code>)</li> </ul>"},{"location":"processor/TaskManager/#maybecommitactivetasksperuserrequested","text":"","title":"maybeCommitActiveTasksPerUserRequested <pre><code>int maybeCommitActiveTasksPerUserRequested()\n</code></pre> <p>With rebalance in progress, <code>maybeCommitActiveTasksPerUserRequested</code> returns <code>-1</code> immediately.</p> <p>Otherwise, <code>maybeCommitActiveTasksPerUserRequested</code> finds tasks (among active tasks) with commitRequested or commitNeeded and, if there is at least one, commits them.</p> <p><code>maybeCommitActiveTasksPerUserRequested</code> is used when:</p> <ul> <li><code>StreamThread</code> is requested to maybeCommit</li> </ul>"},{"location":"processor/TaskManager/#commitandfillinconsumedoffsetsandmetadatapertaskmap","text":"","title":"commitAndFillInConsumedOffsetsAndMetadataPerTaskMap <pre><code>int commitAndFillInConsumedOffsetsAndMetadataPerTaskMap(\n  Collection&lt;Task&gt; tasksToCommit,\n  Map&lt;Task, Map&lt;TopicPartition, OffsetAndMetadata&gt;&gt; consumedOffsetsAndMetadataPerTask)\n</code></pre> <p>With rebalance in progress, <code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> returns <code>-1</code> immediately.</p> <p><code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> requests every Task with commitNeeded (in the given <code>tasksToCommit</code> tasks) to prepareCommit (that gives offsets and metadata per partition). <code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> saves the offsets and metadata per partition for a task (that is active) in the given <code>consumedOffsetsAndMetadataPerTask</code>.</p> <p><code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> commitOffsetsOrTransaction (with the given <code>consumedOffsetsAndMetadataPerTask</code> that may have been updated with some active tasks as described above).</p> <p>Once again, <code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> requests every Task with commitNeeded (in the given <code>tasksToCommit</code> tasks) to clearTaskTimeout and postCommit (with <code>enforceCheckpoint</code> flag disabled).</p> <p>In the end, <code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> returns the number of tasks committed.</p> <p><code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> is used when:</p> <ul> <li><code>TaskManager</code> is requested to commit and handle a TaskCorruptedException</li> </ul>"},{"location":"processor/TaskManager/#partition-rebalancing","text":"","title":"Partition Rebalancing"},{"location":"processor/TaskManager/#rebalanceinprogress-flag","text":"","title":"rebalanceInProgress Flag <p><code>TaskManager</code> uses <code>rebalanceInProgress</code> internal flag to indicate that it is in the middle of partition rebalancing (which is considered not safe to commit and used to skip commitAndFillInConsumedOffsetsAndMetadataPerTaskMap and maybeCommitActiveTasksPerUserRequested).</p> <p>The <code>rebalanceInProgress</code> flag is disabled (<code>false</code>) initially. It is turned on (<code>true</code>) in handleRebalanceStart and off in handleRebalanceComplete.</p>"},{"location":"processor/TaskManager/#isrebalanceinprogress","text":"","title":"isRebalanceInProgress <pre><code>boolean isRebalanceInProgress()\n</code></pre> <p><code>isRebalanceInProgress</code> returns the value of the internal rebalanceInProgress flag.</p> <p><code>isRebalanceInProgress</code>\u00a0is used when:</p> <ul> <li><code>StreamThread</code> is requested to run</li> </ul>"},{"location":"processor/TaskManager/#handlerebalancestart","text":"","title":"handleRebalanceStart <pre><code>void handleRebalanceStart(\n  Set&lt;String&gt; subscribedTopics)\n</code></pre> <p><code>handleRebalanceStart</code> requests the InternalTopologyBuilder to addSubscribedTopicsFromMetadata with the given <code>subscribedTopics</code>.</p> <p><code>handleRebalanceStart</code> tryToLockAllNonEmptyTaskDirectories and turns the rebalanceInProgress internal flag on (<code>true</code>).</p> <p><code>handleRebalanceStart</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to handleRebalanceStart</li> </ul>"},{"location":"processor/TaskManager/#handlerebalancecomplete","text":"","title":"handleRebalanceComplete <pre><code>void handleRebalanceComplete()\n</code></pre> <p><code>handleRebalanceComplete</code> requests the Consumer to pause (suspend) fetching from the partitions that are assigned to this consumer.</p> <p><code>handleRebalanceComplete</code> releaseLockedUnassignedTaskDirectories and turns the rebalanceInProgress internal flag off (<code>false</code>).</p> <p><code>handleRebalanceComplete</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to onPartitionsAssigned</li> </ul>"},{"location":"processor/Tasks/","text":"","title":"Tasks"},{"location":"processor/Tasks/#createtasks","text":"","title":"createTasks <pre><code>void createTasks(\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; activeTasksToCreate,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; standbyTasksToCreate)\n</code></pre> <p><code>createTasks</code>...FIXME</p> <p><code>createTasks</code>\u00a0is used when:</p> <ul> <li><code>TaskManager</code> is requested to handleAssignment</li> </ul>"},{"location":"processor/Tasks/#convertstandbytoactive","text":"","title":"convertStandbyToActive <pre><code>void convertStandbyToActive(\n  StandbyTask standbyTask,\n  Set&lt;TopicPartition&gt; partitions)\n</code></pre> <p><code>convertStandbyToActive</code>...FIXME</p> <p><code>convertStandbyToActive</code>\u00a0is used when:</p> <ul> <li><code>TaskManager</code> is requested to handleCloseAndRecycle</li> </ul>"},{"location":"processor/TimestampExtractor/","text":"<p><code>TimestampExtractor</code> is...FIXME</p>","title":"TimestampExtractor"},{"location":"state/AbstractStoreBuilder/","text":"<p><code>AbstractStoreBuilder</code> is...FIXME</p>","title":"AbstractStoreBuilder"},{"location":"state/CachedStateStore/","text":"<p><code>CachedStateStore</code> is an abstraction of cached state stores.</p>","title":"CachedStateStore"},{"location":"state/CachedStateStore/#contract","text":"","title":"Contract"},{"location":"state/CachedStateStore/#flushcache","text":"","title":"flushCache <pre><code>void flushCache()\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorStateManager</code> is requested to flush store caches</li> </ul>"},{"location":"state/CachedStateStore/#setflushlistener","text":"","title":"setFlushListener <pre><code>boolean setFlushListener(\n  CacheFlushListener&lt;K, V&gt; listener,\n  boolean sendOldValues)\n</code></pre>"},{"location":"state/CachedStateStore/#implementations","text":"<ul> <li><code>CachingKeyValueStore</code></li> <li><code>CachingSessionStore</code></li> <li><code>CachingWindowStore</code></li> <li>WrappedStateStore</li> </ul>","title":"Implementations"},{"location":"state/InMemoryWindowBytesStoreSupplier/","text":"<p><code>InMemoryWindowBytesStoreSupplier</code> is a WindowBytesStoreSupplier.</p>","title":"InMemoryWindowBytesStoreSupplier"},{"location":"state/InMemoryWindowBytesStoreSupplier/#creating-instance","text":"<p><code>InMemoryWindowBytesStoreSupplier</code> takes the following to be created:</p> <ul> <li> Name <li> <code>retentionPeriod</code> <li> <code>windowSize</code> <li> <code>retainDuplicates</code>  <p><code>InMemoryWindowBytesStoreSupplier</code> is created\u00a0when:</p> <ul> <li><code>Stores</code> is requested for in-memory window store</li> </ul>","title":"Creating Instance"},{"location":"state/KeyValueStore/","text":"<p><code>KeyValueStore&lt;K, V&gt;</code>\u00a0is an extension of the StateStore and ReadOnlyKeyValueStore abstractions for read-only key-value stores.</p>","title":"KeyValueStore"},{"location":"state/KeyValueStore/#contract","text":"","title":"Contract"},{"location":"state/KeyValueStore/#delete","text":"","title":"delete <pre><code>V delete(\n  K key)\n</code></pre>"},{"location":"state/KeyValueStore/#put","text":"","title":"put <pre><code>void put(\n  K key,\n  V value)\n</code></pre>"},{"location":"state/KeyValueStore/#putall","text":"","title":"putAll <pre><code>void putAll(\n  List&lt;KeyValue&lt;K, V&gt;&gt; entries)\n</code></pre>"},{"location":"state/KeyValueStore/#putifabsent","text":"","title":"putIfAbsent <pre><code>V putIfAbsent(\n  K key,\n  V value)\n</code></pre>"},{"location":"state/KeyValueStore/#implementations","text":"<ul> <li><code>CachingKeyValueStore</code></li> <li><code>ChangeLoggingKeyValueBytesStore</code></li> <li><code>InMemoryKeyValueStore</code></li> <li><code>InMemoryTimestampedKeyValueStoreMarker</code></li> <li><code>KeyValueStoreFacade</code></li> <li><code>KeyValueStoreReadOnlyDecorator</code></li> <li><code>KeyValueStoreReadWriteDecorator</code></li> <li><code>KeyValueToTimestampedKeyValueByteStoreAdapter</code></li> <li><code>MemoryLRUCache</code></li> <li><code>MeteredKeyValueStore</code></li> <li><code>RocksDBStore</code></li> <li><code>Segment</code></li> <li><code>TimestampedKeyValueStore</code></li> </ul>","title":"Implementations"},{"location":"state/OffsetCheckpoint/","text":"<p><code>OffsetCheckpoint</code> is...FIXME</p>","title":"OffsetCheckpoint"},{"location":"state/QueryableStoreProvider/","text":"<p><code>QueryableStoreProvider</code> is...FIXME</p>","title":"QueryableStoreProvider"},{"location":"state/ReadOnlyKeyValueStore/","text":"<p><code>ReadOnlyKeyValueStore</code> is...FIXME</p>","title":"ReadOnlyKeyValueStore"},{"location":"state/StateSerdes/","text":"<p><code>StateSerdes&lt;K, V&gt;</code> is a factory for creating serializers and deserializers for state stores in Kafka Streams.</p>","title":"StateSerdes"},{"location":"state/StateSerdes/#demo","text":"<pre><code>import org.apache.kafka.streams.state.StateSerdes\nimport java.lang.{Long =&gt; JLong}\nval stateSerdes = StateSerdes.withBuiltinTypes[JLong, String](\"topicName\", classOf[JLong], classOf[String])\n</code></pre> <pre><code>scala&gt; :type stateSerdes\norg.apache.kafka.streams.state.StateSerdes[Long,String]\n</code></pre>","title":"Demo"},{"location":"state/StateSerdes/#creating-instance","text":"<p><code>StateSerdes</code> takes the following to be created:</p> <ul> <li> Topic Name <li> Key <code>Serde</code> <li> Value <code>Serde</code>  <p><code>StateSerdes</code> is created\u00a0when:</p> <ul> <li><code>CachingWindowStore</code> is requested to <code>initInternal</code></li> <li><code>MeteredKeyValueStore</code> is requested to <code>initStoreSerde</code></li> <li><code>MeteredSessionStore</code> is requested to <code>initStoreSerde</code></li> <li><code>MeteredWindowStore</code> is requested to <code>initStoreSerde</code></li> <li>withBuiltinTypes</li> </ul>","title":"Creating Instance"},{"location":"state/StateSerdes/#withbuiltintypes","text":"","title":"withBuiltinTypes <pre><code>StateSerdes&lt;K, V&gt; withBuiltinTypes(\n  String topic,\n  Class&lt;K&gt; keyClass,\n  Class&lt;V&gt; valueClass)\n</code></pre> <p><code>withBuiltinTypes</code> creates a StateSerdes using <code>Serdes.serdeFrom</code> utility with the given key and value classes.</p>"},{"location":"state/StoreBuilder/","text":"<p><code>StoreBuilder</code> is an abstraction of builders of StateStores (with optional caching and logging).</p> <pre><code>StoreBuilder&lt;T extends StateStore&gt;\n</code></pre>","title":"StoreBuilder"},{"location":"state/StoreBuilder/#contract","text":"","title":"Contract"},{"location":"state/StoreBuilder/#building-statestore","text":"","title":"Building StateStore <pre><code>T build()\n</code></pre> <p>Used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to rewriteTopology (and build global state stores)</li> <li><code>StateStoreFactory</code> is requested to build a StateStore</li> </ul>"},{"location":"state/StoreBuilder/#logconfig","text":"","title":"logConfig <pre><code>Map&lt;String, String&gt; logConfig()\n</code></pre>"},{"location":"state/StoreBuilder/#loggingenabled","text":"","title":"loggingEnabled <pre><code>boolean loggingEnabled()\n</code></pre>"},{"location":"state/StoreBuilder/#name","text":"","title":"name <pre><code>String name()\n</code></pre>"},{"location":"state/StoreBuilder/#withcachingdisabled","text":"","title":"withCachingDisabled <pre><code>StoreBuilder&lt;T&gt; withCachingDisabled()\n</code></pre>"},{"location":"state/StoreBuilder/#withcachingenabled","text":"","title":"withCachingEnabled <pre><code>StoreBuilder&lt;T&gt; withCachingEnabled()\n</code></pre>"},{"location":"state/StoreBuilder/#withloggingdisabled","text":"","title":"withLoggingDisabled <pre><code>StoreBuilder&lt;T&gt; withLoggingDisabled()\n</code></pre>"},{"location":"state/StoreBuilder/#withloggingenabled","text":"","title":"withLoggingEnabled <pre><code>StoreBuilder&lt;T&gt; withLoggingEnabled(\n  Map&lt;String, String&gt; config)\n</code></pre>"},{"location":"state/StoreBuilder/#implementations","text":"<ul> <li>AbstractStoreBuilder</li> </ul>","title":"Implementations"},{"location":"state/StoreSupplier/","text":"<p><code>StoreSupplier</code> is an abstraction of suppliers of StateStores.</p>","title":"StoreSupplier"},{"location":"state/StoreSupplier/#contract","text":"","title":"Contract"},{"location":"state/StoreSupplier/#creating-statestore","text":"","title":"Creating StateStore <pre><code>T get()\n</code></pre> <p>Used when:</p> <ul> <li><code>KStreamImplJoin</code> is requested to <code>sharedOuterJoinWindowStoreBuilder</code></li> <li><code>KeyValueStoreBuilder</code> is requested to <code>build</code></li> <li><code>SessionStoreBuilder</code> is requested to <code>build</code></li> <li><code>TimeOrderedWindowStoreBuilder</code> is requested to <code>build</code></li> <li><code>TimestampedKeyValueStoreBuilder</code> is requested to <code>build</code></li> <li><code>TimestampedWindowStoreBuilder</code> is requested to <code>build</code></li> <li><code>WindowStoreBuilder</code> is requested to <code>build</code></li> </ul>"},{"location":"state/StoreSupplier/#metricsscope","text":"","title":"metricsScope <pre><code>String metricsScope()\n</code></pre>"},{"location":"state/StoreSupplier/#name","text":"","title":"name <pre><code>String name()\n</code></pre>"},{"location":"state/StoreSupplier/#implementations","text":"<ul> <li>KeyValueBytesStoreSupplier</li> <li>SessionBytesStoreSupplier</li> <li>WindowBytesStoreSupplier</li> </ul>","title":"Implementations"},{"location":"state/Stores/","text":"<p><code>Stores</code> is a factory for creating state stores in Kafka Streams.</p>","title":"Stores"},{"location":"state/Stores/#inmemorywindowstore","text":"","title":"inMemoryWindowStore <pre><code>WindowBytesStoreSupplier inMemoryWindowStore(\n  String name,\n  Duration retentionPeriod,\n  Duration windowSize,\n  boolean retainDuplicates)\n</code></pre> <p><code>inMemoryWindowStore</code>...FIXME</p> <p><code>inMemoryWindowStore</code>\u00a0is used when:</p> <ul> <li><code>KStreamImplJoin</code> is requested to <code>sharedOuterJoinWindowStoreBuilder</code> (for left outer join)</li> </ul>"},{"location":"state/StreamThreadStateStoreProvider/","text":"<p><code>StreamThreadStateStoreProvider</code> is...FIXME</p>","title":"StreamThreadStateStoreProvider"},{"location":"state/ThreadCache/","text":"<p><code>ThreadCache</code> is...FIXME</p>","title":"ThreadCache"},{"location":"state/TimestampedKeyValueStore/","text":"<p><code>TimestampedKeyValueStore</code> is...FIXME</p>","title":"TimestampedKeyValueStore"},{"location":"state/ValueAndTimestamp/","text":"<p><code>ValueAndTimestamp</code> is...FIXME</p>","title":"ValueAndTimestamp"},{"location":"state/WindowBytesStoreSupplier/","text":"<p><code>WindowBytesStoreSupplier</code>\u00a0is an extension of the StoreSupplier abstraction for state store suppliers of WindowStores (<code>WindowStore&lt;Bytes, byte[]&gt;</code>s).</p>","title":"WindowBytesStoreSupplier"},{"location":"state/WindowBytesStoreSupplier/#contract","text":"","title":"Contract"},{"location":"state/WindowBytesStoreSupplier/#retainduplicates","text":"","title":"retainDuplicates <pre><code>boolean retainDuplicates()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#retentionperiod","text":"","title":"retentionPeriod <pre><code>long retentionPeriod()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#segmentintervalms","text":"","title":"segmentIntervalMs <pre><code>long segmentIntervalMs()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#windowsize","text":"","title":"windowSize <pre><code>long windowSize()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#implementations","text":"<ul> <li>RocksDbWindowBytesStoreSupplier</li> <li>InMemoryWindowBytesStoreSupplier</li> </ul>","title":"Implementations"},{"location":"state/WindowStore/","text":"<p><code>WindowStore</code> is...FIXME</p>","title":"WindowStore"},{"location":"state/WrappedStateStore/","text":"<p><code>WrappedStateStore</code> is an extension of the StateStore and CachedStateStore abstractions for state stores that hold (wrap) another state store.</p>","title":"WrappedStateStore"},{"location":"state/WrappedStateStore/#implementations","text":"<ul> <li><code>AbstractReadOnlyDecorator</code></li> <li><code>AbstractReadWriteDecorator</code></li> <li><code>CachingKeyValueStore</code></li> <li><code>CachingSessionStore</code></li> <li><code>CachingWindowStore</code></li> <li><code>ChangeLoggingKeyValueBytesStore</code></li> <li><code>ChangeLoggingSessionBytesStore</code></li> <li><code>ChangeLoggingWindowBytesStore</code></li> <li><code>MeteredKeyValueStore</code></li> <li><code>MeteredSessionStore</code></li> <li><code>MeteredWindowStore</code></li> <li><code>RocksDBSessionStore</code></li> <li><code>RocksDBTimeOrderedWindowStore</code></li> <li><code>RocksDBWindowStore</code></li> </ul>","title":"Implementations"},{"location":"state/WrappedStateStore/#creating-instance","text":"<p><code>WrappedStateStore</code> takes the following to be created:</p> <ul> <li> StateStore  Abstract Class<p><code>WrappedStateStore</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete WrappedStateStores.</p>","title":"Creating Instance"},{"location":"state/WrappedStateStore/#setflushlistener","text":"","title":"setFlushListener <pre><code>boolean setFlushListener(\n  CacheFlushListener&lt;K, V&gt; listener,\n  boolean sendOldValues)\n</code></pre> <p><code>setFlushListener</code> returns <code>false</code> for the wrapped state store being of any type but a CachedStateStore.</p> <p>Otherwise, <code>setFlushListener</code> returns the value of requesting the CachedStateStore to setFlushListener.</p> <p><code>setFlushListener</code>\u00a0is part of the CachedStateStore abstraction.</p>"}]}