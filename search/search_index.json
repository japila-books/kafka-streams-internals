{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"<p>Welcome to The Internals of Kafka Streams online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark, Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Kafka Streams as much as I have.</p>  <p>Flannery O'Connor</p> <p>I write to discover what I know.</p>  \"The Internals Of\" series<p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p>  <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Kafka Streams \ud83d\udd25</p>  <p>Last update: 2021-10-15</p>","title":"The Internals of Kafka Streams 3.0.0"},{"location":"KafkaClientSupplier/","text":"<p><code>KafkaClientSupplier</code> is...FIXME</p>","title":"KafkaClientSupplier"},{"location":"KafkaStreams/","text":"<p><code>KafkaStreams</code> is the execution environment of a Kafka Streams application.</p> <p><code>KafkaStreams</code> is a Kafka client for continuous stream processing (on input coming from one or more input topics and sending output to zero, one, or more output topics).</p>","title":"KafkaStreams"},{"location":"KafkaStreams/#creating-instance","text":"<p><code>KafkaStreams</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder (or Topology) <li> StreamsConfig <li> KafkaClientSupplier (default: <code>DefaultKafkaClientSupplier</code>) <li> <code>Time</code>  <p>When created, <code>KafkaStreams</code> requests the given InternalTopologyBuilder to rewriteTopology followed by building a task and global task topologies.</p> <p><code>KafkaStreams</code> then...FIXME</p>","title":"Creating Instance"},{"location":"KafkaStreams/#defaultstreamsuncaughtexceptionhandler","text":"","title":"defaultStreamsUncaughtExceptionHandler <pre><code>void defaultStreamsUncaughtExceptionHandler(\n  Throwable throwable)\n</code></pre> <p><code>defaultStreamsUncaughtExceptionHandler</code>...FIXME</p>"},{"location":"KafkaStreams/#task-topology","text":"","title":"Task Topology <p><code>KafkaStreams</code> requests the InternalTopologyBuilder to build a task topology when created.</p> <p>The ProcessorTopology can have persistent local stores.</p>"},{"location":"KafkaStreams/#global-task-topology","text":"","title":"Global Task Topology <p>When created <code>KafkaStreams</code> requests the InternalTopologyBuilder to build a global task topology.</p>"},{"location":"KafkaStreams/#streamthreads","text":"","title":"StreamThreads <p><code>KafkaStreams</code> manages StreamThreads in a <code>threads</code> internal registry.</p> <p>The <code>threads</code> collection starts empty when <code>KafkaStreams</code> is created.</p> <p><code>KafkaStreams</code> adds a new <code>StreamThread</code> when requested to createAndAddStreamThread.</p> <p>A <code>StreamThread</code> is removed when <code>KafkaStreams</code> is requested for the following:</p> <ul> <li>defaultStreamsUncaughtExceptionHandler</li> <li>addStreamThread</li> <li>removeStreamThread</li> <li>getNumLiveStreamThreads</li> <li>getNextThreadIndex</li> </ul> <p><code>KafkaStreams</code> uses processStreamThread to work with the <code>StreamThread</code>s.</p>"},{"location":"KafkaStreams/#processstreamthread","text":"","title":"processStreamThread <pre><code>void processStreamThread(\n  java.util.function.Consumer&lt;StreamThread&gt; consumer)\n</code></pre> <p><code>processStreamThread</code>...FIXME</p>"},{"location":"KafkaStreams/#getnumlivestreamthreads","text":"","title":"getNumLiveStreamThreads <pre><code>int getNumLiveStreamThreads()\n</code></pre> <p><code>getNumLiveStreamThreads</code>...FIXME</p>"},{"location":"KafkaStreams/#globalstreamthread","text":"","title":"GlobalStreamThread <p><code>KafkaStreams</code> can use a GlobalStreamThread if...FIXME</p>"},{"location":"KafkaStreams/#starting-streams-client","text":"","title":"Starting Streams Client <pre><code>void start()\n</code></pre> <p><code>start</code> attempts to enter <code>REBALANCING</code> state and, if successful, prints out the following INFO message to the logs:</p> <pre><code>State transition from [oldState] to REBALANCING\n</code></pre> <p><code>start</code> prints out the following DEBUG message to the logs:</p> <pre><code>Starting Streams client\n</code></pre> <p><code>start</code> requests the GlobalStreamThread to start (if defined).</p> <p><code>start</code> requests all the StreamThreads to start.</p> <p><code>start</code>...FIXME</p>"},{"location":"KafkaStreams/#setuncaughtexceptionhandler","text":"","title":"setUncaughtExceptionHandler <pre><code>void setUncaughtExceptionHandler(\n  StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler)\n</code></pre> <p><code>setUncaughtExceptionHandler</code>...FIXME</p> <p><code>setUncaughtExceptionHandler</code>\u00a0is part of the public API.</p>"},{"location":"KafkaStreams/#handlestreamsuncaughtexception","text":"","title":"handleStreamsUncaughtException <pre><code>void handleStreamsUncaughtException(\n  Throwable throwable,\n  StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler)\n</code></pre> <p><code>handleStreamsUncaughtException</code>...FIXME</p> <p><code>handleStreamsUncaughtException</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is requested to setUncaughtExceptionHandler and defaultStreamsUncaughtExceptionHandler</li> </ul>"},{"location":"KafkaStreams/#replacestreamthread","text":"","title":"replaceStreamThread <pre><code>void replaceStreamThread(\n  Throwable throwable)\n</code></pre> <p><code>replaceStreamThread</code>...FIXME</p>"},{"location":"KafkaStreams/#addstreamthread","text":"","title":"addStreamThread <pre><code>Optional&lt;String&gt; addStreamThread()\n</code></pre> <p><code>addStreamThread</code>...FIXME</p> <p><code>addStreamThread</code> is part of the public API.</p>"},{"location":"KafkaStreams/#createandaddstreamthread","text":"","title":"createAndAddStreamThread <pre><code>StreamThread createAndAddStreamThread(\n  long cacheSizePerThread,\n  int threadIdx)\n</code></pre> <p><code>createAndAddStreamThread</code> creates a StreamThread and requests it to setStateListener with the StreamStateListener.</p> <p><code>createAndAddStreamThread</code> registers the <code>StreamThread</code> (in the threads and threadState internal registries).</p> <p><code>createAndAddStreamThread</code> requests the QueryableStoreProvider to addStoreProviderForThread (with the name of the <code>StreamThread</code> and a new <code>StreamThreadStateStoreProvider</code>).</p> <p><code>createAndAddStreamThread</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created and requested to addStreamThread</li> </ul>"},{"location":"KafkaStreams/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.KafkaStreams</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.KafkaStreams=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"StreamsConfig/","text":"","title":"StreamsConfig"},{"location":"StreamsConfig/#applicationid","text":"","title":"application.id"},{"location":"StreamsConfig/#cachemaxbytesbuffering","text":"","title":"cache.max.bytes.buffering"},{"location":"StreamsConfig/#pollms","text":"","title":"poll.ms <p>Time (in millis) to block waiting for input</p> <p>Default: <code>100L</code></p> <p>Used when:</p> <ul> <li><code>GlobalStateManagerImpl</code> is created</li> <li><code>GlobalStreamThread</code> is requested to initialize</li> <li><code>StoreChangelogReader</code> is created</li> <li><code>StreamThread</code> is created</li> </ul>"},{"location":"StreamsConfig/#tasktimeoutms","text":"","title":"task.timeout.ms"},{"location":"StreamsPartitionAssignor/","text":"<p><code>StreamsPartitionAssignor</code> is...FIXME</p>","title":"StreamsPartitionAssignor"},{"location":"Topology/","text":"<p><code>Topology</code> is a logical representation of a ProcessorTopology.</p> <p><code>Topology</code> is a facade to InternalTopologyBuilder (with all methods delegating to it).</p>","title":"Topology"},{"location":"Topology/#creating-instance","text":"<p><code>Topology</code> takes no arguments to be created.</p> <p><code>Topology</code> is a part of the public API of Kafka Streams and can be created directly or indirectly for StreamsBuilder.</p>","title":"Creating Instance"},{"location":"Topology/#internaltopologybuilder","text":"","title":"InternalTopologyBuilder <p><code>Topology</code> creates an InternalTopologyBuilder when created.</p>"},{"location":"Topology/#addglobalstore","text":"","title":"addGlobalStore <pre><code>&lt;KIn, VIn&gt; Topology addGlobalStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  String sourceName,\n  Deserializer&lt;KIn&gt; keyDeserializer,\n  Deserializer&lt;VIn&gt; valueDeserializer,\n  String topic,\n  String processorName,\n  ProcessorSupplier&lt;KIn, VIn, Void, Void&gt; stateUpdateSupplier) // (1)\n&lt;KIn, VIn&gt; Topology addGlobalStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  String sourceName,\n  TimestampExtractor timestampExtractor,\n  Deserializer&lt;KIn&gt; keyDeserializer,\n  Deserializer&lt;VIn&gt; valueDeserializer,\n  String topic,\n  String processorName,\n  ProcessorSupplier&lt;KIn, VIn, Void, Void&gt; stateUpdateSupplier)\n</code></pre> <ol> <li>Uses no TimestampExtractor</li> </ol> <p><code>addGlobalStore</code> requests the InternalTopologyBuilder to add a global store.</p>"},{"location":"Topology/#addprocessor","text":"","title":"addProcessor <pre><code>Topology addProcessor(\n  String name,\n  ProcessorSupplier&lt;KIn, VIn, KOut, VOut&gt; supplier,\n  String... parentNames)\n</code></pre> <p><code>addProcessor</code> requests the InternalTopologyBuilder to add a processor.</p> <p>If there are any state stores associated with the processor, <code>addProcessor</code> requests the InternalTopologyBuilder to add them.</p>"},{"location":"Topology/#demo","text":"","title":"Demo <pre><code>import org.apache.kafka.streams.Topology\nval topology = new Topology\n</code></pre>"},{"location":"TopologyTestDriver/","text":"<p><code>TopologyTestDriver</code> helps writing tests to verify behavior of topologies (created with Topology or StreamsBuilder).</p> <pre><code>import org.apache.kafka.streams.TopologyTestDriver\n</code></pre>","title":"TopologyTestDriver"},{"location":"TopologyTestDriver/#library-dependency","text":"","title":"Library Dependency <p><code>TopologyTestDriver</code> belongs to a separate module and has to be defined as a dependency in a build configuration (e.g. <code>build.sbt</code>).</p> <pre><code>val kafkaVersion = \"3.0.0\"\nlibraryDependencies += \"org.apache.kafka\" % \"kafka-streams-test-utils\" % kafkaVersion % Test\n</code></pre>"},{"location":"global-stores/","text":"<p>StreamsBuilder.addGlobalStore adds a global StateStore to a topology.</p> <p>Such a <code>StateStore</code> sources its data from all partitions of the provided input topic. This store uses the source topic as changelog (and during restore will insert records directly from the source).</p> <p>Global stores should not be added to <code>Processor</code>, <code>Transformer</code>, or <code>ValueTransformer</code> (unlike regular stores). They have read-only access to all global stores by default.</p> <p>There will be exactly one instance of this <code>StateStore</code> per Kafka Streams instance.</p> <p>A SourceNode will be added to consume the data arriving from the partitions of the input topic.</p>","title":"Global Stores"},{"location":"logging/","text":"","title":"Logging"},{"location":"logging/#log4jproperties","text":"","title":"log4j.properties <p>Use the following <code>log4j.properties</code> in <code>src/main/resources</code> in your Kafka Streams application's project.</p> <pre><code>log4j.rootLogger=INFO, stdout\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.target=System.out\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.logger.org.apache.kafka.streams.processor.internals.StreamThread=ALL\n</code></pre>"},{"location":"logging/#slf4j","text":"","title":"SLF4J <p>Kafka Streams uses Simple Logging Facade for Java (SLF4J) for logging.</p> <p>Use <code>slf4j-api</code> and <code>slf4j-log4j12</code> library dependencies in a Kafka Streams application (in <code>build.sbt</code>) for logging.</p> <pre><code>val slf4jVersion = \"2.0.0-alpha5\"\nlibraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4jVersion\nlibraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4jVersion\n</code></pre>"},{"location":"overview/","text":"<p>Kafka Streams is a library for developing applications for processing records from topics in Apache Kafka.</p> <p>Kafka Streams comes with high-level Streams DSL and low-level Processor API to describe a Topology that eventually is built as a ProcessorTopology.</p> <p>The execution environment of <code>ProcessorTopology</code> is KafkaStreams. Once created, the <code>KafkaStreams</code> instance is supposed to be started to start stream processing.</p>","title":"Kafka Streams\u2009\u2014\u2009Stream Processing Library on Apache Kafka"},{"location":"scala/","text":"<p>Scala API for Kafka Streams is a separate Kafka Streams module (a Scala library) that acts as a wrapper over the existing Java API for Kafka Streams.</p> <p>The Scala API for Kafka Streams defines Scala-friendly types that wrap the corresponding Kafka Streams types and simply delegate all method calls to the underlying Java object with the purpose of making it much more expressive, with less boilerplate and more succinct.</p>","title":"Scala API for Kafka Streams"},{"location":"scala/#scala-package","text":"","title":"scala Package <p>The Scala API is available in the <code>org.apache.kafka.streams.scala</code> package.</p> <pre><code>import org.apache.kafka.streams.scala._\nimport org.apache.kafka.streams.scala.kstream._\n</code></pre>"},{"location":"scala/#library-dependency","text":"","title":"Library Dependency <p>As a separate Scala library Scala API for Kafka Streams has to be defined as a dependency in a build configuration (e.g. <code>build.sbt</code>).</p> <pre><code>val kafkaVersion = \"3.0.0\"\nlibraryDependencies += \"org.apache.kafka\" %% \"kafka-streams-scala\" % kafkaVersion\n</code></pre>"},{"location":"scala/#implicit-conversions","text":"","title":"Implicit Conversions <p>The Scala API for Kafka Streams defines implicit conversions, i.e. <code>Serdes</code>, and <code>ImplicitConversions</code>.</p> <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n</code></pre>"},{"location":"scala/#consumed","text":"","title":"Consumed <p>The Scala API for Kafka Streams comes with Scala objects for creating Consumed, Produced, <code>Materialized</code> and other metadata-related instances with Serdes objects for the key and value types available in implicit scope.</p> <pre><code>import org.apache.kafka.streams.scala.kstream._\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/","text":"","title":"Demo: Developing Kafka Streams Application"},{"location":"demo/developing-kafka-streams-application/#build-topology-using-streamsbuilder","text":"","title":"Build Topology using StreamsBuilder <p>A Kafka Streams application requires a Topology that can be created directly or described (and built) indirectly using StreamsBuilder.</p> <pre><code>import org.apache.kafka.streams.scala.StreamsBuilder\nval streamBuilder = new StreamsBuilder\n</code></pre> <pre><code>import org.apache.kafka.streams.scala.ImplicitConversions._\nimport org.apache.kafka.streams.scala.serialization.Serdes._\n</code></pre> <pre><code>val records = streamBuilder.stream[String, String](topic = \"streams-demo-input\")\nrecords.to(topic = \"streams-demo-output\")\n</code></pre> <pre><code>import org.apache.kafka.streams.Topology\nval topology = streamBuilder.build()\n</code></pre> <p>A topology can be described.</p> <pre><code>println(topology.describe)\n</code></pre> <pre><code>Topologies:\n   Sub-topology: 0\n    Source: KSTREAM-SOURCE-0000000000 (topics: [streams-demo-input])\n      --&gt; KSTREAM-SINK-0000000001\n    Sink: KSTREAM-SINK-0000000001 (topic: streams-demo-output)\n      &lt;-- KSTREAM-SOURCE-0000000000\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#create-kafka-topics","text":"","title":"Create Kafka Topics <p>Kafka Streams requires that all input topics are available before it can be started (or <code>MissingSourceTopicException</code> is thrown).</p> <pre><code>./bin/kafka-topics.sh \\\n  --bootstrap-server :9092 \\\n  --create \\\n  --topic streams-demo-input \\\n  --partitions 1 \\\n  --replication-factor 1\n</code></pre> <pre><code>./bin/kafka-topics.sh \\\n  --bootstrap-server :9092 \\\n  --create \\\n  --topic streams-demo-output \\\n  --partitions 1 \\\n  --replication-factor 1\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#streamsconfig","text":"","title":"StreamsConfig <p>An execution environment of a Kafka Streams application is configured using StreamsConfig.</p> <pre><code>import org.apache.kafka.streams.StreamsConfig\nimport scala.jdk.CollectionConverters._\n// Only required configuration properties\n// And one more for demo purposes to slow processing to 15 secs\n// import java.util.concurrent.TimeUnit\nimport scala.concurrent.duration._\nval props = Map(\n  StreamsConfig.APPLICATION_ID_CONFIG -&gt; \"kafka-streams-demo\",\n  StreamsConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; \":9092\",\n  StreamsConfig.POLL_MS_CONFIG -&gt; 15.seconds.toMillis).asJava\nval config = new StreamsConfig(props)\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#kafkastreams","text":"","title":"KafkaStreams <p>The execution environment of a Kafka Stream application is KafkaStreams.</p> <pre><code>import org.apache.kafka.streams.KafkaStreams\nval streams = new KafkaStreams(topology, config)\n</code></pre> <p>Eventually, <code>KafkaStreams</code> should be started for the stream processing to be executed.</p> <pre><code>streams.start\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#kcat","text":"","title":"kcat <pre><code>kcat -P -b localhost -t streams-demo-input\n</code></pre> <pre><code>kcat -C -b localhost -t streams-demo-output\n</code></pre>"},{"location":"kstream/","text":"<p>Streams DSL (KStream DSL) is a high-level API for developers to define topologies in Kafka Streams.</p> <p>The entry point to the KStream DSL is StreamsBuilder.</p> <p>Main abstractions (for Kafka Streams developers):</p> <ul> <li>Consumed</li> <li>GlobalKTable</li> <li>KStream</li> <li>KTable</li> <li>Materialized</li> <li>Produced</li> <li>others</li> </ul> <p>A typical Kafka Streams application (that uses this Streams DSL and Scala API for Kafka Streams) looks as follows:</p> <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nval builder = new StreamsBuilder\n\n// Add a KStream if needed\n// K and V are the types of keys and values, accordingly\nbuilder.stream[K, V](...)\n\n// Add a KTable if needed\nbuilder.table[K, V](...)\n\n// Add a global store if needed\nbuilder.addGlobalStore(...)\n\n// Add a global store if needed\nbuilder.globalTable[K, V](...)\n\n// In the end, build a topology\nval topology = builder.build\n</code></pre>","title":"Streams DSL"},{"location":"kstream/AbstractStream/","text":"<p><code>AbstractStream</code> is...FIXME</p>","title":"AbstractStream"},{"location":"kstream/Aggregator/","text":"<p><code>Aggregator</code> is...FIXME</p>","title":"Aggregator"},{"location":"kstream/Consumed/","text":"<p><code>Consumed&lt;K, V&gt;</code> describes how to consume records in a topology in the High-Level KStream DSL for the following StreamsBuilder operators:</p> <ul> <li>StreamsBuilder.stream</li> <li>StreamsBuilder.table</li> <li>StreamsBuilder.globalTable</li> <li>StreamsBuilder.addGlobalStore</li> </ul> <p><code>Consumed&lt;K, V&gt;</code> is a NamedOperation.</p>","title":"Consumed \u2014 Metadata for Consuming Records"},{"location":"kstream/Consumed/#demo","text":"<pre><code>import org.apache.kafka.common.serialization.Serdes\nimport org.apache.kafka.streams.kstream.Consumed\nval consumed = Consumed.`with`(Serdes.Long, Serdes.String)\n</code></pre> <pre><code>scala&gt; :type consumed\norg.apache.kafka.streams.kstream.Consumed[Long,String]\n</code></pre>","title":"Demo"},{"location":"kstream/Consumed/#creating-instance","text":"<p><code>Consumed</code> takes the following to be created:</p> <ul> <li> <code>Serde&lt;K&gt;</code> of keys (Apache Kafka) <li> <code>Serde&lt;V&gt;</code> of values (Apache Kafka) <li> <code>TimestampExtractor</code> <li> Reset Policy (<code>Topology.AutoOffsetReset</code>) <li> Processor Name  <p><code>Consumed</code> is created\u00a0using the factories.</p>","title":"Creating Instance"},{"location":"kstream/Consumed/#creating-consumed","text":"","title":"Creating Consumed"},{"location":"kstream/Consumed/#as","text":"","title":"as <pre><code>Consumed&lt;K, V&gt; as(\n  String processorName)\n</code></pre>"},{"location":"kstream/Consumed/#with","text":"","title":"with <pre><code>Consumed&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde)\nConsumed&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde,\n  TimestampExtractor timestampExtractor,\n  Topology.AutoOffsetReset resetPolicy)\nConsumed&lt;K, V&gt; with(\n  TimestampExtractor timestampExtractor)\nConsumed&lt;K, V&gt; with(\n  Topology.AutoOffsetReset resetPolicy)\n</code></pre>"},{"location":"kstream/Consumed/#scala-api","text":"","title":"Scala API <p>Scala API for Kafka Streams makes the optional <code>Consumed</code> metadata an implicit parameter in the StreamsBuilder API.</p> <p>Moreover, <code>ImplicitConversions</code> object defines <code>consumedFromSerde</code> implicit method that creates a <code>Consumed</code> instance with the key and value <code>Serde</code> objects available in implicit scope.</p> <p>And the last but not least, Scala API for Kafka Streams defines <code>Consumed</code> object with <code>with</code> factory methods that use implicit key and value <code>Serde</code> objects.</p>"},{"location":"kstream/GlobalKTable/","text":"<p><code>GlobalKTable</code> is...FIXME</p>","title":"GlobalKTable"},{"location":"kstream/GraphNode/","text":"<p><code>GraphNode</code> is an abstraction of graph nodes (for InternalStreamsBuilder to build a topology for StreamsBuilder).</p>","title":"GraphNode"},{"location":"kstream/GraphNode/#contract","text":"","title":"Contract"},{"location":"kstream/GraphNode/#writetotopology","text":"","title":"writeToTopology <pre><code>void writeToTopology(\n  InternalTopologyBuilder topologyBuilder,\n  Properties props)\n</code></pre> <p>Used when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to build and optimize a topology</li> </ul>"},{"location":"kstream/GraphNode/#implementations","text":"<ul> <li>ProcessorGraphNode</li> <li>StreamToTableNode</li> <li>BaseJoinProcessorNode</li> <li>SourceGraphNode</li> <li>StreamSinkNode</li> <li>StateStoreNode</li> <li>TableProcessorNode</li> <li>BaseRepartitionNode</li> <li>StreamTableJoinNode</li> </ul>","title":"Implementations"},{"location":"kstream/GroupedStreamAggregateBuilder/","text":"<p><code>GroupedStreamAggregateBuilder</code> is...FIXME</p>","title":"GroupedStreamAggregateBuilder"},{"location":"kstream/Initializer/","text":"<p><code>Initializer</code> is...FIXME</p>","title":"Initializer"},{"location":"kstream/InternalStreamsBuilder/","text":"","title":"InternalStreamsBuilder"},{"location":"kstream/InternalStreamsBuilder/#creating-instance","text":"<p><code>InternalStreamsBuilder</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder  <p><code>InternalStreamsBuilder</code> is created\u00a0when:</p> <ul> <li><code>StreamsBuilder</code> is created</li> </ul>","title":"Creating Instance"},{"location":"kstream/InternalStreamsBuilder/#root-node","text":"","title":"Root Node <p><code>InternalStreamsBuilder</code> creates a root GraphNode when created.</p> <p>This root node is used to addGraphNode in the following high-level operators:</p> <ul> <li>stream</li> <li>table</li> <li>globalTable</li> <li>addStateStore</li> <li>addGlobalStore</li> </ul> <p>This root node is then used to build and optimize a topology (for StreamsBuilder).</p>"},{"location":"kstream/InternalStreamsBuilder/#buildandoptimizetopology","text":"","title":"buildAndOptimizeTopology <pre><code>void buildAndOptimizeTopology(\n  Properties props)\n</code></pre> <p><code>buildAndOptimizeTopology</code>...FIXME</p> <p><code>buildAndOptimizeTopology</code>\u00a0is used when:</p> <ul> <li><code>StreamsBuilder</code> is requested to build a topology</li> </ul>"},{"location":"kstream/InternalStreamsBuilder/#mergeduplicatesourcenodes","text":"","title":"mergeDuplicateSourceNodes <pre><code>void mergeDuplicateSourceNodes()\n</code></pre> <p><code>mergeDuplicateSourceNodes</code>...FIXME</p>"},{"location":"kstream/InternalStreamsBuilder/#adding-statestore-to-topology","text":"","title":"Adding StateStore to Topology <pre><code>void addStateStore(\n  StoreBuilder&lt;?&gt; builder)\n</code></pre> <p><code>addStateStore</code> adds a new StateStoreNode to the root node.</p> <p><code>addStateStore</code> is used when:</p> <ul> <li><code>StreamsBuilder</code> is requested to addStateStore</li> <li><code>KTableImpl</code> is requested to <code>doJoinOnForeignKey</code></li> </ul>"},{"location":"kstream/InternalStreamsBuilder/#stream","text":"","title":"stream <pre><code>KStream&lt;K, V&gt; stream(\n  Collection&lt;String&gt; topics,\n  ConsumedInternal&lt;K, V&gt; consumed)\nKStream&lt;K, V&gt; stream(\n  Pattern topicPattern,\n  ConsumedInternal&lt;K, V&gt; consumed)\n</code></pre> <p><code>stream</code>...FIXME</p> <p><code>stream</code>\u00a0is used when:</p> <ul> <li><code>StreamsBuilder</code> is requested to stream</li> </ul>"},{"location":"kstream/KGroupedStream/","text":"<p><code>KGroupedStream</code> is...FIXME</p>","title":"KGroupedStream"},{"location":"kstream/KGroupedStreamImpl/","text":"<p><code>KGroupedStreamImpl</code> is a KGroupedStream (and an AbstractStream).</p>","title":"KGroupedStreamImpl"},{"location":"kstream/KGroupedStreamImpl/#creating-instance","text":"<p><code>KGroupedStreamImpl</code> takes the following to be created:</p> <ul> <li> Name <li> Sub-Topology Source Nodes (Names) <li> <code>GroupedInternal&lt;K, V&gt;</code> <li>repartitionRequired flag</li> <li> GraphNode <li> InternalStreamsBuilder  <p><code>KGroupedStreamImpl</code> is created\u00a0when:</p> <ul> <li><code>KStreamImpl</code> is requested to groupBy and groupByKey</li> </ul>","title":"Creating Instance"},{"location":"kstream/KGroupedStreamImpl/#groupedstreamaggregatebuilder","text":"","title":"GroupedStreamAggregateBuilder <p><code>KGroupedStreamImpl</code> creates a GroupedStreamAggregateBuilder when created.</p>"},{"location":"kstream/KGroupedStreamImpl/#repartitionrequired-flag","text":"","title":"repartitionRequired Flag <p><code>KGroupedStreamImpl</code> is given a <code>repartitionRequired</code> flag when created.</p> <p>The <code>repartitionRequired</code> flag is always <code>true</code> for groupBy.</p>"},{"location":"kstream/KStream/","text":"<p><code>KStream&lt;K, V&gt;</code> is an abstraction of a stream of records (of key-value pairs).</p> <p><code>KStream</code> can be created directly from one or many Kafka topics (using StreamsBuilder.stream operators) or as a result of transformations on an existing <code>KStream</code> instance.</p> <p><code>KStream</code> offers a rich set of operators (KStream API) for building topologies to consume, process and produce (key-value) records.</p>","title":"KStream API \u2014 Stream of Records"},{"location":"kstream/KStream/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"kstream/KStream/#flatmap","text":"","title":"flatMap <pre><code>KStream&lt;KR, VR&gt; flatMap(\n  KeyValueMapper&lt;\n    ? super K,\n    ? super V,\n    ? extends Iterable&lt;? extends KeyValue&lt;? extends KR, ? extends VR&gt;&gt;&gt; mapper)\nKStream&lt;KR, VR&gt; flatMap(\n  KeyValueMapper&lt;\n    ? super K,\n    ? super V,\n    ? extends Iterable&lt;? extends KeyValue&lt;? extends KR, ? extends VR&gt;&gt;&gt; mapper,\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#foreach","text":"","title":"foreach <pre><code>void foreach(\n  ForeachAction&lt;? super K, ? super V&gt; action)\nvoid foreach(\n  ForeachAction&lt;? super K, ? super V&gt; action,\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#groupby","text":"","title":"groupBy <pre><code>KGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector)\nKGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector,\n  Grouped&lt;KR, V&gt; grouped)\n</code></pre>"},{"location":"kstream/KStream/#groupbykey","text":"","title":"groupByKey <pre><code>KGroupedStream&lt;K, V&gt; groupByKey()\nKGroupedStream&lt;K, V&gt; groupByKey(\n  Grouped&lt;K, V&gt; grouped)\n</code></pre>"},{"location":"kstream/KStream/#join","text":"","title":"join <pre><code>KStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoiner&lt;? super V, ? super GV, ? extends RV&gt; joiner)\nKStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoiner&lt;? super V, ? super GV, ? extends RV&gt; joiner,\n  Named named)\nKStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super GV, ? extends RV&gt; joiner)\nKStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super GV, ? extends RV&gt; joiner,\n  Named named)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoiner&lt;? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoiner&lt;? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K, V, VO&gt; streamJoined)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K, V, VO&gt; streamJoined)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoiner&lt;? super V, ? super VT, ? extends VR&gt; joiner)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoiner&lt;? super V, ? super VT, ? extends VR&gt; joiner,\n  Joined&lt;K, V, VT&gt; joined)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VT, ? extends VR&gt; joiner)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VT, ? extends VR&gt; joiner,\n  Joined&lt;K, V, VT&gt; joined)\n</code></pre>"},{"location":"kstream/KStream/#merge","text":"","title":"merge <pre><code>KStream&lt;K, V&gt; merge(\n  KStream&lt;K, V&gt; stream)\nKStream&lt;K, V&gt; merge(\n  KStream&lt;K, V&gt; stream,\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#peek","text":"","title":"peek <pre><code>KStream&lt;K, V&gt; peek(\n  ForeachAction&lt;? super K, ? super V&gt; action)\nKStream&lt;K, V&gt; peek(\n  ForeachAction&lt;? super K, ? super V&gt; action,\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#print","text":"","title":"print <pre><code>void print(\n  Printed&lt;K, V&gt; printed)\n</code></pre>"},{"location":"kstream/KStream/#process","text":"","title":"process <pre><code>void process(\n  ProcessorSupplier&lt;? super K, ? super V, Void, Void&gt; processorSupplier,\n  Named named,\n  String... stateStoreNames)\nvoid process(\n  ProcessorSupplier&lt;? super K, ? super V, Void, Void&gt; processorSupplier,\n  String... stateStoreNames)\n</code></pre>"},{"location":"kstream/KStream/#repartition","text":"","title":"repartition <pre><code>KStream&lt;K, V&gt; repartition()\nKStream&lt;K, V&gt; repartition(\n  Repartitioned&lt;K, V&gt; repartitioned)\n</code></pre>"},{"location":"kstream/KStream/#split","text":"","title":"split <pre><code>BranchedKStream&lt;K, V&gt; split()\nBranchedKStream&lt;K, V&gt; split(\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#to","text":"","title":"to <pre><code>void to(\n  String topic)\nvoid to(\n  String topic,\n  Produced&lt;K, V&gt; produced)\nvoid to(\n  TopicNameExtractor&lt;K, V&gt; topicExtractor)\nvoid to(\n  TopicNameExtractor&lt;K, V&gt; topicExtractor,\n  Produced&lt;K, V&gt; produced)\n</code></pre>"},{"location":"kstream/KStream/#totable","text":"","title":"toTable <pre><code>KTable&lt;K, V&gt; toTable()\nKTable&lt;K, V&gt; toTable(\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\nKTable&lt;K, V&gt; toTable(\n  Named named)\nKTable&lt;K, V&gt; toTable(\n  Named named,\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\n</code></pre>"},{"location":"kstream/KStream/#transform","text":"","title":"transform <pre><code>KStream&lt;K1, V1&gt; transform(\n  TransformerSupplier&lt;? super K, ? super V, KeyValue&lt;K1, V1&gt;&gt; transformerSupplier,\n  Named named,\n  String... stateStoreNames)\nKStream&lt;K1, V1&gt; transform(\n  TransformerSupplier&lt;? super K, ? super V, KeyValue&lt;K1, V1&gt;&gt; transformerSupplier,\n  String... stateStoreNames)\n</code></pre>"},{"location":"kstream/KStream/#implementations","text":"<ul> <li>KStreamImpl</li> </ul>","title":"Implementations"},{"location":"kstream/KStream/#demo","text":"<pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nval builder = new StreamsBuilder\n\n// Use type annotation to describe the stream, i.e. stream[String, String]\n// Else...Scala type inferencer gives us a stream of \"nothing\", i.e. KStream[Nothing, Nothing]\nval input = builder.stream[String, String](\"input\")\n</code></pre> <pre><code>scala&gt; :type input\norg.apache.kafka.streams.scala.kstream.KStream[String,String]\n</code></pre>","title":"Demo"},{"location":"kstream/KStreamAggProcessorSupplier/","text":"<p><code>KStreamAggProcessorSupplier</code> is...FIXME</p>","title":"KStreamAggProcessorSupplier"},{"location":"kstream/KStreamAggregate/","text":"<p><code>KStreamAggregate&lt;K, V, T&gt;</code> is a KStreamAggProcessorSupplier.</p>","title":"KStreamAggregate"},{"location":"kstream/KStreamAggregate/#creating-instance","text":"<p><code>KStreamAggregate</code> takes the following to be created:</p> <ul> <li> Name of a State Store <li> Initializer (of <code>T</code> values) <li> <code>Aggregator&lt;? super K, ? super V, T&gt;</code>  <p><code>KStreamAggregate</code> is created\u00a0when:</p> <ul> <li><code>CogroupedStreamAggregateBuilder</code> is requested to <code>build</code> (a KTable)</li> <li><code>KGroupedStreamImpl</code> is requested to aggregate and doCount</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamAggregateProcessor/","text":"<p><code>KStreamAggregateProcessor</code> is an AbstractProcessor of KStreamAggregate.</p>","title":"KStreamAggregateProcessor"},{"location":"kstream/KStreamAggregateProcessor/#creating-instance","text":"<p><code>KStreamAggregateProcessor</code> takes no arguments to be created.</p> <p><code>KStreamAggregateProcessor</code> is created\u00a0when:</p> <ul> <li><code>KStreamAggregate</code> is requested for a Processor</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamAggregateProcessor/#kstreamaggregate","text":"","title":"KStreamAggregate <p><code>KStreamAggregateProcessor</code> is a <code>private class</code> of KStreamAggregate and so have access to the internal properties (e.g. state name) thereof.</p>"},{"location":"kstream/KStreamAggregateProcessor/#timestampedkeyvaluestore","text":"","title":"TimestampedKeyValueStore <p><code>KStreamAggregateProcessor</code> looks up a TimestampedKeyValueStore by the name given when the owning KStreamAggregate was created.</p> <p>The <code>TimestampedKeyValueStore</code> is used for the following:</p> <ul> <li>Create a TimestampedTupleForwarder (in init)</li> <li>Process a key-value record (using a ValueAndTimestamp)</li> </ul>"},{"location":"kstream/KStreamAggregateProcessor/#timestampedtupleforwarder","text":"","title":"TimestampedTupleForwarder <p><code>KStreamAggregateProcessor</code> creates a new TimestampedTupleForwarder when created.</p> <p>The <code>TimestampedTupleForwarder</code> is used when processing a record.</p>"},{"location":"kstream/KStreamAggregateProcessor/#initializing","text":"","title":"Initializing <pre><code>void init(\n  ProcessorContext context)\n</code></pre> <p><code>init</code>...FIXME</p> <p><code>init</code>\u00a0is part of the AbstractProcessor abstraction.</p>"},{"location":"kstream/KStreamAggregateProcessor/#processing-record","text":"","title":"Processing Record <pre><code>void process(\n  K key, \n  V value)\n</code></pre> <p><code>process</code> requests the TimestampedKeyValueStore for the value for the input key (that gives a ValueAndTimestamp if found).</p> <p>With no previous value found, <code>process</code> requests the parent's Initializer for the initial value and the ProcessorContext for the timestamp.</p> <p><code>process</code> requests the parent's Aggregator for a new aggregate for the input key and value (and the previous or newly-created aggregation).</p> <p><code>process</code> creates a new ValueAndTimestamp with the new aggregate and the timestamp and requests the TimestampedKeyValueStore to store it (for the key).</p> <p>In the end, <code>process</code> requests the TimestampedTupleForwarder to maybeForward.</p>  <p><code>process</code>\u00a0is part of the AbstractProcessor abstraction.</p>"},{"location":"kstream/KStreamImpl/","text":"<p><code>KStreamImpl</code> is a KStream.</p>","title":"KStreamImpl"},{"location":"kstream/KStreamImpl/#creating-instance","text":"<p><code>KStreamImpl</code> takes the following to be created:</p> <ul> <li> Name <li> <code>Serde&lt;K&gt;</code> <li> <code>Serde&lt;V&gt;</code> <li> Sub-Topology Source Nodes (Names) <li>repartitionRequired flag</li> <li> GraphNode <li> InternalStreamsBuilder  <p><code>KStreamImpl</code> is created\u00a0when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to stream</li> <li>others</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamImpl/#repartitionrequired-flag","text":"","title":"repartitionRequired Flag <p><code>KStreamImpl</code> is given a <code>repartitionRequired</code> flag when created.</p>"},{"location":"kstream/KStreamImpl/#dojoin","text":"","title":"doJoin <pre><code>KStream&lt;K, VR&gt; doJoin(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K, V, VO&gt; streamJoined,\n  KStreamImplJoin join)\n</code></pre> <p>In the end, <code>doJoin</code> requests the given KStreamImplJoin to join.</p> <p><code>doJoin</code>\u00a0is used when:</p> <ul> <li><code>KStreamImpl</code> is requested to join, leftJoin and outerJoin</li> </ul>"},{"location":"kstream/KStreamImpl/#groupby","text":"","title":"groupBy <pre><code>KGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector)\nKGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector,\n  Grouped&lt;KR, V&gt; grouped)\n</code></pre> <p><code>groupBy</code>...FIXME</p> <p>In the end, <code>groupBy</code> creates a KGroupedStreamImpl (with the repartitionRequired flag enabled).</p> <p><code>groupBy</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImpl/#groupbykey","text":"","title":"groupByKey <pre><code>KGroupedStream&lt;K, V&gt; groupByKey()\nKGroupedStream&lt;K, V&gt; groupByKey(\n  Grouped&lt;K, V&gt; grouped)\n</code></pre> <p><code>groupByKey</code> creates a KGroupedStreamImpl.</p> <p><code>groupByKey</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImplJoin/","text":"","title":"KStreamImplJoin"},{"location":"kstream/KStreamImplJoin/#creating-instance","text":"<p><code>KStreamImplJoin</code> takes the following to be created:</p> <ul> <li> InternalStreamsBuilder <li> <code>leftOuter</code> flag <li> <code>rightOuter</code> flag  <p><code>KStreamImplJoin</code> is created\u00a0when:</p> <ul> <li><code>KStreamImpl</code> is requested to join, leftJoin and outerJoin</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamImplJoin/#join","text":"","title":"join <pre><code>KStream&lt;K1, R&gt; join(\n  KStream&lt;K1, V1&gt; lhs,\n  KStream&lt;K1, V2&gt; other,\n  ValueJoinerWithKey&lt;? super K1, ? super V1, ? super V2, ? extends R&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K1, V1, V2&gt; streamJoined)\n</code></pre> <p><code>join</code>...FIXME</p> <p><code>join</code>\u00a0is used when:</p> <ul> <li><code>KStreamImpl</code> is requested to doJoin</li> </ul>"},{"location":"kstream/KStreamSlidingWindowAggregateProcessor/","text":"<p><code>KStreamSlidingWindowAggregateProcessor</code> is...FIXME</p>","title":"KStreamSlidingWindowAggregateProcessor"},{"location":"kstream/KStreamWindowAggregateProcessor/","text":"<p><code>KStreamWindowAggregateProcessor</code> is...FIXME</p>","title":"KStreamWindowAggregateProcessor"},{"location":"kstream/KTable/","text":"<p><code>KTable</code> is...FIXME</p>","title":"KTable"},{"location":"kstream/KTableKTableJoinMergeProcessor/","text":"<p><code>KTableKTableJoinMergeProcessor</code> is...FIXME</p>","title":"KTableKTableJoinMergeProcessor"},{"location":"kstream/KTableSource/","text":"<p><code>KTableSource</code> is...FIXME</p>","title":"KTableSource"},{"location":"kstream/Materialized/","text":"<p><code>Materialized</code> is...FIXME</p>","title":"Materialized"},{"location":"kstream/NamedOperation/","text":"<p><code>NamedOperation</code> is an abstraction of metadata with the name of the associated processors (and in turn the names of operations, internal topics and stores).</p>","title":"NamedOperation"},{"location":"kstream/NamedOperation/#contract","text":"","title":"Contract"},{"location":"kstream/NamedOperation/#withname","text":"","title":"withName <pre><code>T withName(\n  String name)\n</code></pre> <p>Processor name</p>"},{"location":"kstream/NamedOperation/#implementations","text":"<ul> <li>Branched</li> <li>Consumed</li> <li>Grouped</li> <li>Joined</li> <li>Named</li> <li>Printed</li> <li>Produced</li> <li>Repartitioned</li> <li>StreamJoined</li> <li>Suppressed</li> </ul>","title":"Implementations"},{"location":"kstream/Produced/","text":"<p><code>Produced&lt;K, V&gt;</code> describes how to produce records in a topology in the High-Level KStream DSL for the following high-level operators:</p> <ul> <li>KStream.to</li> </ul> <p><code>Produced&lt;K, V&gt;</code> is a NamedOperation.</p>","title":"Produced \u2014 Metadata for Producing Records"},{"location":"kstream/Produced/#demo","text":"<pre><code>import org.apache.kafka.common.serialization.Serdes\nimport org.apache.kafka.streams.kstream.Produced\nval produced = Produced.`with`(Serdes.Long, Serdes.String)\n</code></pre> <pre><code>scala&gt; :type produced\norg.apache.kafka.streams.kstream.Produced[Long,String]\n</code></pre>","title":"Demo"},{"location":"kstream/Produced/#creating-instance","text":"<p><code>Produced</code> takes the following to be created:</p> <ul> <li> <code>Serde&lt;K&gt;</code> of keys (Apache Kafka) <li> <code>Serde&lt;V&gt;</code> of values (Apache Kafka) <li> <code>StreamPartitioner&lt;? super K, ? super V&gt;</code> <li> Processor Name  <p><code>Produced</code> is created\u00a0using the factories.</p>","title":"Creating Instance"},{"location":"kstream/Produced/#creating-consumed","text":"","title":"Creating Consumed"},{"location":"kstream/Produced/#as","text":"","title":"as <pre><code>Produced&lt;K, V&gt; as(\n  String processorName)\n</code></pre>"},{"location":"kstream/Produced/#with","text":"","title":"with <pre><code>Produced&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde)\nProduced&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde,\n  StreamPartitioner&lt;? super K, ? super V&gt; partitioner)\n</code></pre>"},{"location":"kstream/Produced/#keyserde","text":"","title":"keySerde <pre><code>Produced&lt;K, V&gt; keySerde(\n  Serde&lt;K&gt; keySerde)\n</code></pre>"},{"location":"kstream/Produced/#valueserde","text":"","title":"valueSerde <pre><code>Produced&lt;K, V&gt; valueSerde(\n  Serde&lt;V&gt; valueSerde)\n</code></pre>"},{"location":"kstream/Produced/#streampartitioner","text":"","title":"streamPartitioner <pre><code>Produced&lt;K, V&gt; streamPartitioner(\n  StreamPartitioner&lt;? super K, ? super V&gt; partitioner)\n</code></pre>"},{"location":"kstream/Produced/#scala-api","text":"","title":"Scala API <p>Scala API for Kafka Streams makes the optional <code>Produced</code> metadata an implicit parameter in the KStream API.</p> <p>Moreover, <code>ImplicitConversions</code> object defines <code>producedFromSerde</code> implicit method that creates a <code>Produced</code> instance with the key and value <code>Serde</code> objects available in implicit scope.</p> <p>And the last but not least, Scala API for Kafka Streams defines <code>Produced</code> object with <code>with</code> factory methods that use implicit key and value <code>Serde</code> objects.</p>"},{"location":"kstream/StateStoreNode/","text":"<p><code>StateStoreNode</code> is a GraphNode.</p>","title":"StateStoreNode"},{"location":"kstream/StateStoreNode/#creating-instance","text":"<p><code>StateStoreNode</code> takes the following to be created:</p> <ul> <li> StoreBuilder  <p><code>StateStoreNode</code> is created\u00a0when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to add a StoreBuilder</li> </ul>","title":"Creating Instance"},{"location":"kstream/StateStoreNode/#writetotopology","text":"","title":"writeToTopology <pre><code>void writeToTopology(\n  InternalTopologyBuilder topologyBuilder,\n  Properties props)\n</code></pre> <p><code>writeToTopology</code> merely requests the given InternalTopologyBuilder to add the storeBuilder.</p> <p><code>writeToTopology</code>\u00a0is part of the GraphNode abstraction.</p>"},{"location":"kstream/StreamsBuilder/","text":"<p><code>StreamsBuilder</code> is the entry point to the High-Level Streams DSL to define and build a stream processing topology.</p> <p>All of the high-level operators use the InternalStreamsBuilder behind the scenes. In other words, <code>StreamsBuilder</code> offers a more developer-friendly high-level API for developing Kafka Streams applications than using the <code>InternalStreamsBuilder</code> API directly (and is a fa\u00e7ade of <code>InternalStreamsBuilder</code>).</p>  <p>Scala API for Kafka Streams</p> <p>Use Scala API for Kafka Streams to make your Kafka Streams development more pleasant with Scala.</p>","title":"StreamsBuilder"},{"location":"kstream/StreamsBuilder/#creating-instance","text":"<p><code>StreamsBuilder</code> takes no arguments to be created.</p> <pre><code>import org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <p>While being created, <code>StreamsBuilder</code> creates an empty Topology that in turn is requested for an InternalTopologyBuilder. In the end, <code>StreamsBuilder</code> creates an InternalStreamsBuilder.</p> <p></p>","title":"Creating Instance"},{"location":"kstream/StreamsBuilder/#topology","text":"","title":"Topology <p><code>StreamsBuilder</code> creates a Topology when created.</p> <p><code>StreamsBuilder</code> uses the <code>Topology</code> to create an InternalTopologyBuilder.</p> <p>The <code>Topology</code> is then optimized and returned when <code>StreamsBuilder</code> is requested to build a topology.</p>"},{"location":"kstream/StreamsBuilder/#building-and-optimizing-topology","text":"","title":"Building and Optimizing Topology <pre><code>Topology build() // (1)\nTopology build(\n  Properties props)\n</code></pre> <ol> <li>Uses undefined properties (<code>null</code>)</li> </ol> <p><code>build</code> requests the InternalStreamsBuilder to build and optimize a topology. In the end, <code>build</code> returns the Topology.</p>"},{"location":"kstream/StreamsBuilder/#globaltable","text":"","title":"globalTable <pre><code>GlobalKTable&lt;K, V&gt; globalTable(\n  String topic)\nGlobalKTable&lt;K, V&gt; globalTable(\n  String topic,\n  Consumed&lt;K, V&gt; consumed)\nGlobalKTable&lt;K, V&gt; globalTable(\n  String topic,\n  Consumed&lt;K, V&gt; consumed,\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\nGlobalKTable&lt;K, V&gt; globalTable(\n  String topic,\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\n</code></pre> <p><code>globalTable</code> adds a GlobalKTable to a topology.</p>"},{"location":"kstream/StreamsBuilder/#demo-non-queryable-globalktable","text":"","title":"Demo: Non-queryable GlobalKTable <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nimport org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <pre><code>val globalTable = builder.globalTable[String, String](topic = \"demo-global-table\")\n</code></pre> <pre><code>scala&gt; :type globalTable\norg.apache.kafka.streams.kstream.GlobalKTable[String,String]\n</code></pre> <pre><code>assert(globalTable.queryableStoreName == null)\n</code></pre> <pre><code>val topology = builder.build()\n</code></pre> <pre><code>scala&gt; println(topology.describe)\nTopologies:\n   Sub-topology: 0 for global store (will not generate tasks)\n    Source: KSTREAM-SOURCE-0000000001 (topics: [demo-global-table])\n      --&gt; KTABLE-SOURCE-0000000002\n    Processor: KTABLE-SOURCE-0000000002 (stores: [demo-global-table-STATE-STORE-0000000000])\n      --&gt; none\n      &lt;-- KSTREAM-SOURCE-0000000001\n</code></pre>"},{"location":"kstream/StreamsBuilder/#demo-queryable-globalktable","text":"","title":"Demo: Queryable GlobalKTable <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nimport org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <pre><code>import org.apache.kafka.streams.state.Stores\nval supplier = Stores.inMemoryKeyValueStore(\"queryable-store-name\")\n\nimport org.apache.kafka.streams.scala.kstream.Materialized\nval materialized = Materialized.as[String, String](supplier)\nval zipCodes = builder.globalTable[String, String](topic = \"zip-codes\", materialized)\n</code></pre> <pre><code>scala&gt; :type zipCodes\norg.apache.kafka.streams.kstream.GlobalKTable[String,String]\n</code></pre> <pre><code>assert(zipCodes.queryableStoreName == \"queryable-store-name\")\n</code></pre> <pre><code>val topology = builder.build()\n</code></pre> <pre><code>scala&gt; println(topology.describe)\nTopologies:\n   Sub-topology: 0 for global store (will not generate tasks)\n    Source: KSTREAM-SOURCE-0000000000 (topics: [zip-codes])\n      --&gt; KTABLE-SOURCE-0000000001\n    Processor: KTABLE-SOURCE-0000000001 (stores: [queryable-store-name])\n      --&gt; none\n      &lt;-- KSTREAM-SOURCE-0000000000\n</code></pre>"},{"location":"kstream/StreamsBuilder/#stream","text":"","title":"stream <pre><code>KStream&lt;K, V&gt; stream(\n  Collection&lt;String&gt; topics)\nKStream&lt;K, V&gt; stream(\n  Collection&lt;String&gt; topics,\n  Consumed&lt;K, V&gt; consumed)\nKStream&lt;K, V&gt; stream(\n  Pattern topicPattern)\nKStream&lt;K, V&gt; stream(\n  Pattern topicPattern,\n  Consumed&lt;K, V&gt; consumed)\nKStream&lt;K, V&gt; stream(\n  String topic)\nKStream&lt;K, V&gt; stream(\n  String topic,\n  Consumed&lt;K, V&gt; consumed)\n</code></pre> <p><code>stream</code> requests the InternalStreamsBuilder to stream.</p>"},{"location":"kstream/StreamsBuilder/#demo-custom-processor-name","text":"","title":"Demo: Custom Processor Name <pre><code>import org.apache.kafka.streams.scala._\nimport org.apache.kafka.streams.scala.kstream._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nimport org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <pre><code>implicit val consumed = Consumed.`with`[String, String].withName(\"processorName\")\nval demo = builder.stream[String, String](\"demo\")\n</code></pre> <pre><code>scala&gt; println(builder.build().describe)\nTopologies:\n   Sub-topology: 0\n    Source: processorName (topics: [demo])\n      --&gt; none\n</code></pre>"},{"location":"kstream/TimestampedTupleForwarder/","text":"<p><code>TimestampedTupleForwarder</code> is used by processors to determine whether or not to forward records to child nodes (downstream processors) (that happens only with no caching).</p>","title":"TimestampedTupleForwarder"},{"location":"kstream/TimestampedTupleForwarder/#creating-instance","text":"<p><code>TimestampedTupleForwarder</code> takes the following to be created:</p> <ul> <li> StateStore <li> ProcessorContext <li> <code>TimestampedCacheFlushListener</code> <li> <code>sendOldValues</code> flag  <p><code>TimestampedTupleForwarder</code> is created\u00a0when:</p> <ul> <li><code>KStreamAggregateProcessor</code> is requested to initialize</li> <li><code>KStreamSlidingWindowAggregateProcessor</code> is requested to initialize</li> <li><code>KStreamWindowAggregateProcessor</code> is requested to initialize</li> <li><code>KTableSource</code> is requested to initialize</li> <li>others</li> </ul>","title":"Creating Instance"},{"location":"kstream/TimestampedTupleForwarder/#cachingenabled-flag","text":"","title":"cachingEnabled Flag <p><code>TimestampedTupleForwarder</code> requests the StateStore to setFlushListener when created. The returned value is used to initialize <code>cachingEnabled</code> internal flag for maybeForward.</p>"},{"location":"kstream/TimestampedTupleForwarder/#maybeforward","text":"","title":"maybeForward <pre><code>void maybeForward(\n  K key,\n  V newValue,\n  V oldValue) // (1)\nvoid maybeForward(\n  K key,\n  V newValue,\n  V oldValue,\n  long timestamp)\nvoid maybeForward(\n  Record&lt;K, Change&lt;V&gt;&gt; record)\n</code></pre> <p><code>maybeForward</code> requests the InternalProcessorContext to forward a record only with the cachingEnabled flag disabled.</p>"},{"location":"processor/","text":"<p>Processor API is a low-level API for developers to define topologies in Kafka Streams (mostly when High-Level Streams DSL would not meet expectations).</p> <p>Processor API comes with the following low-level stream processing abstractions:</p> <ul> <li>Processor</li> <li>ProcessorContext</li> <li>ProcessorSupplier</li> <li>Punctuator</li> </ul>","title":"Processor API"},{"location":"processor/AbstractProcessor/","text":"<p><code>AbstractProcessor</code> is...FIXME</p>","title":"AbstractProcessor"},{"location":"processor/AbstractTask/","text":"<p><code>AbstractTask</code>\u00a0is a base abstraction of the Task abstraction for tasks.</p>","title":"AbstractTask"},{"location":"processor/AbstractTask/#implementations","text":"<ul> <li>StandbyTask</li> <li>StreamTask</li> </ul>","title":"Implementations"},{"location":"processor/AbstractTask/#creating-instance","text":"<p><code>AbstractTask</code> takes the following to be created:</p> <ul> <li> <code>TaskId</code> <li> ProcessorTopology <li> StateDirectory <li> ProcessorStateManager <li> Input <code>TopicPartition</code>s <li> task.timeout.ms configuration property <li> Task Type <li> <code>AbstractTask</code> Class  Abstract Class<p><code>AbstractTask</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete AbstractTasks.</p>","title":"Creating Instance"},{"location":"processor/ActiveTaskCreator/","text":"","title":"ActiveTaskCreator"},{"location":"processor/ActiveTaskCreator/#createtasks","text":"","title":"createTasks <pre><code>Collection&lt;Task&gt; createTasks(\n  Consumer&lt;byte[], byte[]&gt; consumer,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; tasksToBeCreated)\n</code></pre> <p><code>createTasks</code>...FIXME</p> <p><code>createTasks</code>\u00a0is used when:</p> <ul> <li><code>Tasks</code> is requested to createTasks</li> </ul>"},{"location":"processor/ActiveTaskCreator/#createactivetaskfromstandby","text":"","title":"createActiveTaskFromStandby <pre><code>StreamTask createActiveTaskFromStandby(\n  StandbyTask standbyTask,\n  Set&lt;TopicPartition&gt; inputPartitions,\n  Consumer&lt;byte[], byte[]&gt; consumer)\n</code></pre> <p><code>createActiveTaskFromStandby</code>...FIXME</p> <p><code>createActiveTaskFromStandby</code>\u00a0is used when:</p> <ul> <li><code>Tasks</code> is requested to convertStandbyToActive</li> </ul>"},{"location":"processor/ActiveTaskCreator/#createactivetask","text":"","title":"createActiveTask <pre><code>StreamTask createActiveTask(\n  TaskId taskId,\n  Set&lt;TopicPartition&gt; inputPartitions,\n  Consumer&lt;byte[], byte[]&gt; consumer,\n  LogContext logContext,\n  ProcessorTopology topology,\n  ProcessorStateManager stateManager,\n  InternalProcessorContext context)\n</code></pre> <p><code>createActiveTask</code>...FIXME</p> <p><code>createActiveTask</code>\u00a0is used when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createTasks and createActiveTaskFromStandby</li> </ul>"},{"location":"processor/GlobalStateManagerImpl/","text":"<p><code>GlobalStateManagerImpl</code> is...FIXME</p>","title":"GlobalStateManagerImpl"},{"location":"processor/GlobalStreamThread/","text":"<p><code>GlobalStreamThread</code> is...FIXME</p>","title":"GlobalStreamThread"},{"location":"processor/InternalTopologyBuilder/","text":"","title":"InternalTopologyBuilder"},{"location":"processor/InternalTopologyBuilder/#global-topics","text":"","title":"Global Topics <pre><code>Set&lt;String&gt; globalTopics\n</code></pre> <p><code>InternalTopologyBuilder</code> tracks global topics (names) in a <code>globalTopics</code> internal registry.</p> <p>A new topic name is added in addGlobalStore.</p>"},{"location":"processor/InternalTopologyBuilder/#building-processor-topology","text":"","title":"Building Processor Topology <pre><code>ProcessorTopology build(\n  Set&lt;String&gt; nodeGroup)\n</code></pre> <p>For every NodeFactory (in the nodeFactories internal registry), if the name of the factory is in the given node group if defined or simply all node factories go through, <code>build</code> does the following:</p> <ol> <li>Requests the <code>NodeFactory</code> to build a ProcessorNode (and registers it in a local registry of processors by name)</li> <li>For <code>ProcessorNodeFactory</code>s, buildProcessorNode</li> <li>For <code>SourceNodeFactory</code>s, buildSourceNode</li> <li>For <code>SinkNodeFactory</code>s, buildSinkNode</li> </ol> <p>In the end, <code>build</code> creates a new ProcessorTopology.</p> <p><code>build</code>\u00a0is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to build a topology, a subtopology and a global state topology</li> </ul>"},{"location":"processor/InternalTopologyBuilder/#buildprocessornode","text":"","title":"buildProcessorNode <pre><code>void buildProcessorNode(\n  Map&lt;String, ProcessorNode&lt;?, ?, ?, ?&gt;&gt; processorMap,\n  Map&lt;String, StateStore&gt; stateStoreMap,\n  ProcessorNodeFactory&lt;?, ?, ?, ?&gt; factory,\n  ProcessorNode&lt;Object, Object, Object, Object&gt; node)\n</code></pre> <p><code>buildProcessorNode</code>...FIXME</p>"},{"location":"processor/InternalTopologyBuilder/#building-source-node","text":"","title":"Building Source Node <pre><code>void buildSourceNode(\n  Map&lt;String, SourceNode&lt;?, ?&gt;&gt; topicSourceMap,\n  Set&lt;String&gt; repartitionTopics,\n  SourceNodeFactory&lt;?, ?&gt; sourceNodeFactory,\n  SourceNode&lt;?, ?&gt; node)\n</code></pre> <p><code>buildSourceNode</code> mutates (changes) the given <code>SourceNode</code> by topic name (<code>topicSourceMap</code>) and repartition topic names (<code>repartitionTopics</code>) collections.</p>  <p>When the pattern (of the given SourceNodeFactory) is defined, <code>buildSourceNode</code> subscriptionUpdates and requests the <code>SourceNodeFactory</code> to get the topics. Otherwise, <code>buildSourceNode</code> requests the <code>SourceNodeFactory</code> for the topics.</p> <p><code>buildSourceNode</code> adds the topic to the given <code>topicSourceMap</code> collection.</p> <p>For internal topics (in internalTopicNamesWithProperties registry), <code>buildSourceNode</code> decorates the name before adding to the given <code>topicSourceMap</code> collection and adds them to the given <code>repartitionTopics</code> collection.</p>"},{"location":"processor/InternalTopologyBuilder/#buildsinknode","text":"","title":"buildSinkNode <pre><code>void buildSinkNode(\n  Map&lt;String, ProcessorNode&lt;?, ?, ?, ?&gt;&gt; processorMap,\n  Map&lt;String, SinkNode&lt;?, ?&gt;&gt; topicSinkMap,\n  Set&lt;String&gt; repartitionTopics,\n  SinkNodeFactory&lt;?, ?&gt; sinkNodeFactory,\n  SinkNode&lt;?, ?&gt; node)\n</code></pre> <p><code>buildSinkNode</code>...FIXME</p>"},{"location":"processor/InternalTopologyBuilder/#building-local-processor-topology","text":"","title":"Building (Local) Processor Topology <pre><code>ProcessorTopology buildTopology()\n</code></pre> <p><code>buildTopology</code> initializes subscription and then builds a topology (of the node groups without the global node groups).</p> <p><code>buildTopology</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> <li><code>TopologyTestDriver</code> is requested to setupTopology</li> </ul>"},{"location":"processor/InternalTopologyBuilder/#building-processor-subtopology","text":"","title":"Building Processor SubTopology <pre><code>ProcessorTopology buildSubtopology(\n  int topicGroupId)\n</code></pre> <p><code>buildSubtopology</code> takes the <code>topicGroupId</code> node group (from the nodeGroups) and builds a topology.</p> <p><code>buildSubtopology</code>\u00a0is used when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createTasks and createActiveTaskFromStandby</li> <li><code>StandbyTaskCreator</code> is requested to createTasks and createStandbyTaskFromActive</li> </ul>"},{"location":"processor/InternalTopologyBuilder/#building-global-state-processor-topology","text":"","title":"Building Global State Processor Topology <pre><code>ProcessorTopology buildGlobalStateTopology()\n</code></pre> <p><code>buildGlobalStateTopology</code> builds a topology of the global node groups if there are any.</p> <p><code>buildGlobalStateTopology</code> assumes that the applicationId has already been set or throws a <code>NullPointerException</code>:</p> <pre><code>topology has not completed optimization\n</code></pre> <p><code>buildGlobalStateTopology</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> <li><code>TopologyTestDriver</code> is requested to setupTopology</li> </ul>"},{"location":"processor/InternalTopologyBuilder/#rewriting-topology","text":"","title":"Rewriting Topology <pre><code>InternalTopologyBuilder rewriteTopology(\n  StreamsConfig config)\n</code></pre> <p><code>rewriteTopology</code> setApplicationId to the value of application.id configuration property.</p> <p>With cache.max.bytes.buffering enabled, <code>rewriteTopology</code>...FIXME</p> <p><code>rewriteTopology</code> requests the global StoreBuilders to build StateStores.</p> <p>In the end, <code>rewriteTopology</code> saves the StreamsConfig (and returns itself).</p> <p><code>rewriteTopology</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> <li><code>TopologyTestDriver</code> is requested to setupTopology</li> </ul>"},{"location":"processor/InternalTopologyBuilder/#globalnodegroups","text":"","title":"globalNodeGroups <pre><code>Set&lt;String&gt; globalNodeGroups()\n</code></pre> <p><code>globalNodeGroups</code> collects global source nodes from all the node groups.</p> <p><code>globalNodeGroups</code>\u00a0is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to build a local (excluding global state nodes) and global state topologies</li> </ul>"},{"location":"processor/InternalTopologyBuilder/#isglobalsource","text":"","title":"isGlobalSource <pre><code>boolean isGlobalSource(\n  String nodeName)\n</code></pre> <p><code>isGlobalSource</code> finds a NodeFactory (by given <code>nodeName</code>) in nodeFactories registry.</p> <p><code>isGlobalSource</code> is positive (<code>true</code>) when the <code>NodeFactory</code> is a SourceNodeFactory with one topic only that is global. Otherwise, <code>isGlobalSource</code> is negative (<code>false</code>).</p> <p><code>isGlobalSource</code>\u00a0is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to globalNodeGroups, describeGlobalStore and nodeGroupContainsGlobalSourceNode</li> </ul>"},{"location":"processor/InternalTopologyBuilder/#registering-global-store","text":"","title":"Registering Global Store <pre><code>&lt;KIn, VIn&gt; void addGlobalStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  String sourceName,\n  TimestampExtractor timestampExtractor,\n  Deserializer&lt;KIn&gt; keyDeserializer,\n  Deserializer&lt;VIn&gt; valueDeserializer,\n  String topic,\n  String processorName,\n  ProcessorSupplier&lt;KIn, VIn, Void, Void&gt; stateUpdateSupplier)\n</code></pre> <p><code>addGlobalStore</code>...FIXME</p> <p><code>addGlobalStore</code> is used when:</p> <ul> <li><code>Topology</code> is requested to addGlobalStore</li> <li><code>GlobalStoreNode</code> is requested to <code>writeToTopology</code></li> <li><code>TableSourceNode</code> is requested to <code>writeToTopology</code></li> </ul>"},{"location":"processor/InternalTopologyBuilder/#registering-processor","text":"","title":"Registering Processor <pre><code>void addProcessor(\n  String name,\n  ProcessorSupplier&lt;KIn, VIn, KOut, VOut&gt; supplier,\n  String... predecessorNames)\n</code></pre> <p><code>addProcessor</code> creates a ProcessorNodeFactory (that is then added to nodeFactories registry).</p> <p><code>addProcessor</code> adds the name to nodeGrouper registry and requests it to unite the name with the given <code>predecessorNames</code>.</p> <p><code>addProcessor</code> is used when:</p> <ul> <li><code>Topology</code> is requested to addProcessor</li> <li>Some <code>GraphNode</code>s are requested to writeToTopology</li> </ul>"},{"location":"processor/InternalTopologyBuilder/#registering-statestore","text":"","title":"Registering StateStore <pre><code>void addStateStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  String... processorNames) // (1)\nvoid addStateStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  boolean allowOverride,\n  String... processorNames)\n</code></pre> <ol> <li>Uses <code>allowOverride</code> flag disabled (<code>false</code>)</li> </ol> <p><code>addStateStore</code>...FIXME</p> <p><code>addStateStore</code>\u00a0is used when:</p> <ul> <li><code>Topology</code> is requested to addProcessor and addStateStore</li> <li><code>KTableKTableJoinNode</code> is requested to <code>writeToTopology</code></li> <li><code>StatefulProcessorNode</code> is requested to <code>writeToTopology</code></li> <li><code>StateStoreNode</code> is requested to <code>writeToTopology</code></li> <li><code>StreamStreamJoinNode</code> is requested to <code>writeToTopology</code></li> <li><code>StreamToTableNode</code> is requested to <code>writeToTopology</code></li> <li><code>TableProcessorNode</code> is requested to <code>writeToTopology</code></li> <li><code>TableSourceNode</code> is requested to <code>writeToTopology</code></li> </ul>"},{"location":"processor/NodeFactory/","text":"<p><code>NodeFactory</code> is...FIXME</p>","title":"NodeFactory"},{"location":"processor/Processor/","text":"<p><code>Processor&lt;KIn, VIn, KOut, VOut&gt;</code> is an abstraction of processing nodes (in a stream processing topology).</p>","title":"Processor"},{"location":"processor/Processor/#contract","text":"","title":"Contract"},{"location":"processor/Processor/#close","text":"","title":"close <pre><code>void close()\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorNode</code> is requested to close</li> </ul>"},{"location":"processor/Processor/#init","text":"","title":"init <pre><code>void init(\n  ProcessorContext&lt;KOut, VOut&gt; context)\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorNode</code> is requested to init</li> </ul>"},{"location":"processor/Processor/#process","text":"","title":"process <pre><code>void process(\n  Record&lt;KIn, VIn&gt; record)\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorNode</code> is requested to process</li> </ul>"},{"location":"processor/Processor/#implementations","text":"<ul> <li><code>ContextualProcessor</code></li> <li><code>ForeachProcessor</code></li> </ul>","title":"Implementations"},{"location":"processor/ProcessorContext/","text":"<p><code>ProcessorContext</code> is...FIXME</p>","title":"ProcessorContext"},{"location":"processor/ProcessorNode/","text":"<p><code>ProcessorNode</code> is a \"hosting environment\" of a Processor in a processor topology.</p>","title":"ProcessorNode"},{"location":"processor/ProcessorNode/#creating-instance","text":"<p><code>ProcessorNode</code> takes the following to be created:</p> <ul> <li> Name <li> Processor <li> Names of the state stores  <p><code>ProcessorNode</code> is created\u00a0when:</p> <ul> <li><code>ProcessorNodeFactory</code> is requested to build a processor</li> </ul>","title":"Creating Instance"},{"location":"processor/ProcessorNode/#specialized-processornodes","text":"<p>SourceNode and SinkNode are specialized <code>ProcessorNode</code>s.</p>","title":"Specialized ProcessorNodes"},{"location":"processor/ProcessorNode/#terminal-node","text":"","title":"Terminal Node <p><code>ProcessorNode</code> is considered terminal when there are no children.</p>"},{"location":"processor/ProcessorNodeFactory/","text":"<p><code>ProcessorNodeFactory</code> is a NodeFactory.</p>","title":"ProcessorNodeFactory"},{"location":"processor/ProcessorNodeFactory/#creating-instance","text":"<p><code>ProcessorNodeFactory</code> takes the following to be created:</p> <ul> <li> Name <li> Predecessor Nodes <li> ProcessorSupplier  <p><code>ProcessorNodeFactory</code> is created\u00a0when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to addProcessor and addGlobalStore</li> </ul>","title":"Creating Instance"},{"location":"processor/ProcessorStateManager/","text":"<p><code>ProcessorStateManager</code> is a StateManager.</p>","title":"ProcessorStateManager"},{"location":"processor/ProcessorStateManager/#flushing-store-caches","text":"","title":"Flushing Store Caches <pre><code>void flushCache()\n</code></pre> <p><code>flushCache</code>...FIXME</p> <p><code>flushCache</code>\u00a0is used when:</p> <ul> <li><code>StreamTask</code> is requested to prepareCommit</li> </ul>"},{"location":"processor/ProcessorSupplier/","text":"<p><code>ProcessorSupplier</code>\u00a0is an extension of the <code>ConnectedStoreProvider</code> and <code>Supplier</code> (Java) abstractions for processor suppliers.</p> <p><code>ProcessorSupplier</code>\u00a0is marked with <code>FunctionalInterface</code> (Java) annotation.</p>","title":"ProcessorSupplier"},{"location":"processor/ProcessorSupplier/#contract","text":"","title":"Contract"},{"location":"processor/ProcessorSupplier/#creating-processor","text":"","title":"Creating Processor <pre><code>Processor&lt;KIn, VIn, KOut, VOut&gt; get()\n</code></pre> <p>Creates a Processor</p>"},{"location":"processor/ProcessorTopology/","text":"","title":"ProcessorTopology"},{"location":"processor/ProcessorTopology/#creating-instance","text":"<p><code>ProcessorTopology</code> takes the following to be created:</p> <ul> <li> <code>ProcessorNode</code>s <li> <code>SourceNode</code>s by topic <li> <code>SinkNode</code> by topic <li> <code>StateStore</code>s <li> Global <code>StateStore</code>s <li> Store names by topic <li> Repartition topics  <p><code>ProcessorTopology</code> is created\u00a0when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to build a ProcessorTopology</li> </ul>","title":"Creating Instance"},{"location":"processor/Punctuator/","text":"<p><code>Punctuator</code> is...FIXME</p>","title":"Punctuator"},{"location":"processor/SinkNode/","text":"<p><code>SinkNode</code> is...FIXME</p>","title":"SinkNode"},{"location":"processor/SourceNode/","text":"<p><code>SourceNode</code> is...FIXME</p>","title":"SourceNode"},{"location":"processor/SourceNodeFactory/","text":"<p><code>SourceNodeFactory</code> is...FIXME</p>","title":"SourceNodeFactory"},{"location":"processor/StandbyTask/","text":"<p><code>StandbyTask</code> is a Task (and AbstractTask).</p>","title":"StandbyTask"},{"location":"processor/StandbyTask/#creating-instance","text":"<p><code>StandbyTask</code> takes the following to be created:</p> <ul> <li> <code>TaskId</code> <li> Input <code>TopicPartition</code>s <li> ProcessorTopology <li> StreamsConfig <li> <code>StreamsMetricsImpl</code> <li> ProcessorStateManager <li> StateDirectory <li> <code>ThreadCache</code> <li> <code>InternalProcessorContext</code>  <p>When created, <code>StandbyTask</code> requests the InternalProcessorContext to <code>transitionToStandby</code> with the ThreadCache.</p> <p><code>StandbyTask</code> is created\u00a0when:</p> <ul> <li><code>StandbyTaskCreator</code> is requested to createStandbyTask</li> </ul>","title":"Creating Instance"},{"location":"processor/StandbyTask/#abstracttask","text":"","title":"AbstractTask <p><code>StandbyTask</code> is an AbstractTask.</p>"},{"location":"processor/StandbyTask/#task-type","text":"","title":"Task Type <p><code>StandbyTask</code> uses standby-task for task type.</p>"},{"location":"processor/StandbyTask/#class","text":"","title":"Class <p><code>StandbyTask</code> uses <code>StandbyTask.class</code> for clazz.</p>"},{"location":"processor/StandbyTaskCreator/","text":"","title":"StandbyTaskCreator"},{"location":"processor/StandbyTaskCreator/#creating-instance","text":"<p><code>StandbyTaskCreator</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder <li> StreamsConfig <li> <code>StreamsMetricsImpl</code> <li> StateDirectory <li> <code>ChangelogReader</code> <li> Thread ID <li> <code>Logger</code>  <p>When created, <code>StandbyTaskCreator</code> initializes a task sensor and a ThreadCache.</p> <p><code>StandbyTaskCreator</code> is created\u00a0when:</p> <ul> <li><code>StreamThread</code> utility is used to create a StreamThread</li> </ul>","title":"Creating Instance"},{"location":"processor/StateDirectory/","text":"<p><code>StateDirectory</code> is...FIXME</p>","title":"StateDirectory"},{"location":"processor/StateManager/","text":"<p><code>StateManager</code> is...FIXME</p>","title":"StateManager"},{"location":"processor/StateStore/","text":"<p><code>StateStore</code> is...FIXME</p>","title":"StateStore"},{"location":"processor/StateStoreFactory/","text":"<p><code>StateStoreFactory</code> is a factory of StateStores.</p> <pre><code>StateStoreFactory&lt;S extends StateStore&gt;\n</code></pre> <p><code>StateStoreFactory</code> is a <code>public static class</code> of InternalTopologyBuilder.</p>","title":"StateStoreFactory"},{"location":"processor/StateStoreFactory/#creating-instance","text":"<p><code>StateStoreFactory</code> takes the following to be created:</p> <ul> <li> StoreBuilder (of <code>S</code> StateStores)  <p><code>StateStoreFactory</code> is created\u00a0when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to addStateStore</li> </ul>","title":"Creating Instance"},{"location":"processor/StoreChangelogReader/","text":"<p><code>StoreChangelogReader</code> is...FIXME</p>","title":"StoreChangelogReader"},{"location":"processor/StreamTask/","text":"<p><code>StreamTask</code> is a concrete AbstractTask.</p>","title":"StreamTask"},{"location":"processor/StreamTask/#creating-instance","text":"<p><code>StreamTask</code> takes the following to be created:</p> <ul> <li> <code>TaskId</code> <li> Input <code>TopicPartition</code>s <li> <code>ProcessorTopology</code> <li> Main <code>Consumer&lt;byte[], byte[]&gt;</code> <li> StreamsConfig <li> <code>StreamsMetricsImpl</code> <li> <code>StateDirectory</code> <li> <code>ThreadCache</code> <li> <code>Time</code> <li> <code>ProcessorStateManager</code> <li> <code>RecordCollector</code> <li> <code>InternalProcessorContext</code> <li> <code>LogContext</code>  <p><code>StreamTask</code> is created\u00a0when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createActiveTask</li> <li><code>TopologyTestDriver</code> is requested to <code>setupTask</code></li> </ul>","title":"Creating Instance"},{"location":"processor/StreamTask/#preparecommit","text":"","title":"prepareCommit <pre><code>Map&lt;TopicPartition, OffsetAndMetadata&gt; prepareCommit()\n</code></pre> <p><code>prepareCommit</code>...FIXME</p> <p><code>prepareCommit</code>\u00a0is part of the Task abstraction.</p>"},{"location":"processor/StreamThread/","text":"<p><code>StreamThread</code> is a <code>Thread</code> (Java).</p>","title":"StreamThread"},{"location":"processor/StreamThread/#creating-instance","text":"<p><code>StreamThread</code> takes the following to be created:</p> <ul> <li> <code>Time</code> <li> StreamsConfig <li> <code>Admin</code> <li> Main <code>Consumer&lt;byte[], byte[]&gt;</code> <li> Restore <code>Consumer&lt;byte[], byte[]&gt;</code> <li> <code>ChangelogReader</code> <li> <code>originalReset</code> <li> TaskManager <li> <code>StreamsMetricsImpl</code> <li> <code>InternalTopologyBuilder</code> <li> Thread ID <li> <code>LogContext</code> <li> <code>assignmentErrorCode</code> <li> <code>nextProbingRebalanceMs</code> <li> Shutdown Error Hook <li> <code>java.util.function.Consumer&lt;Throwable&gt;</code> <li> <code>java.util.function.Consumer&lt;Long&gt;</code>  <p><code>StreamThread</code> is created\u00a0using create utility.</p>","title":"Creating Instance"},{"location":"processor/StreamThread/#creating-streamthread","text":"","title":"Creating StreamThread <pre><code>StreamThread create(\n  InternalTopologyBuilder builder,\n  StreamsConfig config,\n  KafkaClientSupplier clientSupplier,\n  Admin adminClient,\n  UUID processId,\n  String clientId,\n  StreamsMetricsImpl streamsMetrics,\n  Time time,\n  StreamsMetadataState streamsMetadataState,\n  long cacheSizeBytes,\n  StateDirectory stateDirectory,\n  StateRestoreListener userStateRestoreListener,\n  int threadIdx,\n  Runnable shutdownErrorHook,\n  java.util.function.Consumer&lt;Throwable&gt; streamsUncaughtExceptionHandler)\n</code></pre> <p><code>create</code> prints out the following INFO message to the logs:</p> <pre><code>Creating restore consumer client\n</code></pre> <p><code>create</code> requests the given <code>StreamsConfig</code> for the restore consumer configs (with getRestoreConsumerClientId) and requests the given KafkaClientSupplier for a restore consumer.</p> <p><code>create</code> creates a StoreChangelogReader.</p> <p><code>create</code> creates a ThreadCache.</p> <p><code>create</code> creates a ActiveTaskCreator, a StandbyTaskCreator and a TaskManager.</p> <p><code>create</code> prints out the following INFO message to the logs:</p> <pre><code>Creating consumer client\n</code></pre> <p><code>create</code>...FIXME</p> <p><code>create</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is requested to createAndAddStreamThread</li> </ul>"},{"location":"processor/StreamThread/#starting-execution","text":"","title":"Starting Execution <pre><code>void run()\n</code></pre> <p><code>run</code>...FIXME</p> <p><code>run</code>\u00a0is part of the <code>Thread</code> (Java) abstraction.</p>"},{"location":"processor/StreamThread/#runloop","text":"","title":"runLoop <pre><code>void runLoop()\n</code></pre> <p><code>runLoop</code>...FIXME</p>"},{"location":"processor/StreamThread/#runonce","text":"","title":"runOnce <pre><code>void runOnce()\n</code></pre> <p><code>runOnce</code>...FIXME</p>"},{"location":"processor/StreamThread/#maybecommit","text":"","title":"maybeCommit <pre><code>int maybeCommit()\n</code></pre> <p><code>maybeCommit</code>...FIXME</p>"},{"location":"processor/StreamThread/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.processor.internals.StreamThread</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.processor.internals.StreamThread=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"processor/Task/","text":"<p><code>Task</code> is an abstraction of tasks.</p>","title":"Task"},{"location":"processor/Task/#contract","text":"","title":"Contract"},{"location":"processor/Task/#addrecords","text":"","title":"addRecords <pre><code>void addRecords(\n  TopicPartition partition,\n  Iterable&lt;ConsumerRecord&lt;byte[], byte[]&gt;&gt; records)\n</code></pre> <p>Used when:</p> <ul> <li><code>TaskManager</code> is requested to addRecordsToTasks</li> <li><code>TopologyTestDriver</code> is requested to enqueueTaskRecord</li> </ul>"},{"location":"processor/Task/#changelogoffsets","text":"","title":"changelogOffsets <pre><code>Map&lt;TopicPartition, Long&gt; changelogOffsets()\n</code></pre>"},{"location":"processor/Task/#changelogpartitions","text":"","title":"changelogPartitions <pre><code>Collection&lt;TopicPartition&gt; changelogPartitions()\n</code></pre>"},{"location":"processor/Task/#cleartasktimeout","text":"","title":"clearTaskTimeout <pre><code>void clearTaskTimeout()\n</code></pre>"},{"location":"processor/Task/#closeclean","text":"","title":"closeClean <pre><code>void closeClean()\n</code></pre>"},{"location":"processor/Task/#closecleanandrecyclestate","text":"","title":"closeCleanAndRecycleState <pre><code>void closeCleanAndRecycleState()\n</code></pre>"},{"location":"processor/Task/#closedirty","text":"","title":"closeDirty <pre><code>void closeDirty()\n</code></pre>"},{"location":"processor/Task/#commitneeded","text":"","title":"commitNeeded <pre><code>boolean commitNeeded()\n</code></pre>"},{"location":"processor/Task/#committedoffsets","text":"","title":"committedOffsets <pre><code>Map&lt;TopicPartition, Long&gt; committedOffsets()\n</code></pre>"},{"location":"processor/Task/#completerestoration","text":"","title":"completeRestoration <pre><code>void completeRestoration(\n  java.util.function.Consumer&lt;Set&lt;TopicPartition&gt;&gt; offsetResetter)\n</code></pre>"},{"location":"processor/Task/#getstore","text":"","title":"getStore <pre><code>StateStore getStore(\n  String name)\n</code></pre> <p>Used when:</p> <ul> <li><code>StreamThreadStateStoreProvider</code> is requested for stores</li> </ul>"},{"location":"processor/Task/#highwatermark","text":"","title":"highWaterMark <pre><code>Map&lt;TopicPartition, Long&gt; highWaterMark()\n</code></pre>"},{"location":"processor/Task/#taskid","text":"","title":"TaskId <pre><code>TaskId id()\n</code></pre>"},{"location":"processor/Task/#initializeifneeded","text":"","title":"initializeIfNeeded <pre><code>void initializeIfNeeded()\n</code></pre>"},{"location":"processor/Task/#inputpartitions","text":"","title":"inputPartitions <pre><code>Set&lt;TopicPartition&gt; inputPartitions()\n</code></pre>"},{"location":"processor/Task/#isactive","text":"","title":"isActive <pre><code>boolean isActive()\n</code></pre>"},{"location":"processor/Task/#markchangelogascorrupted","text":"","title":"markChangelogAsCorrupted <pre><code>void markChangelogAsCorrupted(\n  Collection&lt;TopicPartition&gt; partitions)\n</code></pre>"},{"location":"processor/Task/#markchangelogascorrupted_1","text":"","title":"markChangelogAsCorrupted <pre><code>void maybeInitTaskTimeoutOrThrow(\n  long currentWallClockMs,\n  Exception cause)\n</code></pre>"},{"location":"processor/Task/#postcommit","text":"","title":"postCommit <pre><code>void postCommit(\n  boolean enforceCheckpoint)\n</code></pre>"},{"location":"processor/Task/#preparecommit","text":"","title":"prepareCommit <pre><code>Map&lt;TopicPartition, OffsetAndMetadata&gt; prepareCommit()\n</code></pre> <p>Used when:</p> <ul> <li><code>TaskManager</code> is requested to closeDirtyAndRevive, handleCloseAndRecycle, prepareCommitAndAddOffsetsToMap, closeTaskDirty, tryCloseCleanAllActiveTasks, tryCloseCleanAllStandbyTasks and commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</li> <li><code>TopologyTestDriver</code> is requested to completeAllProcessableWork, advanceWallClockTime and close</li> </ul>"},{"location":"processor/Task/#resume","text":"","title":"resume <pre><code>void resume()\n</code></pre>"},{"location":"processor/Task/#revive","text":"","title":"revive <pre><code>void revive()\n</code></pre>"},{"location":"processor/Task/#state","text":"","title":"state <pre><code>State state()\n</code></pre>"},{"location":"processor/Task/#suspend","text":"","title":"suspend <pre><code>void suspend()\n</code></pre>"},{"location":"processor/Task/#timecurrentidlingstarted","text":"","title":"timeCurrentIdlingStarted <pre><code>Optional&lt;Long&gt; timeCurrentIdlingStarted()\n</code></pre>"},{"location":"processor/Task/#updateinputpartitions","text":"","title":"updateInputPartitions <pre><code>void updateInputPartitions(\n  Set&lt;TopicPartition&gt; topicPartitions,\n  Map&lt;String, List&lt;String&gt;&gt; allTopologyNodesToSourceTopics)\n</code></pre>"},{"location":"processor/Task/#implementations","text":"<ul> <li>AbstractTask</li> <li>StandbyTask</li> <li>StreamTask</li> </ul>","title":"Implementations"},{"location":"processor/TaskManager/","text":"","title":"TaskManager"},{"location":"processor/TaskManager/#creating-instance","text":"<p><code>TaskManager</code> takes the following to be created:</p> <ul> <li> <code>Time</code> <li> <code>ChangelogReader</code> <li> Process UUID <li> Log Prefix <li> <code>StreamsMetricsImpl</code> <li> ActiveTaskCreator <li> StandbyTaskCreator <li> <code>InternalTopologyBuilder</code> <li> <code>Admin</code> <li> <code>StateDirectory</code> <li> <code>StreamThread.ProcessingMode</code>  <p><code>TaskManager</code> is created\u00a0when:</p> <ul> <li><code>StreamThread</code> utility is used to create a StreamThread</li> </ul>","title":"Creating Instance"},{"location":"processor/TaskManager/#committing-tasks","text":"","title":"Committing Tasks <pre><code>int commit(\n  Collection&lt;Task&gt; tasksToCommit)\n</code></pre> <p><code>commit</code>...FIXME</p> <p><code>commit</code>\u00a0is used when:</p> <ul> <li><code>StreamThread</code> is requested to maybeCommit</li> <li><code>TaskManager</code> is requested to maybeCommitActiveTasksPerUserRequested</li> </ul>"},{"location":"processor/TaskManager/#handleassignment","text":"","title":"handleAssignment <pre><code>void handleAssignment(\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; activeTasks,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; standbyTasks)\n</code></pre> <p><code>handleAssignment</code>...FIXME</p> <p><code>handleAssignment</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to onAssignment</li> </ul>"},{"location":"processor/TaskManager/#handlecloseandrecycle","text":"","title":"handleCloseAndRecycle <pre><code>void handleCloseAndRecycle(\n  Set&lt;Task&gt; tasksToRecycle,\n  Set&lt;Task&gt; tasksToCloseClean,\n  Set&lt;Task&gt; tasksToCloseDirty,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; activeTasksToCreate,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; standbyTasksToCreate,\n  LinkedHashMap&lt;TaskId, RuntimeException&gt; taskCloseExceptions)\n</code></pre> <p><code>handleCloseAndRecycle</code>...FIXME</p>"},{"location":"processor/TaskManager/#handlecorruption","text":"","title":"handleCorruption <pre><code>void handleCorruption(\n  Set&lt;TaskId&gt; corruptedTasks)\n</code></pre> <p><code>handleCorruption</code>...FIXME</p> <p><code>handleCorruption</code> is used when:</p> <ul> <li><code>StreamThread</code> is requested to runLoop (and caught a <code>TaskCorruptedException</code>)</li> </ul>"},{"location":"processor/TaskManager/#maybecommitactivetasksperuserrequested","text":"","title":"maybeCommitActiveTasksPerUserRequested <pre><code>int maybeCommitActiveTasksPerUserRequested()\n</code></pre> <p><code>maybeCommitActiveTasksPerUserRequested</code>...FIXME</p> <p><code>maybeCommitActiveTasksPerUserRequested</code> is used when:</p> <ul> <li><code>StreamThread</code> is requested to maybeCommit</li> </ul>"},{"location":"processor/TaskManager/#commitandfillinconsumedoffsetsandmetadatapertaskmap","text":"","title":"commitAndFillInConsumedOffsetsAndMetadataPerTaskMap <pre><code>int commitAndFillInConsumedOffsetsAndMetadataPerTaskMap(\n  Collection&lt;Task&gt; tasksToCommit,\n  Map&lt;Task, Map&lt;TopicPartition, OffsetAndMetadata&gt;&gt; consumedOffsetsAndMetadataPerTask)\n</code></pre> <p><code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code>...FIXME</p> <p><code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> is used when:</p> <ul> <li><code>TaskManager</code> is requested to handleCorruption and commit</li> </ul>"},{"location":"processor/Tasks/","text":"","title":"Tasks"},{"location":"processor/Tasks/#createtasks","text":"","title":"createTasks <pre><code>void createTasks(\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; activeTasksToCreate,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; standbyTasksToCreate)\n</code></pre> <p><code>createTasks</code>...FIXME</p> <p><code>createTasks</code>\u00a0is used when:</p> <ul> <li><code>TaskManager</code> is requested to handleAssignment</li> </ul>"},{"location":"processor/Tasks/#convertstandbytoactive","text":"","title":"convertStandbyToActive <pre><code>void convertStandbyToActive(\n  StandbyTask standbyTask,\n  Set&lt;TopicPartition&gt; partitions)\n</code></pre> <p><code>convertStandbyToActive</code>...FIXME</p> <p><code>convertStandbyToActive</code>\u00a0is used when:</p> <ul> <li><code>TaskManager</code> is requested to handleCloseAndRecycle</li> </ul>"},{"location":"processor/TimestampExtractor/","text":"<p><code>TimestampExtractor</code> is...FIXME</p>","title":"TimestampExtractor"},{"location":"state/AbstractStoreBuilder/","text":"<p><code>AbstractStoreBuilder</code> is...FIXME</p>","title":"AbstractStoreBuilder"},{"location":"state/CachedStateStore/","text":"<p><code>CachedStateStore</code> is an abstraction of cached state stores.</p>","title":"CachedStateStore"},{"location":"state/CachedStateStore/#contract","text":"","title":"Contract"},{"location":"state/CachedStateStore/#flushcache","text":"","title":"flushCache <pre><code>void flushCache()\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorStateManager</code> is requested to flush store caches</li> </ul>"},{"location":"state/CachedStateStore/#setflushlistener","text":"","title":"setFlushListener <pre><code>boolean setFlushListener(\n  CacheFlushListener&lt;K, V&gt; listener,\n  boolean sendOldValues)\n</code></pre>"},{"location":"state/CachedStateStore/#implementations","text":"<ul> <li><code>CachingKeyValueStore</code></li> <li><code>CachingSessionStore</code></li> <li><code>CachingWindowStore</code></li> <li>WrappedStateStore</li> </ul>","title":"Implementations"},{"location":"state/InMemoryWindowBytesStoreSupplier/","text":"<p><code>InMemoryWindowBytesStoreSupplier</code> is a WindowBytesStoreSupplier.</p>","title":"InMemoryWindowBytesStoreSupplier"},{"location":"state/InMemoryWindowBytesStoreSupplier/#creating-instance","text":"<p><code>InMemoryWindowBytesStoreSupplier</code> takes the following to be created:</p> <ul> <li> Name <li> <code>retentionPeriod</code> <li> <code>windowSize</code> <li> <code>retainDuplicates</code>  <p><code>InMemoryWindowBytesStoreSupplier</code> is created\u00a0when:</p> <ul> <li><code>Stores</code> is requested for in-memory window store</li> </ul>","title":"Creating Instance"},{"location":"state/KeyValueStore/","text":"<p><code>KeyValueStore&lt;K, V&gt;</code>\u00a0is an extension of the StateStore and ReadOnlyKeyValueStore abstractions for read-only key-value stores.</p>","title":"KeyValueStore"},{"location":"state/KeyValueStore/#contract","text":"","title":"Contract"},{"location":"state/KeyValueStore/#delete","text":"","title":"delete <pre><code>V delete(\n  K key)\n</code></pre>"},{"location":"state/KeyValueStore/#put","text":"","title":"put <pre><code>void put(\n  K key,\n  V value)\n</code></pre>"},{"location":"state/KeyValueStore/#putall","text":"","title":"putAll <pre><code>void putAll(\n  List&lt;KeyValue&lt;K, V&gt;&gt; entries)\n</code></pre>"},{"location":"state/KeyValueStore/#putifabsent","text":"","title":"putIfAbsent <pre><code>V putIfAbsent(\n  K key,\n  V value)\n</code></pre>"},{"location":"state/KeyValueStore/#implementations","text":"<ul> <li><code>CachingKeyValueStore</code></li> <li><code>ChangeLoggingKeyValueBytesStore</code></li> <li><code>InMemoryKeyValueStore</code></li> <li><code>InMemoryTimestampedKeyValueStoreMarker</code></li> <li><code>KeyValueStoreFacade</code></li> <li><code>KeyValueStoreReadOnlyDecorator</code></li> <li><code>KeyValueStoreReadWriteDecorator</code></li> <li><code>KeyValueToTimestampedKeyValueByteStoreAdapter</code></li> <li><code>MemoryLRUCache</code></li> <li><code>MeteredKeyValueStore</code></li> <li><code>RocksDBStore</code></li> <li><code>Segment</code></li> <li><code>TimestampedKeyValueStore</code></li> </ul>","title":"Implementations"},{"location":"state/QueryableStoreProvider/","text":"<p><code>QueryableStoreProvider</code> is...FIXME</p>","title":"QueryableStoreProvider"},{"location":"state/ReadOnlyKeyValueStore/","text":"<p><code>ReadOnlyKeyValueStore</code> is...FIXME</p>","title":"ReadOnlyKeyValueStore"},{"location":"state/StateSerdes/","text":"<p><code>StateSerdes&lt;K, V&gt;</code> is a factory for creating serializers and deserializers for state stores in Kafka Streams.</p>","title":"StateSerdes"},{"location":"state/StateSerdes/#demo","text":"<pre><code>import org.apache.kafka.streams.state.StateSerdes\nimport java.lang.{Long =&gt; JLong}\nval stateSerdes = StateSerdes.withBuiltinTypes[JLong, String](\"topicName\", classOf[JLong], classOf[String])\n</code></pre> <pre><code>scala&gt; :type stateSerdes\norg.apache.kafka.streams.state.StateSerdes[Long,String]\n</code></pre>","title":"Demo"},{"location":"state/StateSerdes/#creating-instance","text":"<p><code>StateSerdes</code> takes the following to be created:</p> <ul> <li> Topic Name <li> Key <code>Serde</code> <li> Value <code>Serde</code>  <p><code>StateSerdes</code> is created\u00a0when:</p> <ul> <li><code>CachingWindowStore</code> is requested to <code>initInternal</code></li> <li><code>MeteredKeyValueStore</code> is requested to <code>initStoreSerde</code></li> <li><code>MeteredSessionStore</code> is requested to <code>initStoreSerde</code></li> <li><code>MeteredWindowStore</code> is requested to <code>initStoreSerde</code></li> <li>withBuiltinTypes</li> </ul>","title":"Creating Instance"},{"location":"state/StateSerdes/#withbuiltintypes","text":"","title":"withBuiltinTypes <pre><code>StateSerdes&lt;K, V&gt; withBuiltinTypes(\n  String topic,\n  Class&lt;K&gt; keyClass,\n  Class&lt;V&gt; valueClass)\n</code></pre> <p><code>withBuiltinTypes</code> creates a StateSerdes using <code>Serdes.serdeFrom</code> utility with the given key and value classes.</p>"},{"location":"state/StoreBuilder/","text":"<p><code>StoreBuilder</code> is an abstraction of builders of StateStores (with optional caching and logging).</p> <pre><code>StoreBuilder&lt;T extends StateStore&gt;\n</code></pre>","title":"StoreBuilder"},{"location":"state/StoreBuilder/#contract","text":"","title":"Contract"},{"location":"state/StoreBuilder/#building-statestore","text":"","title":"Building StateStore <pre><code>T build()\n</code></pre> <p>Used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to rewriteTopology (and build global state stores)</li> <li><code>StateStoreFactory</code> is requested to build a StateStore</li> </ul>"},{"location":"state/StoreBuilder/#logconfig","text":"","title":"logConfig <pre><code>Map&lt;String, String&gt; logConfig()\n</code></pre>"},{"location":"state/StoreBuilder/#loggingenabled","text":"","title":"loggingEnabled <pre><code>boolean loggingEnabled()\n</code></pre>"},{"location":"state/StoreBuilder/#name","text":"","title":"name <pre><code>String name()\n</code></pre>"},{"location":"state/StoreBuilder/#withcachingdisabled","text":"","title":"withCachingDisabled <pre><code>StoreBuilder&lt;T&gt; withCachingDisabled()\n</code></pre>"},{"location":"state/StoreBuilder/#withcachingenabled","text":"","title":"withCachingEnabled <pre><code>StoreBuilder&lt;T&gt; withCachingEnabled()\n</code></pre>"},{"location":"state/StoreBuilder/#withloggingdisabled","text":"","title":"withLoggingDisabled <pre><code>StoreBuilder&lt;T&gt; withLoggingDisabled()\n</code></pre>"},{"location":"state/StoreBuilder/#withloggingenabled","text":"","title":"withLoggingEnabled <pre><code>StoreBuilder&lt;T&gt; withLoggingEnabled(\n  Map&lt;String, String&gt; config)\n</code></pre>"},{"location":"state/StoreBuilder/#implementations","text":"<ul> <li>AbstractStoreBuilder</li> </ul>","title":"Implementations"},{"location":"state/StoreSupplier/","text":"<p><code>StoreSupplier</code> is an abstraction of suppliers of StateStores.</p>","title":"StoreSupplier"},{"location":"state/StoreSupplier/#contract","text":"","title":"Contract"},{"location":"state/StoreSupplier/#creating-statestore","text":"","title":"Creating StateStore <pre><code>T get()\n</code></pre> <p>Used when:</p> <ul> <li><code>KStreamImplJoin</code> is requested to <code>sharedOuterJoinWindowStoreBuilder</code></li> <li><code>KeyValueStoreBuilder</code> is requested to <code>build</code></li> <li><code>SessionStoreBuilder</code> is requested to <code>build</code></li> <li><code>TimeOrderedWindowStoreBuilder</code> is requested to <code>build</code></li> <li><code>TimestampedKeyValueStoreBuilder</code> is requested to <code>build</code></li> <li><code>TimestampedWindowStoreBuilder</code> is requested to <code>build</code></li> <li><code>WindowStoreBuilder</code> is requested to <code>build</code></li> </ul>"},{"location":"state/StoreSupplier/#metricsscope","text":"","title":"metricsScope <pre><code>String metricsScope()\n</code></pre>"},{"location":"state/StoreSupplier/#name","text":"","title":"name <pre><code>String name()\n</code></pre>"},{"location":"state/StoreSupplier/#implementations","text":"<ul> <li>KeyValueBytesStoreSupplier</li> <li>SessionBytesStoreSupplier</li> <li>WindowBytesStoreSupplier</li> </ul>","title":"Implementations"},{"location":"state/Stores/","text":"<p><code>Stores</code> is a factory for creating state stores in Kafka Streams.</p>","title":"Stores"},{"location":"state/Stores/#inmemorywindowstore","text":"","title":"inMemoryWindowStore <pre><code>WindowBytesStoreSupplier inMemoryWindowStore(\n  String name,\n  Duration retentionPeriod,\n  Duration windowSize,\n  boolean retainDuplicates)\n</code></pre> <p><code>inMemoryWindowStore</code>...FIXME</p> <p><code>inMemoryWindowStore</code>\u00a0is used when:</p> <ul> <li><code>KStreamImplJoin</code> is requested to <code>sharedOuterJoinWindowStoreBuilder</code> (for left outer join)</li> </ul>"},{"location":"state/StreamThreadStateStoreProvider/","text":"<p><code>StreamThreadStateStoreProvider</code> is...FIXME</p>","title":"StreamThreadStateStoreProvider"},{"location":"state/ThreadCache/","text":"<p><code>ThreadCache</code> is...FIXME</p>","title":"ThreadCache"},{"location":"state/TimestampedKeyValueStore/","text":"<p><code>TimestampedKeyValueStore</code> is...FIXME</p>","title":"TimestampedKeyValueStore"},{"location":"state/ValueAndTimestamp/","text":"<p><code>ValueAndTimestamp</code> is...FIXME</p>","title":"ValueAndTimestamp"},{"location":"state/WindowBytesStoreSupplier/","text":"<p><code>WindowBytesStoreSupplier</code>\u00a0is an extension of the StoreSupplier abstraction for state store suppliers of WindowStores (<code>WindowStore&lt;Bytes, byte[]&gt;</code>s).</p>","title":"WindowBytesStoreSupplier"},{"location":"state/WindowBytesStoreSupplier/#contract","text":"","title":"Contract"},{"location":"state/WindowBytesStoreSupplier/#retainduplicates","text":"","title":"retainDuplicates <pre><code>boolean retainDuplicates()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#retentionperiod","text":"","title":"retentionPeriod <pre><code>long retentionPeriod()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#segmentintervalms","text":"","title":"segmentIntervalMs <pre><code>long segmentIntervalMs()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#windowsize","text":"","title":"windowSize <pre><code>long windowSize()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#implementations","text":"<ul> <li>RocksDbWindowBytesStoreSupplier</li> <li>InMemoryWindowBytesStoreSupplier</li> </ul>","title":"Implementations"},{"location":"state/WindowStore/","text":"<p><code>WindowStore</code> is...FIXME</p>","title":"WindowStore"},{"location":"state/WrappedStateStore/","text":"<p><code>WrappedStateStore</code> is an extension of the StateStore and CachedStateStore abstractions for state stores that hold (wrap) another state store.</p>","title":"WrappedStateStore"},{"location":"state/WrappedStateStore/#implementations","text":"<ul> <li><code>AbstractReadOnlyDecorator</code></li> <li><code>AbstractReadWriteDecorator</code></li> <li><code>CachingKeyValueStore</code></li> <li><code>CachingSessionStore</code></li> <li><code>CachingWindowStore</code></li> <li><code>ChangeLoggingKeyValueBytesStore</code></li> <li><code>ChangeLoggingSessionBytesStore</code></li> <li><code>ChangeLoggingWindowBytesStore</code></li> <li><code>MeteredKeyValueStore</code></li> <li><code>MeteredSessionStore</code></li> <li><code>MeteredWindowStore</code></li> <li><code>RocksDBSessionStore</code></li> <li><code>RocksDBTimeOrderedWindowStore</code></li> <li><code>RocksDBWindowStore</code></li> </ul>","title":"Implementations"},{"location":"state/WrappedStateStore/#creating-instance","text":"<p><code>WrappedStateStore</code> takes the following to be created:</p> <ul> <li> StateStore  Abstract Class<p><code>WrappedStateStore</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete WrappedStateStores.</p>","title":"Creating Instance"},{"location":"state/WrappedStateStore/#setflushlistener","text":"","title":"setFlushListener <pre><code>boolean setFlushListener(\n  CacheFlushListener&lt;K, V&gt; listener,\n  boolean sendOldValues)\n</code></pre> <p><code>setFlushListener</code> returns <code>false</code> for the wrapped state store being of any type but a CachedStateStore.</p> <p>Otherwise, <code>setFlushListener</code> returns the value of requesting the CachedStateStore to setFlushListener.</p> <p><code>setFlushListener</code>\u00a0is part of the CachedStateStore abstraction.</p>"}]}