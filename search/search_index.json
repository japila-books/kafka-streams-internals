{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","tags":false},"docs":[{"location":"","text":"<p>Welcome to The Internals of Kafka Streams online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark, Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Kafka Streams as much as I have.</p>  <p>Flannery O'Connor</p> <p>I write to discover what I know.</p>   \"The Internals Of\" series <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p>  <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Kafka Streams \ud83d\udd25</p>  <p>Last update: 2022-03-25</p>","title":"The Internals of Kafka Streams 3.1.0"},{"location":"AbstractTask/","text":"<p><code>AbstractTask</code>\u00a0is a base abstraction of the Task abstraction for tasks.</p>","title":"AbstractTask"},{"location":"AbstractTask/#implementations","text":"<ul> <li>StandbyTask</li> <li>StreamTask</li> </ul>","title":"Implementations"},{"location":"AbstractTask/#creating-instance","text":"<p><code>AbstractTask</code> takes the following to be created:</p> <ul> <li> TaskId <li> ProcessorTopology <li> StateDirectory <li> ProcessorStateManager <li>Input Partitions</li> <li> task.timeout.ms configuration property <li> Task Type <li> <code>AbstractTask</code> Implementation   Abstract Class <p><code>AbstractTask</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete AbstractTasks.</p>","title":"Creating Instance"},{"location":"AbstractTask/#input-partitions","text":"","title":"Input Partitions <p><code>AbstractTask</code> is given a set of input <code>TopicPartition</code>s to consume and process records from when created and later when updateInputPartitions.</p>"},{"location":"ActiveTaskCreator/","text":"<p><code>ActiveTaskCreator</code> is used by TaskManager.</p>","title":"ActiveTaskCreator"},{"location":"ActiveTaskCreator/#creating-instance","text":"<p><code>ActiveTaskCreator</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder <li> StreamsConfig <li> StreamsMetricsImpl <li> StateDirectory <li> ChangelogReader <li> ThreadCache <li> <code>Time</code> <li> KafkaClientSupplier <li> Thread ID <li> Process ID <li> <code>Logger</code>  <p><code>ActiveTaskCreator</code> is created\u00a0when:</p> <ul> <li><code>StreamThread</code> utility is used to create a StreamThread (for a TaskManager)</li> </ul>","title":"Creating Instance"},{"location":"ActiveTaskCreator/#streamsproducer","text":"","title":"StreamsProducer <p><code>ActiveTaskCreator</code> creates a StreamsProducer when created.</p>"},{"location":"ActiveTaskCreator/#createtasks","text":"","title":"createTasks <pre><code>Collection&lt;Task&gt; createTasks(\n  Consumer&lt;byte[], byte[]&gt; consumer,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; tasksToBeCreated)\n</code></pre> <p>For every TaskId and <code>TopicPartition</code>s pair (in the given <code>tasksToBeCreated</code> collection), <code>createTasks</code> requests the InternalTopologyBuilder to buildSubtopology. <code>createTasks</code> createActiveTask (with the ProcessorTopology, a new ProcessorStateManager and ProcessorContextImpl).</p> <p>In the end, <code>createTasks</code> returns the newly-created StreamTasks.</p> <p><code>createTasks</code>\u00a0is used when:</p> <ul> <li><code>Tasks</code> is requested to createTasks</li> </ul>"},{"location":"ActiveTaskCreator/#createactivetaskfromstandby","text":"","title":"createActiveTaskFromStandby <pre><code>StreamTask createActiveTaskFromStandby(\n  StandbyTask standbyTask,\n  Set&lt;TopicPartition&gt; inputPartitions,\n  Consumer&lt;byte[], byte[]&gt; consumer)\n</code></pre> <p><code>createActiveTaskFromStandby</code>...FIXME</p> <p><code>createActiveTaskFromStandby</code>\u00a0is used when:</p> <ul> <li><code>Tasks</code> is requested to convertStandbyToActive</li> </ul>"},{"location":"ActiveTaskCreator/#creating-active-streamtask","text":"","title":"Creating Active StreamTask <pre><code>StreamTask createActiveTask(\n  TaskId taskId,\n  Set&lt;TopicPartition&gt; inputPartitions,\n  Consumer&lt;byte[], byte[]&gt; consumer,\n  LogContext logContext,\n  ProcessorTopology topology,\n  ProcessorStateManager stateManager,\n  InternalProcessorContext context)\n</code></pre> <p><code>createActiveTask</code> determines whether to create a new StreamsProducer or use the existing one based on ProcessingMode (<code>ProcessingMode.EXACTLY_ONCE_ALPHA</code> or not, respectively).</p> <p><code>createActiveTask</code> creates a RecordCollectorImpl.</p> <p><code>createActiveTask</code> creates a StreamTask.</p> <p><code>createActiveTask</code> prints out the following TRACE message to the logs:</p> <pre><code>Created task [taskId] with assigned partitions [inputPartitions]\n</code></pre> <p>In the end, <code>createActiveTask</code> requests the createTaskSensor to record this occurrence.</p> <p><code>createActiveTask</code>\u00a0is used when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createTasks and createActiveTaskFromStandby</li> </ul>"},{"location":"ActiveTaskCreator/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.processor.internals.ActiveTaskCreator</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.processor.internals.ActiveTaskCreator=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"AssignorConfiguration/","text":"","title":"AssignorConfiguration"},{"location":"AssignorConfiguration/#creating-instance","text":"<p><code>AssignorConfiguration</code> takes the following to be created:</p> <ul> <li> Configuration Properties  <p>While being created, <code>AssignorConfiguration</code> creates a new <code>LogContext</code> with the following log prefix (based on client.id):</p> <pre><code>stream-thread [client.id]\n</code></pre> <p><code>AssignorConfiguration</code> uses the given configs to look up __reference.container.instance__ internal property to be the ReferenceContainer.</p> <p><code>AssignorConfiguration</code>...FIXME (<code>INTERNAL_TASK_ASSIGNOR_CLASS</code>)</p>  <p><code>AssignorConfiguration</code> is created\u00a0when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to configure</li> </ul>","title":"Creating Instance"},{"location":"AssignorConfiguration/#referencecontainer","text":"","title":"ReferenceContainer <p><code>AssignorConfiguration</code> looks up a ReferenceContainer when created.</p> <p>The <code>ReferenceContainer</code> is used in the following:</p> <ul> <li>Create an InternalTopicManager</li> <li><code>StreamsPartitionAssignor</code> is requested to configure</li> </ul>"},{"location":"AssignorConfiguration/#rebalanceprotocol","text":"","title":"rebalanceProtocol <pre><code>RebalanceProtocol rebalanceProtocol()\n</code></pre> <p><code>rebalanceProtocol</code> takes the value of upgrade.from configuration property (from the StreamsConfig).</p> <p>Unless <code>upgrade.from</code> is defined, <code>rebalanceProtocol</code> prints out the following INFO message to the logs and returns <code>RebalanceProtocol.COOPERATIVE</code>.</p> <pre><code>Cooperative rebalancing enabled now\n</code></pre> <p>With <code>upgrade.from</code> defined, <code>rebalanceProtocol</code>...FIXME</p> <p><code>rebalanceProtocol</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to configure</li> </ul>"},{"location":"AssignorConfiguration/#copartitionedtopicsenforcer","text":"","title":"CopartitionedTopicsEnforcer <pre><code>CopartitionedTopicsEnforcer copartitionedTopicsEnforcer()\n</code></pre> <p><code>copartitionedTopicsEnforcer</code> creates a new CopartitionedTopicsEnforcer (with the logPrefix).</p> <p><code>copartitionedTopicsEnforcer</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to configure</li> </ul>"},{"location":"AssignorConfiguration/#internaltopicmanager","text":"","title":"InternalTopicManager <pre><code>InternalTopicManager internalTopicManager()\n</code></pre> <p><code>internalTopicManager</code> creates a new InternalTopicManager.</p> <p><code>internalTopicManager</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to configure</li> </ul>"},{"location":"ChangelogReader/","text":"<p><code>ChangelogReader</code> is...FIXME</p>","title":"ChangelogReader"},{"location":"ChangelogTopics/","text":"","title":"ChangelogTopics"},{"location":"ChangelogTopics/#creating-instance","text":"<p><code>ChangelogTopics</code> takes the following to be created:</p> <ul> <li> InternalTopicManager <li> Topic Groups (<code>Map&lt;Subtopology, TopicsInfo&gt;</code>) <li> Tasks for Topic Groups (<code>Map&lt;Subtopology, Set&lt;TaskId&gt;&gt;</code>) <li> Log Prefix  <p><code>ChangelogTopics</code> is created\u00a0when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to assignTasksToClients</li> </ul>","title":"Creating Instance"},{"location":"ChangelogTopics/#setup","text":"","title":"setup <pre><code>void setup()\n</code></pre> <p><code>setup</code>...FIXME</p> <p><code>setup</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested for consumer group assignment (and assignTasksToClients)</li> </ul>"},{"location":"CopartitionedTopicsEnforcer/","text":"","title":"CopartitionedTopicsEnforcer"},{"location":"CopartitionedTopicsEnforcer/#creating-instance","text":"<p><code>CopartitionedTopicsEnforcer</code> takes the following to be created:</p> <ul> <li> Log Prefix  <p><code>CopartitionedTopicsEnforcer</code> is created\u00a0when:</p> <ul> <li><code>AssignorConfiguration</code> is requested to copartitionedTopicsEnforcer</li> </ul>","title":"Creating Instance"},{"location":"InternalConfig/","text":"","title":"InternalConfig"},{"location":"InternalConfig/#__referencecontainerinstance__","text":"","title":"__reference.container.instance__ <p>ReferenceContainer that <code>StreamThread</code> utility creates when requested to create a StreamThread.</p> <p>Used when <code>AssignorConfiguration</code> is created.</p>"},{"location":"InternalTopicManager/","text":"","title":"InternalTopicManager"},{"location":"InternalTopicManager/#creating-instance","text":"<p><code>InternalTopicManager</code> takes the following to be created:</p> <ul> <li> <code>Time</code> <li> <code>Admin</code> (Apache Kafka) <li> StreamsConfig  <p><code>InternalTopicManager</code> is created\u00a0when:</p> <ul> <li><code>AssignorConfiguration</code> is requested for an InternalTopicManager</li> </ul>","title":"Creating Instance"},{"location":"InternalTopicManager/#makeready","text":"","title":"makeReady <pre><code>Set&lt;String&gt; makeReady(\n  Map&lt;String, InternalTopicConfig&gt; topics)\n</code></pre> <p><code>makeReady</code>...FIXME</p> <p><code>makeReady</code>\u00a0is used when:</p> <ul> <li><code>ChangelogTopics</code> is requested to setup</li> <li><code>RepartitionTopics</code> is requested to setup</li> </ul>"},{"location":"InternalTopologyBuilder/","text":"<p><code>InternalTopologyBuilder</code> uses the nodeFactories internal registry and up to building a ProcessorTopology the nodes are merely names in a \"topology namespace\" (with no knowledge about real nodes in a topology except their names).</p>","title":"InternalTopologyBuilder"},{"location":"InternalTopologyBuilder/#creating-instance","text":"<p><code>InternalTopologyBuilder</code> takes the following to be created:</p> <ul> <li> Topology Name  <p><code>InternalTopologyBuilder</code> is created\u00a0when:</p> <ul> <li><code>Topology</code> is created</li> </ul>","title":"Creating Instance"},{"location":"InternalTopologyBuilder/#nodefactories","text":"","title":"nodeFactories <p><code>InternalTopologyBuilder</code> uses a <code>nodeFactories</code> internal registry of NodeFactoryies by name.</p> <p>A new <code>NodeFactory</code> is added when:</p> <ul> <li>addSource</li> <li>addSink</li> <li>addProcessor</li> <li>addGlobalStore</li> </ul> <p>Used when:</p> <ul> <li>connectProcessorAndStateStore</li> <li>findSourcesForProcessorPredecessors</li> <li>makeNodeGroups</li> <li>build</li> <li>setRegexMatchedTopicsToSourceNodes</li> <li>isGlobalSource</li> <li>describeGlobalStore</li> <li>describeSubtopology</li> </ul>"},{"location":"InternalTopologyBuilder/#node-groups","text":"","title":"Node Groups <pre><code>Map&lt;Integer, Set&lt;String&gt;&gt; nodeGroups\n</code></pre> <p><code>InternalTopologyBuilder</code> defines <code>nodeGroups</code> internal registry of subtopologies and an associated group of (source) topics.</p> <p><code>nodeGroups</code> is initially undefined (<code>null</code>) and is built on demand when undefined that happens after <code>InternalTopologyBuilder</code> is requested for the following:</p> <ul> <li>addSource</li> <li>addSink</li> <li>addProcessor</li> <li>addStateStore</li> <li>addGlobalStore</li> <li>connectProcessorAndStateStores</li> </ul> <p>Node groups are uniquely identified by node group ID (starting from <code>0</code>).</p> <p>Used when:</p> <ul> <li>Building a topology</li> <li>Building a sub-topology</li> <li>globalNodeGroups</li> <li>topicGroups</li> <li>Describing a topology</li> </ul>"},{"location":"InternalTopologyBuilder/#makenodegroups","text":"","title":"makeNodeGroups <pre><code>Map&lt;Integer, Set&lt;String&gt;&gt; makeNodeGroups()\n</code></pre> <p>For every node (in the nodeFactories registry) <code>makeNodeGroups</code> putNodeGroupName.</p> <p><code>makeNodeGroups</code> uses local mutable <code>nodeGroups</code> and <code>nodeGroupId</code> values that can be modified every putNodeGroupName.</p> <p>In the end, <code>makeNodeGroups</code> returns the <code>nodeGroups</code> local collection.</p>"},{"location":"InternalTopologyBuilder/#putnodegroupname","text":"","title":"putNodeGroupName <pre><code>int putNodeGroupName(\n  String nodeName,\n  int nodeGroupId,\n  Map&lt;Integer, Set&lt;String&gt;&gt; nodeGroups,\n  Map&lt;String, Set&lt;String&gt;&gt; rootToNodeGroup)\n</code></pre> <p><code>putNodeGroupName</code> requests the nodeGrouper for the name of the root node of the given <code>nodeName</code>.</p> <p><code>putNodeGroupName</code> looks up the name of the root node in the given <code>rootToNodeGroup</code>.</p> <p>If the node group is found (by the name of the root node), <code>putNodeGroupName</code> simply adds the given <code>nodeName</code> and returns the given <code>nodeGroupId</code> (unchanged).</p> <p>Otherwise, if the name of the root node is not among the available node groups (in the given <code>rootToNodeGroup</code>), <code>putNodeGroupName</code> adds the root name to the given <code>rootToNodeGroup</code> and <code>nodeGroups</code> (with an empty node group and a new node group ID).</p> <p>In the end, <code>putNodeGroupName</code> returns a new or the given node group ID (based on availability of the root node).</p>"},{"location":"InternalTopologyBuilder/#node-grouper","text":"","title":"Node Grouper <p><code>InternalTopologyBuilder</code> creates a node grouper (<code>QuickUnion&lt;String&gt;</code>) when created.</p> <p>The node grouper is requested to add a node name for the following:</p> <ul> <li>addSource</li> <li>addSink</li> <li>addProcessor</li> <li>addGlobalStore</li> </ul> <p>The node grouper is requested to unite names (of a node and predecessors) for the following:</p> <ul> <li>addSink</li> <li>addProcessor</li> <li>addGlobalStore</li> <li>connectProcessorAndStateStore</li> </ul> <p>In the end, the node grouper is requested for a root node in putNodeGroupName.</p>"},{"location":"InternalTopologyBuilder/#describing-topology","text":"","title":"Describing Topology <pre><code>TopologyDescription describe()\n</code></pre> <p><code>describe</code> creates a new <code>TopologyDescription</code> (that is going to be the returned value in the end).</p> <p>For every node group <code>describe</code> checks if the group contains a global (state) source.</p> <p>If so, <code>describe</code> describeGlobalStore. Otherwise, <code>describe</code> describeSubtopology.</p> <p><code>describe</code> is used when:</p> <ul> <li><code>Topology</code> is requested to describe</li> </ul>"},{"location":"InternalTopologyBuilder/#copartitionsourcegroups","text":"","title":"copartitionSourceGroups <pre><code>List&lt;Set&lt;String&gt;&gt; copartitionSourceGroups\n</code></pre> <p><code>InternalTopologyBuilder</code> defines <code>copartitionSourceGroups</code> internal registry of groups of source processors that need to be co-partitioned.</p> <p>A new entry is added when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to copartitionSources</li> </ul> <p>The registry is used when <code>InternalTopologyBuilder</code> is requested for the following:</p> <ul> <li>maybeUpdateCopartitionSourceGroups</li> <li>validateCopartition</li> <li>copartitionGroups</li> </ul>"},{"location":"InternalTopologyBuilder/#maybeupdatecopartitionsourcegroups","text":"","title":"maybeUpdateCopartitionSourceGroups <pre><code>void maybeUpdateCopartitionSourceGroups(\n  String replacedNodeName,\n  String optimizedNodeName)\n</code></pre> <p><code>maybeUpdateCopartitionSourceGroups</code>...FIXME</p> <p><code>maybeUpdateCopartitionSourceGroups</code>\u00a0is used when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to maybeOptimizeRepartitionOperations</li> </ul>"},{"location":"InternalTopologyBuilder/#copartitiongroups","text":"","title":"copartitionGroups <pre><code>Collection&lt;Set&lt;String&gt;&gt; copartitionGroups()\n</code></pre> <p><code>copartitionGroups</code>...FIXME</p> <p><code>copartitionGroups</code>\u00a0is used when:</p> <ul> <li><code>RepartitionTopics</code> is requested to setup</li> </ul>"},{"location":"InternalTopologyBuilder/#copartitionsources","text":"","title":"copartitionSources <pre><code>void copartitionSources(\n  Collection&lt;String&gt; sourceNodes)\n</code></pre> <p><code>copartitionSources</code> simply adds the given <code>sourceNodes</code> to the copartitionSourceGroups internal registry.</p> <p><code>copartitionSources</code>\u00a0is used when:</p> <ul> <li><code>AbstractStream</code> is requested to ensureCopartitionWith</li> <li><code>KTableImpl</code> is requested to doJoinOnForeignKey</li> </ul>"},{"location":"InternalTopologyBuilder/#internaltopicnameswithproperties","text":"","title":"internalTopicNamesWithProperties <pre><code>Map&lt;String, InternalTopicProperties&gt; internalTopicNamesWithProperties\n</code></pre> <p><code>InternalTopologyBuilder</code> defines <code>internalTopicNamesWithProperties</code> internal registry of all the internal topics with their corresponding properties.</p> <p>A new internal topic is added when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to addInternalTopic</li> </ul> <p>The registry is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to validateCopartition, buildSinkNode, buildSourceNode, topicGroups, maybeDecorateInternalSourceTopics</li> <li><code>SinkNodeFactory</code> is requested to build a processor node</li> </ul>"},{"location":"InternalTopologyBuilder/#addinternaltopic","text":"","title":"addInternalTopic <pre><code>void addInternalTopic(\n  String topicName,\n  InternalTopicProperties internalTopicProperties)\n</code></pre> <p><code>addInternalTopic</code>...FIXME</p> <p><code>addInternalTopic</code>\u00a0is used when:</p> <ul> <li><code>KTableImpl</code> is requested to doJoinOnForeignKey</li> <li><code>GroupedTableOperationRepartitionNode</code> is requested to writeToTopology</li> <li><code>OptimizableRepartitionNode</code> is requested to writeToTopology</li> <li><code>UnoptimizableRepartitionNode</code> is requested to writeToTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#validatecopartition","text":"","title":"validateCopartition <pre><code>void validateCopartition()\n</code></pre> <p><code>validateCopartition</code>...FIXME</p> <p><code>validateCopartition</code>\u00a0is used when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to buildAndOptimizeTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#global-topics","text":"","title":"Global Topics <pre><code>Set&lt;String&gt; globalTopics\n</code></pre> <p><code>InternalTopologyBuilder</code> tracks global topics (names) in a <code>globalTopics</code> internal registry.</p> <p>A new topic name is added in addGlobalStore.</p>"},{"location":"InternalTopologyBuilder/#building-processor-topology","text":"","title":"Building Processor Topology <pre><code>ProcessorTopology build(\n  Set&lt;String&gt; nodeGroup)\n</code></pre> <p>For every NodeFactory (in the nodeFactories internal registry), if the name of the factory is in the given node group if defined or simply all node factories go through, <code>build</code> does the following:</p> <ol> <li>Requests the <code>NodeFactory</code> to build a ProcessorNode (and registers it in a local registry of processors by name)</li> <li>For <code>ProcessorNodeFactory</code>s, buildProcessorNode</li> <li>For <code>SourceNodeFactory</code>s, buildSourceNode</li> <li>For <code>SinkNodeFactory</code>s, buildSinkNode</li> </ol> <p>In the end, <code>build</code> creates a new ProcessorTopology.</p> <p><code>build</code>\u00a0is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to build a topology, a subtopology and a global state topology</li> </ul>"},{"location":"InternalTopologyBuilder/#buildprocessornode","text":"","title":"buildProcessorNode <pre><code>void buildProcessorNode(\n  Map&lt;String, ProcessorNode&lt;?, ?, ?, ?&gt;&gt; processorMap,\n  Map&lt;String, StateStore&gt; stateStoreMap,\n  ProcessorNodeFactory&lt;?, ?, ?, ?&gt; factory,\n  ProcessorNode&lt;Object, Object, Object, Object&gt; node)\n</code></pre> <p><code>buildProcessorNode</code>...FIXME</p>"},{"location":"InternalTopologyBuilder/#building-source-node","text":"","title":"Building Source Node <pre><code>void buildSourceNode(\n  Map&lt;String, SourceNode&lt;?, ?&gt;&gt; topicSourceMap,\n  Set&lt;String&gt; repartitionTopics,\n  SourceNodeFactory&lt;?, ?&gt; sourceNodeFactory,\n  SourceNode&lt;?, ?&gt; node)\n</code></pre> <p><code>buildSourceNode</code> mutates (changes) the given <code>SourceNode</code> by topic name (<code>topicSourceMap</code>) and repartition topic names (<code>repartitionTopics</code>) collections.</p>  <p>When the pattern (of the given SourceNodeFactory) is defined, <code>buildSourceNode</code> subscriptionUpdates and requests the <code>SourceNodeFactory</code> to get the topics. Otherwise, <code>buildSourceNode</code> requests the <code>SourceNodeFactory</code> for the topics.</p> <p><code>buildSourceNode</code> adds the topic to the given <code>topicSourceMap</code> collection.</p> <p>For internal topics (in internalTopicNamesWithProperties registry), <code>buildSourceNode</code> decorates the name before adding to the given <code>topicSourceMap</code> collection and adds them to the given <code>repartitionTopics</code> collection.</p>"},{"location":"InternalTopologyBuilder/#buildsinknode","text":"","title":"buildSinkNode <pre><code>void buildSinkNode(\n  Map&lt;String, ProcessorNode&lt;?, ?, ?, ?&gt;&gt; processorMap,\n  Map&lt;String, SinkNode&lt;?, ?&gt;&gt; topicSinkMap,\n  Set&lt;String&gt; repartitionTopics,\n  SinkNodeFactory&lt;?, ?&gt; sinkNodeFactory,\n  SinkNode&lt;?, ?&gt; node)\n</code></pre> <p><code>buildSinkNode</code>...FIXME</p>"},{"location":"InternalTopologyBuilder/#building-local-processor-topology","text":"","title":"Building (Local) Processor Topology <pre><code>ProcessorTopology buildTopology()\n</code></pre> <p><code>buildTopology</code> initializes subscription and then builds a topology (of the node groups without the global node groups).</p> <p><code>buildTopology</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> <li><code>TopologyTestDriver</code> is requested to setupTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#building-processor-subtopology","text":"","title":"Building Processor SubTopology <pre><code>ProcessorTopology buildSubtopology(\n  int topicGroupId)\n</code></pre> <p><code>buildSubtopology</code> takes the <code>topicGroupId</code> node group (from the nodeGroups) and builds a topology.</p> <p><code>buildSubtopology</code>\u00a0is used when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createTasks and createActiveTaskFromStandby</li> <li><code>StandbyTaskCreator</code> is requested to createTasks and createStandbyTaskFromActive</li> </ul>"},{"location":"InternalTopologyBuilder/#building-global-state-processor-topology","text":"","title":"Building Global State Processor Topology <pre><code>ProcessorTopology buildGlobalStateTopology()\n</code></pre> <p><code>buildGlobalStateTopology</code> builds a topology of the global node groups if there are any.</p> <p><code>buildGlobalStateTopology</code> assumes that the applicationId has already been set or throws a <code>NullPointerException</code>:</p> <pre><code>topology has not completed optimization\n</code></pre> <p><code>buildGlobalStateTopology</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> <li><code>TopologyTestDriver</code> is requested to setupTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#rewriting-topology","text":"","title":"Rewriting Topology <pre><code>InternalTopologyBuilder rewriteTopology(\n  StreamsConfig config)\n</code></pre> <p><code>rewriteTopology</code> setApplicationId to the value of application.id configuration property.</p> <p>With cache.max.bytes.buffering enabled, <code>rewriteTopology</code>...FIXME</p> <p><code>rewriteTopology</code> requests the global StoreBuilders to build StateStores.</p> <p>In the end, <code>rewriteTopology</code> saves the StreamsConfig (and returns itself).</p> <p><code>rewriteTopology</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> <li><code>TopologyTestDriver</code> is requested to setupTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#globalnodegroups","text":"","title":"globalNodeGroups <pre><code>Set&lt;String&gt; globalNodeGroups()\n</code></pre> <p><code>globalNodeGroups</code> collects global source nodes from all the node groups.</p> <p><code>globalNodeGroups</code>\u00a0is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to build a local (excluding global state nodes) and global state topologies</li> </ul>"},{"location":"InternalTopologyBuilder/#isglobalsource","text":"","title":"isGlobalSource <pre><code>boolean isGlobalSource(\n  String nodeName)\n</code></pre> <p><code>isGlobalSource</code> finds a NodeFactory (by given <code>nodeName</code>) in nodeFactories registry.</p> <p><code>isGlobalSource</code> is positive (<code>true</code>) when the <code>NodeFactory</code> is a SourceNodeFactory with one topic only that is global. Otherwise, <code>isGlobalSource</code> is negative (<code>false</code>).</p> <p><code>isGlobalSource</code>\u00a0is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to globalNodeGroups, describeGlobalStore and nodeGroupContainsGlobalSourceNode</li> </ul>"},{"location":"InternalTopologyBuilder/#registering-global-store","text":"","title":"Registering Global Store <pre><code>&lt;KIn, VIn&gt; void addGlobalStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  String sourceName,\n  TimestampExtractor timestampExtractor,\n  Deserializer&lt;KIn&gt; keyDeserializer,\n  Deserializer&lt;VIn&gt; valueDeserializer,\n  String topic,\n  String processorName,\n  ProcessorSupplier&lt;KIn, VIn, Void, Void&gt; stateUpdateSupplier)\n</code></pre> <p><code>addGlobalStore</code>...FIXME</p> <p><code>addGlobalStore</code> is used when:</p> <ul> <li><code>Topology</code> is requested to addGlobalStore</li> <li><code>GlobalStoreNode</code> is requested to <code>writeToTopology</code></li> <li><code>TableSourceNode</code> is requested to <code>writeToTopology</code></li> </ul>"},{"location":"InternalTopologyBuilder/#registering-processor","text":"","title":"Registering Processor <pre><code>void addProcessor(\n  String name,\n  ProcessorSupplier&lt;KIn, VIn, KOut, VOut&gt; supplier,\n  String... predecessorNames)\n</code></pre> <p><code>addProcessor</code> creates a ProcessorNodeFactory (that is then added to nodeFactories registry).</p> <p><code>addProcessor</code> adds the name to nodeGrouper to unite the name with the given <code>predecessorNames</code>.</p> <p><code>addProcessor</code> is used when:</p> <ul> <li><code>Topology</code> is requested to addProcessor</li> <li>Some <code>GraphNode</code>s are requested to writeToTopology</li> </ul>"},{"location":"InternalTopologyBuilder/#registering-statestore","text":"","title":"Registering StateStore <pre><code>void addStateStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  String... processorNames) // (1)\nvoid addStateStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  boolean allowOverride,\n  String... processorNames)\n</code></pre> <ol> <li>Uses <code>allowOverride</code> flag disabled (<code>false</code>)</li> </ol> <p><code>addStateStore</code>...FIXME</p> <p><code>addStateStore</code>\u00a0is used when:</p> <ul> <li><code>Topology</code> is requested to addProcessor and addStateStore</li> <li><code>KTableKTableJoinNode</code> is requested to <code>writeToTopology</code></li> <li><code>StatefulProcessorNode</code> is requested to writeToTopology</li> <li><code>StateStoreNode</code> is requested to <code>writeToTopology</code></li> <li><code>StreamStreamJoinNode</code> is requested to <code>writeToTopology</code></li> <li><code>StreamToTableNode</code> is requested to <code>writeToTopology</code></li> <li><code>TableProcessorNode</code> is requested to <code>writeToTopology</code></li> <li><code>TableSourceNode</code> is requested to <code>writeToTopology</code></li> </ul>"},{"location":"InternalTopologyBuilder/#topicgroups","text":"","title":"topicGroups <pre><code>Map&lt;Subtopology, TopicsInfo&gt; topicGroups()\n</code></pre> <p><code>topicGroups</code>...FIXME</p> <p><code>topicGroups</code>\u00a0is used when:</p> <ul> <li><code>RepartitionTopics</code> is requested to setup</li> <li><code>StreamsPartitionAssignor</code> is requested for consumer group assignment</li> </ul>"},{"location":"InternalTopologyBuilder/#addsource","text":"","title":"addSource <pre><code>void addSource(\n  Topology.AutoOffsetReset offsetReset,\n  String name,\n  TimestampExtractor timestampExtractor,\n  Deserializer&lt;?&gt; keyDeserializer,\n  Deserializer&lt;?&gt; valDeserializer,\n  Pattern topicPattern)\nvoid addSource(\n  Topology.AutoOffsetReset offsetReset,\n  String name,\n  TimestampExtractor timestampExtractor,\n  Deserializer&lt;?&gt; keyDeserializer,\n  Deserializer&lt;?&gt; valDeserializer,\n  String... topics)\n</code></pre> <p><code>addSource</code> creates a new SourceNodeFactory and adds it to the nodeFactories registry (under the given <code>name</code>).</p> <p><code>addSource</code> adds every topic (in the given <code>topics</code>) to the sourceTopicNames internal registry.</p> <p><code>addSource</code> registers the given <code>name</code> with the <code>topics</code> or the <code>topicPattern</code> (in the nodeToSourceTopics or the nodeToSourcePatterns registries, respectively).</p> <p><code>addSource</code> adds the given <code>name</code> to nodeGrouper and clears out the nodeGroups (so it has to be built again next time it is requested).</p> <p><code>addSource</code>\u00a0is used when:</p> <ul> <li><code>GroupedTableOperationRepartitionNode</code> is requested to <code>writeToTopology</code></li> <li><code>OptimizableRepartitionNode</code> is requested to <code>writeToTopology</code></li> <li><code>StreamSourceNode</code> is requested to writeToTopology</li> <li><code>TableSourceNode</code> is requested to <code>writeToTopology</code></li> <li><code>Topology</code> is requested to addSource</li> <li><code>UnoptimizableRepartitionNode</code> is requested to <code>writeToTopology</code></li> </ul>"},{"location":"KafkaClientSupplier/","text":"<p><code>KafkaClientSupplier</code> is...FIXME</p>","title":"KafkaClientSupplier"},{"location":"KafkaStreams/","text":"<p><code>KafkaStreams</code> is the execution environment of a single instance of a Kafka Streams application (KafkaStreams instance).</p> <p><code>KafkaStreams</code> is a Kafka client for continuous stream processing (on input coming from one or more input topics and sending output to zero, one, or more output topics).</p>","title":"KafkaStreams"},{"location":"KafkaStreams/#creating-instance","text":"<p><code>KafkaStreams</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder (or Topology) <li> StreamsConfig <li> KafkaClientSupplier (default: <code>DefaultKafkaClientSupplier</code>) <li> <code>Time</code>  <p>When created, <code>KafkaStreams</code> requests the given InternalTopologyBuilder to rewriteTopology followed by building a task and global task topologies.</p> <p><code>KafkaStreams</code> then...FIXME</p>","title":"Creating Instance"},{"location":"KafkaStreams/#defaultstreamsuncaughtexceptionhandler","text":"","title":"defaultStreamsUncaughtExceptionHandler <pre><code>void defaultStreamsUncaughtExceptionHandler(\n  Throwable throwable)\n</code></pre> <p><code>defaultStreamsUncaughtExceptionHandler</code>...FIXME</p>"},{"location":"KafkaStreams/#task-topology","text":"","title":"Task Topology <p><code>KafkaStreams</code> requests the InternalTopologyBuilder to build a task topology when created.</p> <p>The ProcessorTopology can have persistent local stores.</p>"},{"location":"KafkaStreams/#global-task-topology","text":"","title":"Global Task Topology <p>When created <code>KafkaStreams</code> requests the InternalTopologyBuilder to build a global task topology.</p>"},{"location":"KafkaStreams/#streamthreads","text":"","title":"StreamThreads <p><code>KafkaStreams</code> manages StreamThreads in a <code>threads</code> internal registry.</p> <p>The <code>threads</code> collection starts empty when <code>KafkaStreams</code> is created.</p> <p><code>KafkaStreams</code> adds a new <code>StreamThread</code> when requested to createAndAddStreamThread.</p> <p>A <code>StreamThread</code> is removed when <code>KafkaStreams</code> is requested for the following:</p> <ul> <li>defaultStreamsUncaughtExceptionHandler</li> <li>addStreamThread</li> <li>removeStreamThread</li> <li>getNumLiveStreamThreads</li> <li>getNextThreadIndex</li> </ul> <p><code>KafkaStreams</code> uses processStreamThread to work with the <code>StreamThread</code>s.</p>"},{"location":"KafkaStreams/#processstreamthread","text":"","title":"processStreamThread <pre><code>void processStreamThread(\n  java.util.function.Consumer&lt;StreamThread&gt; consumer)\n</code></pre> <p><code>processStreamThread</code>...FIXME</p>"},{"location":"KafkaStreams/#getnumlivestreamthreads","text":"","title":"getNumLiveStreamThreads <pre><code>int getNumLiveStreamThreads()\n</code></pre> <p><code>getNumLiveStreamThreads</code>...FIXME</p>"},{"location":"KafkaStreams/#globalstreamthread","text":"","title":"GlobalStreamThread <p><code>KafkaStreams</code> can use a GlobalStreamThread if...FIXME</p>"},{"location":"KafkaStreams/#starting-streams-client","text":"","title":"Starting Streams Client <pre><code>void start()\n</code></pre> <p><code>start</code> attempts to enter <code>REBALANCING</code> state and, if successful, prints out the following INFO message to the logs:</p> <pre><code>State transition from [oldState] to REBALANCING\n</code></pre> <p><code>start</code> prints out the following DEBUG message to the logs:</p> <pre><code>Starting Streams client\n</code></pre> <p><code>start</code> requests the GlobalStreamThread to start (if defined).</p> <p><code>start</code> requests all the StreamThreads to start.</p> <p><code>start</code>...FIXME</p>"},{"location":"KafkaStreams/#setuncaughtexceptionhandler","text":"","title":"setUncaughtExceptionHandler <pre><code>void setUncaughtExceptionHandler(\n  StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler)\n</code></pre> <p><code>setUncaughtExceptionHandler</code>...FIXME</p> <p><code>setUncaughtExceptionHandler</code>\u00a0is part of the public API.</p>"},{"location":"KafkaStreams/#handlestreamsuncaughtexception","text":"","title":"handleStreamsUncaughtException <pre><code>void handleStreamsUncaughtException(\n  Throwable throwable,\n  StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler)\n</code></pre> <p><code>handleStreamsUncaughtException</code>...FIXME</p> <p><code>handleStreamsUncaughtException</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is requested to setUncaughtExceptionHandler and defaultStreamsUncaughtExceptionHandler</li> </ul>"},{"location":"KafkaStreams/#replacestreamthread","text":"","title":"replaceStreamThread <pre><code>void replaceStreamThread(\n  Throwable throwable)\n</code></pre> <p><code>replaceStreamThread</code>...FIXME</p>"},{"location":"KafkaStreams/#addstreamthread","text":"","title":"addStreamThread <pre><code>Optional&lt;String&gt; addStreamThread()\n</code></pre> <p><code>addStreamThread</code>...FIXME</p> <p><code>addStreamThread</code> is part of the public API.</p>"},{"location":"KafkaStreams/#createandaddstreamthread","text":"","title":"createAndAddStreamThread <pre><code>StreamThread createAndAddStreamThread(\n  long cacheSizePerThread,\n  int threadIdx)\n</code></pre> <p><code>createAndAddStreamThread</code> creates a StreamThread and requests it to setStateListener with the StreamStateListener.</p> <p><code>createAndAddStreamThread</code> registers the <code>StreamThread</code> (in the threads and threadState internal registries).</p> <p><code>createAndAddStreamThread</code> requests the QueryableStoreProvider to addStoreProviderForThread (with the name of the <code>StreamThread</code> and a new <code>StreamThreadStateStoreProvider</code>).</p> <p><code>createAndAddStreamThread</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is created and requested to addStreamThread</li> </ul>"},{"location":"KafkaStreams/#streamsmetadatastate","text":"","title":"StreamsMetadataState <p><code>KafkaStreams</code> creates a new StreamsMetadataState when created (with the endpoint based on application.server configuration property).</p> <p>The <code>StreamsMetadataState</code> is used to create a StreamThread and for the following state-related metadata operators:</p> <ul> <li>metadataForAllStreamsClients</li> <li>streamsMetadataForStore</li> <li>queryMetadataForKey</li> <li>allLocalStorePartitionLags</li> </ul>"},{"location":"KafkaStreams/#querymetadataforkey","text":"","title":"queryMetadataForKey <pre><code>KeyQueryMetadata queryMetadataForKey(\n  String storeName,\n  K key,\n  Serializer&lt;K&gt; keySerializer)\nKeyQueryMetadata queryMetadataForKey(\n  String storeName,\n  K key,\n  StreamPartitioner&lt;? super K, ?&gt; partitioner)\n</code></pre> <p><code>queryMetadataForKey</code> requests the StreamsMetadataState to getKeyQueryMetadataForKey.</p>"},{"location":"KafkaStreams/#performance-metrics","text":"","title":"Performance Metrics <p><code>KafkaStreams</code> gets the configured metrics when created.</p> <p><code>KafkaStreams</code> uses the metrics to create a StreamsMetricsImpl right after.</p>"},{"location":"KafkaStreams/#metrics-recording-service","text":"","title":"Metrics Recording Service <p><code>KafkaStreams</code> may create a <code>rocksDBMetricsRecordingService</code> executor service (ScheduledExecutorService) when created (and the value of metrics.recording.level configuration property is <code>DEBUG</code>).</p> <p><code>KafkaStreams</code> uses the <code>ScheduledExecutorService</code> to submit a RocksDBMetricsRecordingTrigger (of the StreamsMetricsImpl) to be executed every 1 minute (non-configurable).</p> <p>The <code>ScheduledExecutorService</code> is shut down when shutdownHelper.</p>"},{"location":"KafkaStreams/#maybecreaterocksdbmetricsrecordingservice","text":"","title":"maybeCreateRocksDBMetricsRecordingService <pre><code>ScheduledExecutorService maybeCreateRocksDBMetricsRecordingService(\n  String clientId,\n  StreamsConfig config)\n</code></pre> <p>Only with metrics.recording.level configuration property as <code>DEBUG</code>, <code>maybeCreateRocksDBMetricsRecordingService</code> creates a single-threaded executor. The name of this one daemon thread is as follows:</p> <pre><code>[clientId]-RocksDBMetricsRecordingTrigger\n</code></pre>"},{"location":"KafkaStreams/#streamsmetricsimpl","text":"","title":"StreamsMetricsImpl <p>When created, <code>KafkaStreams</code> creates a StreamsMetricsImpl (with the Metrics, the clientId and built.in.metrics.version configuration property).</p> <p><code>KafkaStreams</code> registers ClientMetrics.</p> <p><code>KafkaStreams</code> passes the <code>StreamsMetricsImpl</code> in while creating a GlobalStreamThread and StreamThreads.</p> <p>When started, <code>KafkaStreams</code> requests the <code>StreamsMetricsImpl</code> for rocksDBMetricsRecordingTrigger (to schedule it at fixed rate using the Metrics Recording Service).</p>"},{"location":"KafkaStreams/#getmetrics","text":"","title":"getMetrics <pre><code>Metrics getMetrics(\n  StreamsConfig config,\n  Time time,\n  String clientId)\n</code></pre> <p><code>getMetrics</code> creates a <code>MetricConfig</code> (Apache Kafka) based on the following configuration properties:</p> <ul> <li>metrics.num.samples</li> <li>metrics.recording.level</li> <li>metrics.sample.window.ms</li> </ul> <p><code>getMetrics</code> requests the given StreamsConfig for configured <code>MetricsReporter</code>s (Apache Kafka) per metric.reporters configuration property.</p> <p><code>getMetrics</code> always adds <code>JmxReporter</code> to the list of configured <code>MetricsReporter</code>s. <code>JmxReporter</code> is configured to use <code>kafka.streams</code> JMX prefix.</p> <p>In the end, <code>getMetrics</code> creates a <code>Metrics</code> (Apache Kafka) (with the <code>MetricConfig</code>, the <code>MetricsReporter</code>s, et al.)</p>"},{"location":"KafkaStreams/#cachemaxbytesbuffering","text":"","title":"cache.max.bytes.buffering <p><code>KafkaStreams</code> reads cache.max.bytes.buffering when created for getCacheSizePerThread.</p>"},{"location":"KafkaStreams/#getcachesizeperthread","text":"","title":"getCacheSizePerThread <pre><code>long getCacheSizePerThread(\n  int numStreamThreads)\n</code></pre> <p><code>getCacheSizePerThread</code> returns the totalCacheSize when the given <code>numStreamThreads</code> is <code>0</code>. Otherwise, <code>getCacheSizePerThread</code> is the totalCacheSize divided by the given <code>numStreamThreads</code> with an extra <code>1</code> for the globalTaskTopology thread (if used).</p> <p><code>getCacheSizePerThread</code> is used when:</p> <ul> <li><code>KafkaStreams</code> is created, is requested to addStreamThread and removeStreamThread.</li> </ul>"},{"location":"KafkaStreams/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.KafkaStreams</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.KafkaStreams=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"PartitionGroup/","text":"","title":"PartitionGroup"},{"location":"PartitionGroup/#creating-instance","text":"<p><code>PartitionGroup</code> takes the following to be created:</p> <ul> <li> <code>LogContext</code> <li> Partition Queues (<code>Map&lt;TopicPartition, RecordQueue&gt;</code>) <li> Lag Provider Function (<code>Function&lt;TopicPartition, OptionalLong&gt;</code>) <li>recordLateness Sensor</li> <li>enforcedProcessing Sensor</li> <li> max.task.idle.ms configuration property  <p><code>PartitionGroup</code> is created when:</p> <ul> <li><code>StreamTask</code> is created</li> </ul>","title":"Creating Instance"},{"location":"PartitionGroup/#sensors","text":"","title":"Sensors"},{"location":"PartitionGroup/#recordlateness","text":"","title":"recordLateness <p><code>PartitionGroup</code> is given the recordLateness metric sensor when created.</p> <p>The <code>recordLateness</code> sensor is requested to record the following values:</p> <ul> <li> <p><code>0</code> when the event time advances (the timestamp of the current record is smaller than the current stream time)</p> </li> <li> <p>the difference (lateness) between the current stream time and the timestamp of the current otherwise</p> </li> </ul>"},{"location":"PartitionGroup/#enforcedprocessing","text":"","title":"enforcedProcessing <p><code>PartitionGroup</code> is given an <code>enforcedProcessingSensor</code> metric sensor when created.</p>"},{"location":"PartitionGroup/#stream-time","text":"","title":"Stream Time <p><code>PartitionGroup</code> defines <code>streamTime</code> internal registry for the highest time (event-time watermark) across already-processed events (across all the partitions assigned to this <code>StreamTask</code>).</p> <p>The <code>streamTime</code> is initially <code>RecordQueue.UNKNOWN</code> when <code>PartitionGroup</code> is created and later when cleared.</p> <p>The <code>streamTime</code> changes to the given timestamp in setPartitionTime and next record.</p> <p>The <code>streamTime</code> is used when:</p> <ul> <li><code>StreamTask</code> is requested to maybePunctuateStreamTime and stream time</li> </ul>"},{"location":"PartitionGroup/#setpartitiontime","text":"","title":"setPartitionTime <pre><code>void setPartitionTime(\n  TopicPartition partition, \n  long partitionTime)\n</code></pre> <p><code>setPartitionTime</code>...FIXME</p> <p><code>setPartitionTime</code> is used when:</p> <ul> <li><code>StreamTask</code> is requested to initializeTaskTime</li> </ul>"},{"location":"PartitionGroup/#next-stampedrecord","text":"","title":"Next (Stamped)Record <pre><code>StampedRecord nextRecord(\n  RecordInfo info, \n  long wallClockTime)\n</code></pre> <p><code>nextRecord</code>...FIXME</p> <p><code>nextRecord</code> is used when:</p> <ul> <li><code>StreamTask</code> is requested to process one record</li> </ul>"},{"location":"PartitionGroup/#adding-consumerrecords","text":"","title":"Adding ConsumerRecords <pre><code>int addRawRecords(\n  TopicPartition partition,\n  Iterable&lt;ConsumerRecord&lt;byte[], byte[]&gt;&gt; rawRecords)\n</code></pre> <p><code>addRawRecords</code>...FIXME</p> <p><code>addRawRecords</code> is used when:</p> <ul> <li><code>StreamTask</code> is requested to add records</li> </ul>"},{"location":"PartitionGrouper/","text":"","title":"PartitionGrouper"},{"location":"PartitionGrouper/#creating-instance","text":"<p><code>PartitionGrouper</code> takes no arguments to be created.</p> <p><code>PartitionGrouper</code> is created\u00a0when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to configure</li> </ul>","title":"Creating Instance"},{"location":"PartitionGrouper/#partitiongroups","text":"","title":"partitionGroups <pre><code>Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; partitionGroups(\n  Map&lt;Subtopology, Set&lt;String&gt;&gt; topicGroups, \n  Cluster metadata)\n</code></pre> <p><code>partitionGroups</code>...FIXME</p> <p><code>partitionGroups</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to assign</li> </ul>"},{"location":"PartitionGrouper/#maximum-number-of-partitions","text":"","title":"Maximum Number of Partitions <pre><code>int maxNumPartitions(\n  Cluster metadata, \n  Set&lt;String&gt; topics)\n</code></pre> <p><code>maxNumPartitions</code> finds the maximum number of partitions across all the given <code>topics</code> (using <code>Cluster</code> metadata).</p>"},{"location":"ProcessorStateManager/","text":"<p><code>ProcessorStateManager</code> is a StateManager.</p>","title":"ProcessorStateManager"},{"location":"ProcessorStateManager/#creating-instance","text":"<p><code>ProcessorStateManager</code> takes the following to be created:</p> <ul> <li> TaskId <li> <code>TaskType</code> <li>eosEnabled flag</li> <li> <code>LogContext</code> <li> StateDirectory <li> <code>ChangelogRegister</code> <li> <code>storeToChangelogTopic</code> collection <li> Source <code>TopicPartition</code>s  <p><code>ProcessorStateManager</code> is created\u00a0when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createTasks</li> <li><code>StandbyTaskCreator</code> is requested to createTasks</li> <li><code>TopologyTestDriver</code> is requested to setupTask</li> </ul>","title":"Creating Instance"},{"location":"ProcessorStateManager/#eosenabled-flag","text":"","title":"eosEnabled Flag <p><code>ProcessorStateManager</code> is given <code>eosEnabled</code> flag when created.</p>"},{"location":"ProcessorStateManager/#offset-checkpoint-file","text":"","title":"Offset Checkpoint File <p>When created, <code>ProcessorStateManager</code> requests the given StateDirectory for a checkpoint file for the given TaskId and creates a new <code>OffsetCheckpoint</code>.</p> <p><code>ProcessorStateManager</code> uses the <code>OffsetCheckpoint</code> for the following:</p> <ul> <li>initializeStoreOffsetsFromCheckpoint (to read offsets and then delete it with eosEnabled)</li> <li>Checkpoint</li> <li>deleteCheckPointFileIfEOSEnabled</li> </ul>"},{"location":"ProcessorStateManager/#flushing-state-stores","text":"","title":"Flushing State Stores <pre><code>void flush()\n</code></pre> <p><code>flush</code> does nothing (noop) when there are no state stores registered.</p>  <p><code>flush</code> prints out the following DEBUG message to the logs:</p> <pre><code>Flushing all stores registered in the state manager: [stores]\n</code></pre> <p>For every state store, <code>flush</code> prints out the following TRACE message to the logs and requests the <code>StateStore</code> to flush cached data:</p> <pre><code>Flushing store [name]\n</code></pre>  <p><code>flush</code> is part of the StateManager abstraction.</p>"},{"location":"ProcessorStateManager/#flushing-store-caches","text":"","title":"Flushing Store Caches <pre><code>void flushCache()\n</code></pre> <p><code>flushCache</code>...FIXME</p> <p><code>flushCache</code>\u00a0is used when:</p> <ul> <li><code>StreamTask</code> is requested to prepareCommit</li> </ul>"},{"location":"ProcessorStateManager/#checkpointing","text":"","title":"Checkpointing <pre><code>void checkpoint()\n</code></pre> <p><code>checkpoint</code> finds all the persistent state stores (in the stores registry) that are logged (with a <code>changelogPartition</code>) and are not corrupted. For every state store, <code>checkpoint</code> records the <code>changelogPartition</code> and the offset (in a local <code>checkpointingOffsets</code> collection).</p> <p><code>checkpoint</code> prints out the following DEBUG message to the logs:</p> <pre><code>Writing checkpoint: [checkpointingOffsets]\n</code></pre> <p><code>checkpoint</code> requests the OffsetCheckpoint file to write out the offsets.</p>  <p>In case of any IO exceptions, <code>checkpoint</code> prints out the following WARN message to the logs:</p> <pre><code>Failed to write offset checkpoint file to [checkpointFile].\nThis may occur if OS cleaned the state.dir in case when it located in ${java.io.tmpdir} directory.\nThis may also occur due to running multiple instances on the same machine using the same state dir.\nChanging the location of state.dir may resolve the problem.\n</code></pre>  <p><code>checkpoint</code>\u00a0is part of the StateManager abstraction.</p>"},{"location":"ProcessorStateManager/#registerstore","text":"","title":"registerStore <pre><code>void registerStore(\n  StateStore store,\n  StateRestoreCallback stateRestoreCallback)\n</code></pre> <p><code>registerStore</code>...FIXME</p> <p><code>registerStore</code>\u00a0is part of the StateManager abstraction.</p>"},{"location":"ProcessorStateManager/#registerstatestores","text":"","title":"registerStateStores <pre><code>void registerStateStores(\n  List&lt;StateStore&gt; allStores, \n  InternalProcessorContext processorContext)\n</code></pre> <p><code>registerStateStores</code>...FIXME</p> <p><code>registerStateStores</code>\u00a0is used when:</p> <ul> <li><code>StateManagerUtil</code> is requested to registerStateStores</li> </ul>"},{"location":"ProcessorStateManager/#mayberegisterstorewithchangelogreader","text":"","title":"maybeRegisterStoreWithChangelogReader <pre><code>void maybeRegisterStoreWithChangelogReader(\n  String storeName)\n</code></pre> <p><code>maybeRegisterStoreWithChangelogReader</code>...FIXME</p> <p><code>maybeRegisterStoreWithChangelogReader</code>\u00a0is used when:</p> <ul> <li><code>ProcessorStateManager</code> is requested to registerStateStores and registerStore</li> </ul>"},{"location":"ProcessorStateManager/#getstorepartition","text":"","title":"getStorePartition <pre><code>TopicPartition getStorePartition(\n  String storeName)\n</code></pre> <p><code>getStorePartition</code> creates a <code>TopicPartition</code> with the following:</p> <ul> <li>changelogFor with the given <code>storeName</code> for the name of the (changelog) topic</li> <li>The partition of the TaskId for the partition (of the changelog topic)</li> </ul> <p><code>getStorePartition</code>\u00a0is used when:</p> <ul> <li><code>ProcessorStateManager</code> is requested to maybeRegisterStoreWithChangelogReader and registerStore</li> </ul>"},{"location":"ProcessorStateManager/#initializestoreoffsetsfromcheckpoint","text":"","title":"initializeStoreOffsetsFromCheckpoint <pre><code>void initializeStoreOffsetsFromCheckpoint(\n  boolean storeDirIsEmpty)\n</code></pre> <p><code>initializeStoreOffsetsFromCheckpoint</code>...FIXME</p> <p><code>initializeStoreOffsetsFromCheckpoint</code>\u00a0is used when:</p> <ul> <li><code>StateManagerUtil</code> is requested to registerStateStores</li> </ul>"},{"location":"ProcessorStateManager/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.processor.internals.ProcessorStateManager</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.processor.internals.ProcessorStateManager=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"PunctuationQueue/","text":"","title":"PunctuationQueue"},{"location":"PunctuationQueue/#creating-instance","text":"<p><code>PunctuationQueue</code> takes no arguments to be created.</p> <p><code>PunctuationQueue</code> is created along with a StreamTask.</p>","title":"Creating Instance"},{"location":"PunctuationQueue/#punctuationschedules","text":"","title":"PunctuationSchedules <p><code>PunctuationQueue</code> creates a <code>PriorityQueue</code> (Java) of PunctuationSchedules.</p> <p>A new <code>PunctuationSchedule</code> is added in schedule.</p>"},{"location":"PunctuationQueue/#scheduling-punctuationschedule","text":"","title":"Scheduling PunctuationSchedule <pre><code>Cancellable schedule(\n  PunctuationSchedule sched)\n</code></pre> <p><code>schedule</code> adds the given PunctuationSchedule to the PunctuationSchedules.</p> <p><code>schedule</code> is used when:</p> <ul> <li><code>StreamTask</code> is requested to schedule a Punctuator</li> </ul>"},{"location":"PunctuationQueue/#maypunctuate","text":"","title":"mayPunctuate <pre><code>boolean mayPunctuate(\n  long timestamp,\n  PunctuationType type,\n  ProcessorNodePunctuator processorNodePunctuator)\n</code></pre> <p><code>mayPunctuate</code> returns <code>true</code> when one or more punctuators were executed.</p>  <p><code>mayPunctuate</code> takes a peek at the first PunctuationSchedule in the PriorityQueue (with order by <code>timestamp</code>).</p> <p><code>mayPunctuate</code> goes over the <code>PunctuationSchedule</code>s in the PriorityQueue until there are no more <code>PunctuationSchedule</code>s or their timestamp is after the given <code>timestamp</code> (in a <code>while</code> loop).</p> <p>If the timestamp of the <code>PunctuationSchedule</code> is below or equal to the given <code>timestamp</code>, <code>mayPunctuate</code> takes it out (and removes) from the PriorityQueue.</p> <p>If the <code>PunctuationSchedule</code> is not cancelled, <code>mayPunctuate</code> requests the given ProcessorNodePunctuator to punctuate.</p> <p>Only when the <code>PunctuationSchedule</code> was not cancelled when the punctuator was executed, <code>mayPunctuate</code> requests the <code>PunctuationSchedule</code> for next PunctuationSchedule (for the given current <code>timestamp</code>) and adds it to the PriorityQueue.</p> <p><code>mayPunctuate</code> takes a peek again at the (next) first <code>PunctuationSchedule</code> in the PriorityQueue and starts (the loop) again.</p> <p><code>mayPunctuate</code> is used when:</p> <ul> <li><code>StreamTask</code> is requested to maybePunctuateStreamTime and maybePunctuateSystemTime</li> </ul>"},{"location":"PunctuationSchedule/","text":"<p><code>PunctuationSchedule</code> is a <code>Stamped</code> of ProcessorNodes.</p> <p>As a <code>Stamped</code>, <code>PunctuationSchedule</code> is also a <code>Comparable</code> (Java) and used in PunctuationQueues (which are <code>PriorityQueue</code>s under the covers).</p>","title":"PunctuationSchedule"},{"location":"PunctuationSchedule/#creating-instance","text":"<p><code>PunctuationSchedule</code> takes the following to be created:</p> <ul> <li> ProcessorNode <li> Timestamp <li> Interval <li> Punctuator <li> <code>RepointableCancellable</code>  <p><code>PunctuationSchedule</code> is created when:</p> <ul> <li><code>PunctuationSchedule</code> is requested for the next PunctuationSchedule</li> <li><code>StreamTask</code> is requested to schedule a punctuator</li> </ul>","title":"Creating Instance"},{"location":"PunctuationSchedule/#iscancelled","text":"","title":"isCancelled <p><code>PunctuationSchedule</code> defines <code>isCancelled</code> internal flag when created.</p> <p><code>isCancelled</code> is off (<code>false</code>) by default and can be turned on (<code>true</code>) in markCancelled.</p>"},{"location":"PunctuationSchedule/#markcancelled","text":"","title":"markCancelled <pre><code>void markCancelled()\n</code></pre> <p><code>markCancelled</code> turns the isCancelled flag on.</p>"},{"location":"RecordCollector/","text":"<p><code>RecordCollector</code> is...FIXME</p>","title":"RecordCollector"},{"location":"RecordCollectorImpl/","text":"<p><code>RecordCollectorImpl</code> is a RecordCollector.</p>","title":"RecordCollectorImpl"},{"location":"RecordCollectorImpl/#creating-instance","text":"<p><code>RecordCollectorImpl</code> takes the following to be created:</p> <ul> <li> <code>LogContext</code> <li> TaskId <li> StreamsProducer <li> <code>ProductionExceptionHandler</code> <li> StreamsMetricsImpl  <p><code>RecordCollectorImpl</code> is created\u00a0when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createActiveTask</li> <li><code>TopologyTestDriver</code> is requested to setupTask</li> </ul>","title":"Creating Instance"},{"location":"RecordCollectorImpl/#sending-record","text":"","title":"Sending Record <pre><code>void send(\n  String topic,\n  K key,\n  V value,\n  Headers headers,\n  Integer partition,\n  Long timestamp,\n  Serializer&lt;K&gt; keySerializer,\n  Serializer&lt;V&gt; valueSerializer)\nvoid send(\n  String topic,\n  K key,\n  V value,\n  Headers headers,\n  Long timestamp,\n  Serializer&lt;K&gt; keySerializer,\n  Serializer&lt;V&gt; valueSerializer,\n  StreamPartitioner&lt;? super K, ? super V&gt; partitioner)\n</code></pre> <p><code>send</code> uses the given StreamPartitioner to determine the partition (of the record to be sent out) (if defined) and creates a <code>ProducerRecord</code>.</p> <p><code>send</code> requests the StreamsProducer to send the record and records the offset in the offsets registry.</p> <p><code>send</code>\u00a0is part of the RecordCollector abstraction.</p>"},{"location":"RecordDeserializer/","text":"<p><code>RecordDeserializer</code> is...FIXME</p>","title":"RecordDeserializer"},{"location":"RecordQueue/","text":"<p><code>RecordQueue</code> is a queue of <code>ConsumerRecord</code>s (with the head as a deserialized record and the tail of raw records).</p> <p><code>RecordQueue</code> is created for an input partition (for the PartitionGroup of a StreamTask).</p>","title":"RecordQueue"},{"location":"RecordQueue/#creating-instance","text":"<p><code>RecordQueue</code> takes the following to be created:</p> <ul> <li> <code>TopicPartition</code> <li> SourceNode <li>TimestampExtractor</li> <li> <code>DeserializationExceptionHandler</code> <li> InternalProcessorContext <li> <code>LogContext</code>  <p><code>RecordQueue</code> is created when:</p> <ul> <li><code>RecordQueueCreator</code> is requested for a new RecordQueue</li> </ul>","title":"Creating Instance"},{"location":"RecordQueue/#timestampextractor","text":"","title":"TimestampExtractor <p><code>RecordQueue</code> is given a TimestampExtractor when created.</p>"},{"location":"RecordQueue/#fifoqueue-of-consumerrecords","text":"","title":"fifoQueue (of ConsumerRecords) <p><code>RecordQueue</code> defines <code>fifoQueue</code> internal registry as a <code>ArrayDeque</code> (Java) of serialized (raw) <code>ConsumerRecord&lt;byte[], byte[]&gt;</code>s.</p> <p><code>RecordQueue</code> creates an empty <code>ArrayDeque</code> when created.</p> <p><code>ConsumerRecord</code>s are added in addRawRecords.</p>"},{"location":"RecordQueue/#stampedrecord","text":"","title":"StampedRecord <p><code>RecordQueue</code> defines <code>headRecord</code> internal registry of <code>StampedRecord</code> (with a <code>ConsumerRecord&lt;Object, Object&gt;</code> and the extracted timestamp).</p> <p>A <code>StampedRecord</code> is retrieved (and removed) from the fifoQueue when <code>RecordQueue</code> is requested to updateHead.</p>"},{"location":"RecordQueue/#addrawrecords","text":"","title":"addRawRecords <pre><code>int addRawRecords(\n  Iterable&lt;ConsumerRecord&lt;byte[], byte[]&gt;&gt; rawRecords)\n</code></pre> <p><code>addRawRecords</code>...FIXME</p> <p><code>addRawRecords</code> is used when:</p> <ul> <li><code>PartitionGroup</code> is requested to add ConsumerRecords</li> </ul>"},{"location":"RecordQueue/#poll","text":"","title":"poll <pre><code>StampedRecord poll()\n</code></pre> <p><code>poll</code>...FIXME</p> <p><code>poll</code> is used when:</p> <ul> <li><code>PartitionGroup</code> is requested for next record</li> </ul>"},{"location":"RecordQueue/#updating-head-record","text":"","title":"Updating Head Record <pre><code>void updateHead()\n</code></pre> <p><code>updateHead</code> does its work until the headRecord is found in the fifoQueue.</p>  <p>In other words, <code>updateHead</code> makes sure that the headRecord is available (for the follow-up operations) if there are timestamp-valid records in the fifoQueue.</p>  <p><code>updateHead</code> takes (and removes) the first <code>ConsumerRecord</code> from the fifoQueue and requests the RecordDeserializer to deserialize the record (with the ProcessorContext).</p> <p><code>updateHead</code> skips this record if the RecordDeserializer returns <code>null</code> (to announce to skip the record) and starts over.</p> <p><code>updateHead</code> requests the TimestampExtractor to extract the timestamp (with the partitionTime).</p> <p><code>updateHead</code> prints out the following TRACE message to the logs:</p> <pre><code>Source node [name] extracted timestamp [timestamp] for record [record]\n</code></pre> <p><code>updateHead</code> creates a new <code>StampedRecord</code> (with the deserialized <code>ConsumerRecord</code> and the timestamp).</p>  <p>If the extracted timestamp from the deserialized <code>ConsumerRecord</code> is negative, <code>updateHead</code> prints out the following WARN message to the logs, requests the droppedRecords sensor to record the event and starts over.</p> <pre><code>Skipping record due to negative extracted timestamp. topic=[[topic]] partition=[[partition]] offset=[[offset]] extractedTimestamp=[[timestamp]] extractor=[[timestampExtractor]]\n</code></pre>  <p><code>updateHead</code> is used when:</p> <ul> <li><code>RecordQueue</code> is requested to addRawRecords and poll</li> </ul>"},{"location":"RecordQueue/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.processor.internals.RecordQueue</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.processor.internals.RecordQueue=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"RecordQueueCreator/","text":"","title":"RecordQueueCreator"},{"location":"RecordQueueCreator/#creating-instance","text":"<p><code>RecordQueueCreator</code> takes the following to be created:</p> <ul> <li> <code>LogContext</code> <li> default.timestamp.extractor configuration property <li> default.deserialization.exception.handler configuration property  <p><code>RecordQueueCreator</code> is created when:</p> <ul> <li><code>StreamTask</code> is created</li> </ul>","title":"Creating Instance"},{"location":"RecordQueueCreator/#creating-recordqueue","text":"","title":"Creating RecordQueue <pre><code>RecordQueue createQueue(\n  TopicPartition partition)\n</code></pre> <p><code>createQueue</code> requests the ProcessorTopology for the SourceNode for the topic (of the given <code>TopicPartition</code>).</p> <p><code>createQueue</code> determines the TimestampExtractor that is one of the following:</p> <ol> <li>TimestampExtractor of the <code>SourceNode</code> if defined</li> <li>Default TimestampExtractor</li> </ol> <p><code>createQueue</code> is used when:</p> <ul> <li><code>StreamTask</code> is requested to createPartitionQueues and updateInputPartitions</li> </ul>"},{"location":"ReferenceContainer/","text":"<p><code>ReferenceContainer</code> is used to hold all of the following instances:</p> <ul> <li> Main Kafka <code>Consumer&lt;byte[], byte[]&gt;</code> (Apache Kafka) <li> <code>Admin</code> Client (Apache Kafka) <li> TaskManager <li> StreamsMetadataState <li> Assignment Error Code <li> Next Scheduled Rebalance Time <li> <code>Time</code>","title":"ReferenceContainer"},{"location":"ReferenceContainer/#creating-instance","text":"<p><code>ReferenceContainer</code> takes no arguments to be created.</p> <p><code>ReferenceContainer</code> is created\u00a0when:</p> <ul> <li><code>StreamThread</code> utility is used to create a StreamThread</li> </ul>","title":"Creating Instance"},{"location":"RepartitionTopics/","text":"<p><code>RepartitionTopics</code> is a helper class of StreamsPartitionAssignor (to prepare repartition topics).</p>","title":"RepartitionTopics"},{"location":"RepartitionTopics/#creating-instance","text":"<p><code>RepartitionTopics</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder <li> InternalTopicManager <li> CopartitionedTopicsEnforcer <li> <code>Cluster</code> metadata <li> Log prefix","title":"Creating Instance"},{"location":"RepartitionTopics/#topicpartitioninfos","text":"","title":"topicPartitionInfos <pre><code>Map&lt;TopicPartition, PartitionInfo&gt; topicPartitionInfos\n</code></pre> <p><code>RepartitionTopics</code> defines a <code>topicPartitionInfos</code> internal registry of <code>TopicPartition</code>s and the associated <code>PartitionInfo</code>.</p> <p><code>topicPartitionInfos</code> is initially empty and filled up when requested to setup.</p>"},{"location":"RepartitionTopics/#setup","text":"","title":"setup <pre><code>void setup()\n</code></pre> <p><code>setup</code> requests the InternalTopologyBuilder for the topic groups (that gives TopicsInfos by <code>Subtopology</code>).</p> <p><code>setup</code> computeRepartitionTopicConfig for the topic groups and the cluster metadata (that gives a <code>Map&lt;String, InternalTopicConfig&gt;</code>).</p> <p><code>setup</code> ensureCopartitioning of the copartitionGroups (from InternalTopologyBuilder).</p> <p><code>setup</code> requests the InternalTopicManager to make the repartition source topics ready (exist and have proper number of partitions, creating if necessary).</p>"},{"location":"RepartitionTopics/#computerepartitiontopicconfig","text":"","title":"computeRepartitionTopicConfig <pre><code>Map&lt;String, InternalTopicConfig&gt; computeRepartitionTopicConfig(\n  Map&lt;Subtopology, TopicsInfo&gt; topicGroups,\n  Cluster clusterMetadata)\n</code></pre> <p><code>computeRepartitionTopicConfig</code>...FIXME</p>"},{"location":"RepartitionTopics/#ensurecopartitioning","text":"","title":"ensureCopartitioning <pre><code>void ensureCopartitioning(\n  Collection&lt;Set&lt;String&gt;&gt; copartitionGroups,\n  Map&lt;String, InternalTopicConfig&gt; repartitionTopicMetadata,\n  Cluster clusterMetadata)\n</code></pre> <p><code>ensureCopartitioning</code>...FIXME</p>"},{"location":"StandbyTask/","text":"<p><code>StandbyTask</code> is a Task (and AbstractTask).</p>","title":"StandbyTask"},{"location":"StandbyTask/#creating-instance","text":"<p><code>StandbyTask</code> takes the following to be created:</p> <ul> <li> <code>TaskId</code> <li> Input <code>TopicPartition</code>s <li> ProcessorTopology <li> StreamsConfig <li> StreamsMetricsImpl <li> ProcessorStateManager <li> StateDirectory <li> <code>ThreadCache</code> <li> <code>InternalProcessorContext</code>  <p>When created, <code>StandbyTask</code> requests the InternalProcessorContext to <code>transitionToStandby</code> with the ThreadCache.</p> <p><code>StandbyTask</code> is created\u00a0when:</p> <ul> <li><code>StandbyTaskCreator</code> is requested to createStandbyTask</li> </ul>","title":"Creating Instance"},{"location":"StandbyTask/#abstracttask","text":"","title":"AbstractTask <p><code>StandbyTask</code> is an AbstractTask.</p>"},{"location":"StandbyTask/#task-type","text":"","title":"Task Type <p><code>StandbyTask</code> uses standby-task for task type.</p>"},{"location":"StandbyTask/#class","text":"","title":"Class <p><code>StandbyTask</code> uses <code>StandbyTask.class</code> for clazz.</p>"},{"location":"StandbyTaskCreator/","text":"","title":"StandbyTaskCreator"},{"location":"StandbyTaskCreator/#creating-instance","text":"<p><code>StandbyTaskCreator</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder <li> StreamsConfig <li> StreamsMetricsImpl <li> StateDirectory <li> ChangelogReader <li> Thread ID <li> <code>Logger</code>  <p>When created, <code>StandbyTaskCreator</code> initializes a task sensor and a ThreadCache.</p> <p><code>StandbyTaskCreator</code> is created\u00a0when:</p> <ul> <li><code>StreamThread</code> utility is used to create a StreamThread</li> </ul>","title":"Creating Instance"},{"location":"StateDirectory/","text":"<p><code>StateDirectory</code> is...FIXME</p>","title":"StateDirectory"},{"location":"StateManager/","text":"<p><code>StateManager</code> is an abstraction of state managers.</p>","title":"StateManager"},{"location":"StateManager/#contract","text":"","title":"Contract"},{"location":"StateManager/#basedir","text":"","title":"baseDir <pre><code>File baseDir()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractProcessorContext</code> is requested for the stateDir</li> <li><code>GlobalStateUpdateTask</code> is requested to close</li> <li><code>StateManagerUtil</code> is requested to closeStateManager</li> </ul>"},{"location":"StateManager/#changelogfor","text":"","title":"changelogFor <pre><code>String changelogFor(\n  String storeName)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractProcessorContext</code> is requested for the changelogFor</li> <li><code>GlobalStateManagerImpl</code> is created</li> <li><code>ProcessorStateManager</code> is requested to getStorePartition and isLoggingEnabled</li> </ul>"},{"location":"StateManager/#changelogoffsets","text":"","title":"changelogOffsets <pre><code>Map&lt;TopicPartition, Long&gt; changelogOffsets()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractTask</code> is requested to maybeWriteCheckpoint</li> <li><code>GlobalStateUpdateTask</code> is requested to initialize</li> <li><code>ProcessorStateManager</code> is requested to changelogPartitions</li> <li><code>StandbyTask</code> is requested to commitNeeded and changelogOffsets</li> <li><code>StoreChangelogReader</code> is requested to getPositionString</li> <li><code>StreamTask</code> is requested to changelogOffsets</li> </ul>"},{"location":"StateManager/#checkpoint","text":"","title":"checkpoint <pre><code>void checkpoint()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractTask</code> is requested to maybeWriteCheckpoint</li> <li><code>GlobalStateUpdateTask</code> is requested to flushState</li> </ul>"},{"location":"StateManager/#close","text":"","title":"close <pre><code>void close()\n</code></pre> <p>Used when:</p> <ul> <li><code>GlobalStateUpdateTask</code> is requested to close</li> <li><code>StateManagerUtil</code> is requested to closeStateManager</li> </ul>"},{"location":"StateManager/#flush","text":"","title":"flush <pre><code>void flush()\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractTask</code> is requested to maybeWriteCheckpoint</li> <li><code>GlobalStateUpdateTask</code> is requested to flushState</li> </ul>"},{"location":"StateManager/#getglobalstore","text":"","title":"getGlobalStore <pre><code>StateStore getGlobalStore(\n  String name)\n</code></pre> <p>Used when:</p> <ul> <li><code>GlobalProcessorContextImpl</code> is requested to getStateStore</li> <li><code>GlobalStateManagerImpl</code> is requested to getStore</li> <li><code>ProcessorContextImpl</code> is requested to getStateStore</li> </ul>"},{"location":"StateManager/#getstore","text":"","title":"getStore <pre><code>StateStore getStore(\n  String name)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractTask</code> is requested to getStore</li> <li><code>ProcessorContextImpl</code> is requested to getStateStore</li> <li><code>TopologyTestDriver</code> is requested to getStateStore</li> </ul>"},{"location":"StateManager/#registerstore","text":"","title":"registerStore <pre><code>void registerStore(\n  StateStore store,\n  StateRestoreCallback stateRestoreCallback)\n</code></pre> <p>Used when:</p> <ul> <li><code>AbstractProcessorContext</code> is requested to register</li> </ul>"},{"location":"StateManager/#tasktype","text":"","title":"taskType <pre><code>TaskType taskType()\n</code></pre>"},{"location":"StateManager/#updatechangelogoffsets","text":"","title":"updateChangelogOffsets <pre><code>void updateChangelogOffsets(\n  Map&lt;TopicPartition, Long&gt; writtenOffsets)\n</code></pre> <p>Used when:</p> <ul> <li><code>GlobalStateUpdateTask</code> is requested to flushState</li> <li><code>StreamTask</code> is requested to maybeWriteCheckpoint</li> </ul>"},{"location":"StateManager/#implementations","text":"<ul> <li>GlobalStateManager</li> <li>ProcessorStateManager</li> </ul>","title":"Implementations"},{"location":"StateManagerUtil/","text":"","title":"StateManagerUtil"},{"location":"StateManagerUtil/#registerstatestores","text":"","title":"registerStateStores <pre><code>void registerStateStores(\n  Logger log,\n  String logPrefix,\n  ProcessorTopology topology,\n  ProcessorStateManager stateMgr,\n  StateDirectory stateDirectory,\n  InternalProcessorContext processorContext)\n</code></pre> <p><code>registerStateStores</code>...FIXME</p> <p><code>registerStateStores</code>\u00a0is used when:</p> <ul> <li><code>StandbyTask</code> is requested to initializeIfNeeded</li> <li><code>StreamTask</code> is requested to initializeIfNeeded</li> </ul>"},{"location":"StreamTask/","text":"<p><code>StreamTask</code> is a concrete AbstractTask.</p> <p><code>StreamTask</code> creates exactly one PartitionGroup to handle all the input partitions.</p>","title":"StreamTask"},{"location":"StreamTask/#creating-instance","text":"<p><code>StreamTask</code> takes the following to be created:</p> <ul> <li> TaskId <li> Input <code>TopicPartition</code>s <li> ProcessorTopology <li>Kafka Consumer</li> <li> StreamsConfig <li> StreamsMetricsImpl <li> StateDirectory <li> ThreadCache <li> <code>Time</code> <li> ProcessorStateManager <li>RecordCollector</li> <li> InternalProcessorContext <li> <code>LogContext</code>  <p><code>StreamTask</code> is created\u00a0when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to create an active task</li> <li><code>TopologyTestDriver</code> is requested to setup a task</li> </ul>","title":"Creating Instance"},{"location":"StreamTask/#recordqueuecreator","text":"","title":"RecordQueueCreator <p>When created, <code>StreamTask</code> creates a RecordQueueCreator based on the following configuration properties:</p> <ul> <li>default.timestamp.extractor</li> <li>default.deserialization.exception.handler</li> </ul> <p><code>StreamTask</code> uses the <code>RecordQueueCreator</code> when:</p> <ul> <li>createPartitionQueues</li> <li>updateInputPartitions</li> </ul>"},{"location":"StreamTask/#bufferedrecordsperpartition","text":"","title":"buffered.records.per.partition <p><code>StreamTask</code> uses buffered.records.per.partition configuration property to control when to pause and resume record consumption (on a partition) when requested to add and process records, respectively.</p>"},{"location":"StreamTask/#main-kafka-consumer","text":"","title":"Main Kafka Consumer <p><code>StreamTask</code> is given a <code>Consumer&lt;byte[], byte[]&gt;</code> (Apache Kafka) when created.</p> <p><code>StreamTask</code> uses the <code>Consumer</code> for the following:</p> <ul> <li>Adding Records</li> <li>addPartitionsForOffsetReset</li> <li>prepareCommit (to committableOffsetsAndMetadata)</li> <li>Processing One Record</li> <li>completeRestoration (to resetOffsetsIfNeededAndInitializeMetadata)</li> </ul>"},{"location":"StreamTask/#recordcollector","text":"","title":"RecordCollector <p><code>StreamTask</code> is given a RecordCollector when created.</p>"},{"location":"StreamTask/#partitiongroup","text":"","title":"PartitionGroup <p>When created, <code>StreamTask</code> creates a PartitionGroup with the following:</p> <ul> <li>Partition Queues</li> <li><code>currentLag</code> function (Apache Kafka) of the main Consumer</li> <li> recordLatenessSensor <li>enforcedProcessingSensor</li> <li>max.task.idle.ms configuration property</li>"},{"location":"StreamTask/#createpartitionqueues","text":"","title":"createPartitionQueues <pre><code>Map&lt;TopicPartition, RecordQueue&gt; createPartitionQueues()\n</code></pre> <p><code>createPartitionQueues</code> requests the RecordQueueCreator to create one RecordQueue per every partition in the input partitions.</p>"},{"location":"StreamTask/#updateinputpartitions","text":"","title":"updateInputPartitions <pre><code>void updateInputPartitions(\n  Set&lt;TopicPartition&gt; topicPartitions,\n  Map&lt;String, List&lt;String&gt;&gt; allTopologyNodesToSourceTopics)\n</code></pre> <p><code>updateInputPartitions</code> updateInputPartitions.</p> <p>In the end, <code>updateInputPartitions</code> requests the PartitionGroup to updatePartitions (with the given <code>TopicPartition</code>s and createQueue factory).</p> <p><code>updateInputPartitions</code> is part of the Task abstraction.</p>"},{"location":"StreamTask/#streamtime","text":"","title":"streamTime <pre><code>long streamTime()\n</code></pre> <p><code>streamTime</code> requests the PartitionGroup for the streamTime.</p> <p><code>streamTime</code> is used when:</p> <ul> <li><code>ProcessorContextImpl</code> is requested for the current stream time</li> </ul>"},{"location":"StreamTask/#scheduling-recurring-action","text":"","title":"Scheduling Recurring Action <pre><code>Cancellable schedule(\n  long startTime,\n  long interval,\n  PunctuationType type,\n  Punctuator punctuator)\nCancellable schedule(\n  long interval,\n  PunctuationType type,\n  Punctuator punctuator)\n</code></pre> <p><code>schedule</code> creates a PunctuationSchedule (for the current ProcessorNode) and requests the stream-time or system-time <code>PunctuationQueue</code>s to schedule the PunctuationSchedule based on the given <code>PunctuationType</code> (<code>STREAM_TIME</code> or <code>WALL_CLOCK_TIME</code>, respectively).</p> <p><code>schedule</code>\u00a0is used when:</p> <ul> <li><code>ProcessorContextImpl</code> is requested to schedule a recurring action</li> </ul>"},{"location":"StreamTask/#stream-and-system-time-punctuationqueues","text":"","title":"Stream- and System-Time PunctuationQueues <p><code>StreamTask</code> creates two PunctuationQueues when created for the following Punctuators:</p> <ol> <li><code>streamTimePunctuationQueue</code> queue for <code>STREAM_TIME</code>-type punctuators</li> <li><code>systemTimePunctuationQueue</code> queue for <code>WALL_CLOCK_TIME</code>-type punctuators</li> </ol> <p><code>Punctuator</code>s are added to a corresponding <code>PunctuationQueue</code> when <code>StreamTask</code> is requested to schedule a recurring action.</p> <p>The actions are executed every maybePunctuateStreamTime or maybePunctuateSystemTime (when <code>TaskManager</code> is requested to punctuate).</p>"},{"location":"StreamTask/#maybepunctuatestreamtime","text":"","title":"maybePunctuateStreamTime <pre><code>boolean maybePunctuateStreamTime()\n</code></pre> <p><code>maybePunctuateStreamTime</code> returns <code>true</code> when at least one <code>STREAM_TIME</code> punctuator has been executed.</p>  <p><code>maybePunctuateStreamTime</code> requests the PartitionGroup for the stream time.</p>  <p>Note</p> <p>Stream time for <code>STREAM_TIME</code> punctuators is determined using the PartitionGroup.</p>  <p><code>maybePunctuateStreamTime</code> returns <code>false</code> for the stream time as <code>RecordQueue.UNKNOWN</code> (the stream time is yet to be determined and unknown).</p> <p><code>maybePunctuateStreamTime</code> requests the stream-time PunctuationQueue to mayPunctuate (with the stream time, <code>STREAM_TIME</code> punctuation type and this <code>StreamTask</code>). If there was at least one recurring action triggered (punctuated), <code>maybePunctuateStreamTime</code> marks this <code>StreamTask</code> as commitNeeded.</p> <p><code>maybePunctuateStreamTime</code> is part of the Task abstraction.</p>"},{"location":"StreamTask/#maybepunctuatesystemtime","text":"","title":"maybePunctuateSystemTime <pre><code>boolean maybePunctuateSystemTime()\n</code></pre> <p><code>maybePunctuateSystemTime</code> returns <code>true</code> when at least one <code>WALL_CLOCK_TIME</code> punctuator has been executed.</p>  <p><code>maybePunctuateSystemTime</code> requests the Time for the current time (in ms).</p>  <p>Note</p> <p>System time for <code>WALL_CLOCK_TIME</code> punctuators is determined using the Time.</p>  <p><code>maybePunctuateSystemTime</code> requests the system-time PunctuationQueue to mayPunctuate (with the current system time, <code>WALL_CLOCK_TIME</code> punctuation type and this <code>StreamTask</code>). If there was at least one recurring action triggered (punctuated), <code>maybePunctuateSystemTime</code> marks this <code>StreamTask</code> as commitNeeded.</p> <p><code>maybePunctuateSystemTime</code> is part of the Task abstraction.</p>"},{"location":"StreamTask/#preparecommit","text":"","title":"prepareCommit <pre><code>Map&lt;TopicPartition, OffsetAndMetadata&gt; prepareCommit()\n</code></pre> <p><code>prepareCommit</code>...FIXME</p> <p><code>prepareCommit</code>\u00a0is part of the Task abstraction.</p>"},{"location":"StreamTask/#committableoffsetsandmetadata","text":"","title":"committableOffsetsAndMetadata <pre><code>Map&lt;TopicPartition, OffsetAndMetadata&gt; committableOffsetsAndMetadata()\n</code></pre> <p><code>committableOffsetsAndMetadata</code>...FIXME</p>"},{"location":"StreamTask/#adding-records","text":"","title":"Adding Records <pre><code>void addRecords(\n  TopicPartition partition,\n  Iterable&lt;ConsumerRecord&lt;byte[], byte[]&gt;&gt; records)\n</code></pre> <p><code>addRecords</code> requests the PartitionGroup to add the given ConsumerRecords (for the given <code>TopicPartition</code>).</p> <p><code>addRecords</code> prints out the following TRACE message to the logs:</p> <pre><code>Added records into the buffered queue of partition [partition],\nnew queue size is [newQueueSize]\n</code></pre> <p><code>addRecords</code> can request the main Consumer to pause (suspend) fetching and consuming records from the partition if the queue size for the partition (after adding the new raw records) crossed the buffered.records.per.partition threshold.</p> <p><code>addRecords</code>\u00a0is part of the Task abstraction.</p>"},{"location":"StreamTask/#addpartitionsforoffsetreset","text":"","title":"addPartitionsForOffsetReset <pre><code>void addPartitionsForOffsetReset(\n  Set&lt;TopicPartition&gt; partitionsForOffsetReset)\n</code></pre> <p><code>addPartitionsForOffsetReset</code>...FIXME</p> <p><code>addPartitionsForOffsetReset</code>\u00a0is part of the Task abstraction.</p>"},{"location":"StreamTask/#processing-one-record","text":"","title":"Processing One Record <pre><code>boolean process(\n  long wallClockTime)\n</code></pre> <p><code>process</code>...FIXME</p> <p><code>process</code>\u00a0is part of the Task abstraction.</p>"},{"location":"StreamTask/#punctuate","text":"","title":"punctuate <pre><code>void punctuate(\n  ProcessorNode&lt;?, ?, ?, ?&gt; node,\n  long timestamp,\n  PunctuationType type,\n  Punctuator punctuator)\n</code></pre> <p><code>punctuate</code>...FIXME</p> <p><code>punctuate</code>\u00a0is part of the ProcessorNodePunctuator abstraction.</p>"},{"location":"StreamTask/#completerestoration","text":"","title":"completeRestoration <pre><code>void completeRestoration(\n  java.util.function.Consumer&lt;Set&lt;TopicPartition&gt;&gt; offsetResetter)\n</code></pre> <p><code>completeRestoration</code>...FIXME</p> <p><code>completeRestoration</code>\u00a0is part of the Task abstraction.</p>"},{"location":"StreamTask/#resetoffsetsifneededandinitializemetadata","text":"","title":"resetOffsetsIfNeededAndInitializeMetadata <pre><code>void resetOffsetsIfNeededAndInitializeMetadata(\n  java.util.function.Consumer&lt;Set&lt;TopicPartition&gt;&gt; offsetResetter)\n</code></pre> <p><code>resetOffsetsIfNeededAndInitializeMetadata</code>...FIXME</p>"},{"location":"StreamTask/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.processor.internals.StreamTask</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.processor.internals.StreamTask=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"StreamThread/","text":"<p><code>StreamThread</code> is a <code>Thread</code> (Java).</p>","title":"StreamThread"},{"location":"StreamThread/#creating-instance","text":"<p><code>StreamThread</code> takes the following to be created:</p> <ul> <li> <code>Time</code> <li> StreamsConfig <li> <code>Admin</code> <li> Main <code>Consumer&lt;byte[], byte[]&gt;</code> <li> Restore <code>Consumer&lt;byte[], byte[]&gt;</code> <li> ChangelogReader <li> <code>originalReset</code> <li> TaskManager <li> StreamsMetricsImpl <li> <code>InternalTopologyBuilder</code> <li> Thread ID <li> <code>LogContext</code> <li> <code>assignmentErrorCode</code> <li> <code>nextProbingRebalanceMs</code> <li> Shutdown Error Hook <li> <code>java.util.function.Consumer&lt;Throwable&gt;</code> <li> <code>java.util.function.Consumer&lt;Long&gt;</code>  <p><code>StreamThread</code> is created\u00a0using create utility.</p>","title":"Creating Instance"},{"location":"StreamThread/#commitintervalms","text":"","title":"commit.interval.ms <p><code>StreamThread</code> uses commit.interval.ms configuration property to control whether to commit tasks or not.</p>"},{"location":"StreamThread/#creating-streamthread","text":"","title":"Creating StreamThread <pre><code>StreamThread create(\n  InternalTopologyBuilder builder,\n  StreamsConfig config,\n  KafkaClientSupplier clientSupplier,\n  Admin adminClient,\n  UUID processId,\n  String clientId,\n  StreamsMetricsImpl streamsMetrics,\n  Time time,\n  StreamsMetadataState streamsMetadataState,\n  long cacheSizeBytes,\n  StateDirectory stateDirectory,\n  StateRestoreListener userStateRestoreListener,\n  int threadIdx,\n  Runnable shutdownErrorHook,\n  java.util.function.Consumer&lt;Throwable&gt; streamsUncaughtExceptionHandler)\n</code></pre> <p><code>create</code> creates a new ReferenceContainer with the given arguments:</p> <ul> <li><code>Admin</code> client (Apache Kafka)</li> <li>StreamsMetadataState</li> <li><code>Time</code></li> </ul> <p><code>create</code> prints out the following INFO message to the logs:</p> <pre><code>Creating restore consumer client\n</code></pre> <p><code>create</code> requests the given <code>StreamsConfig</code> for the restore consumer configs (with getRestoreConsumerClientId) and requests the given KafkaClientSupplier for a restore consumer.</p> <p><code>create</code> creates a StoreChangelogReader.</p> <p><code>create</code> creates a ThreadCache (with the given <code>cacheSizeBytes</code> and StreamsMetricsImpl of the parent <code>KafkaStreams</code>).</p> <p><code>create</code> creates a ActiveTaskCreator, a StandbyTaskCreator and a TaskManager.</p> <p><code>create</code> prints out the following INFO message to the logs:</p> <pre><code>Creating consumer client\n</code></pre> <p><code>create</code>...FIXME</p> <p><code>create</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is requested to createAndAddStreamThread</li> </ul>"},{"location":"StreamThread/#starting-execution","text":"","title":"Starting Execution <pre><code>void run()\n</code></pre> <p><code>run</code>...FIXME</p> <p><code>run</code>\u00a0is part of the <code>Thread</code> (Java) abstraction.</p>"},{"location":"StreamThread/#runloop","text":"","title":"runLoop <pre><code>void runLoop()\n</code></pre> <p><code>runLoop</code>...FIXME</p>"},{"location":"StreamThread/#runonce","text":"","title":"runOnce <pre><code>void runOnce()\n</code></pre> <p><code>runOnce</code> records the start time and poll latency of the poll phase.</p> <p><code>runOnce</code> continues work only while in running state. Otherwise, <code>runOnce</code> prints out the following DEBUG message to the logs and returns:</p> <pre><code>Thread state is already [state], skipping the run once call after poll request\n</code></pre> <p><code>runOnce</code> initializeAndRestorePhase and computes the latency.</p> <p>Only when in <code>RUNNING</code> state, <code>runOnce</code> processes tasks (in iterations).</p> <p>In the end, <code>runOnce</code> updates the <code>Sensor</code>s and every 2 minutes (non-configurable) prints out the following INFO message to the logs:</p> <pre><code>Processed [totalRecordsProcessedSinceLastSummary] total records,\nran [totalPunctuatorsSinceLastSummary] punctuators,\nand committed [totalCommittedSinceLastSummary] total tasks since the last update\n</code></pre>"},{"location":"StreamThread/#number-of-iterations","text":"","title":"Number of Iterations <p><code>StreamThread</code> uses <code>numIterations</code> internal registry for the maximum number of iterations.</p> <p>The <code>numIterations</code> starts as <code>1</code> when <code>StreamThread</code> is created and is updated (incremented or half'ed) at the end of every iteration (until active tasks processed no rows).</p> <p>The <code>numIterations</code> is used as the maximum number of records for the TaskManager to process records.</p> <p><code>numIterations</code> is printed out twice to the logs at DEBUG level.</p>"},{"location":"StreamThread/#processing-tasks-in-iterations","text":"","title":"Processing Tasks (in Iterations) <p>When in <code>RUNNING</code> state, <code>runOnce</code> executes the following steps the maximum number of iterations.</p> <p><code>runOnce</code>...FIXME</p> <p><code>runOnce</code> requests the TaskManager to punctuate.</p> <p><code>runOnce</code>...FIXME</p>"},{"location":"StreamThread/#initializeandrestorephase","text":"","title":"initializeAndRestorePhase <pre><code>void initializeAndRestorePhase()\n</code></pre> <p><code>initializeAndRestorePhase</code>...FIXME</p> <p><code>initializeAndRestorePhase</code> prints out the following DEBUG message to the logs:</p> <pre><code>Idempotently invoking restoration logic in state [state]\n</code></pre> <p>In the end, <code>initializeAndRestorePhase</code> requests the ChangelogReader to restore state stores and prints out the following DEBUG message to the logs:</p> <pre><code>Idempotent restore call done. Thread state has not changed.\n</code></pre>"},{"location":"StreamThread/#maybecommit","text":"","title":"maybeCommit <pre><code>int maybeCommit()\n</code></pre> <p><code>maybeCommit</code> checks out whether to commit active and standby tasks (based on the last commit time and commit.interval.ms).</p> <p>If the last commit happened enough long ago, <code>maybeCommit</code> prints out the following DEBUG message to the logs:</p> <pre><code>Committing all active tasks [ids] and standby tasks [ids] since [time]ms has elapsed (commit interval is [time]ms)\n</code></pre> <p><code>maybeCommit</code> requests the TaskManager to commit the tasks that are <code>RUNNING</code> or <code>RESTORING</code>.</p> <p>If there were offsets committed, <code>maybeCommit</code> requests the TaskManager to maybePurgeCommittedRecords. Otherwise, <code>maybeCommit</code> prints out the following DEBUG message to the logs:</p> <pre><code>Unable to commit as we are in the middle of a rebalance, will try again when it completes.\n</code></pre> <p>If the last commit happened fairly recently, <code>maybeCommit</code> merely requests the TaskManager to maybeCommitActiveTasksPerUserRequested</p> <p>Either way, in the end, <code>maybeCommit</code> returns the number of committed offsets.</p>"},{"location":"StreamThread/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.processor.internals.StreamThread</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.processor.internals.StreamThread=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"StreamsConfig/","text":"","title":"StreamsConfig"},{"location":"StreamsConfig/#applicationid","text":"","title":"application.id"},{"location":"StreamsConfig/#applicationserver","text":"","title":"application.server <p>A <code>host:port</code> endpoint address of the KafkaStreams instance</p>"},{"location":"StreamsConfig/#bufferedrecordsperpartition","text":"","title":"buffered.records.per.partition <p>Maximum number of records to buffer per partition (that <code>StreamTask</code> uses for pausing and resuming record fetching)</p> <p>Default: <code>1000</code></p>"},{"location":"StreamsConfig/#builtinmetricsversion","text":"","title":"built.in.metrics.version"},{"location":"StreamsConfig/#cachemaxbytesbuffering","text":"","title":"cache.max.bytes.buffering <p>Maximum number of memory bytes for buffering across all threads (that KafkaStreams uses to indirectly create ThreadCaches).</p> <p>Default: <code>10MB</code> (<code>10 * 1024 * 1024L</code>)</p> <p>Must be at least <code>0</code></p> <p>Used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> <li><code>InternalTopologyBuilder</code> is requested to rewriteTopology (and disable caching for local and global state stores)</li> <li><code>TopologyTestDriver</code> is created</li> </ul>"},{"location":"StreamsConfig/#clientid","text":"","title":"client.id <p>See Apache Kafka</p>"},{"location":"StreamsConfig/#commitintervalms","text":"","title":"commit.interval.ms <p>How often to save (commit and flush) the position of a processor</p> <p>Default: <code>30000L</code> (or <code>100L</code> for processing.guarantee being <code>exactly_once_v2</code> or deprecated <code>exactly_once</code>)</p> <p>Must be at least <code>0</code></p> <p><code>commit.interval.ms</code> has to be lower than <code>transaction.timeout.ms</code> (or ongoing transaction always time out due to inactivity caused by long commit interval)</p> <p>Used when:</p> <ul> <li><code>GlobalStreamThread</code> is requested to initialize (and create a <code>StateConsumer</code>)</li> <li><code>StoreChangelogReader</code> is created</li> <li><code>StreamThread</code> is created</li> </ul>"},{"location":"StreamsConfig/#defaultdeserializationexceptionhandler","text":"","title":"default.deserialization.exception.handler <p>The default <code>DeserializationExceptionHandler</code></p> <p>Default: <code>LogAndFailExceptionHandler</code></p>"},{"location":"StreamsConfig/#defaulttimestampextractor","text":"","title":"default.timestamp.extractor <p>The default TimestampExtractor</p> <p>Default: FailOnInvalidTimestamp</p>"},{"location":"StreamsConfig/#maxtaskidlems","text":"","title":"max.task.idle.ms"},{"location":"StreamsConfig/#metricsnumsamples","text":"","title":"metrics.num.samples <p>(Re)Defined in <code>StreamsConfig</code> as <code>CommonClientConfigs.METRICS_NUM_SAMPLES_CONFIG</code> (Apache Kafka)</p>"},{"location":"StreamsConfig/#metricsrecordinglevel","text":"","title":"metrics.recording.level <p>(Re)Defined in <code>StreamsConfig</code> as <code>CommonClientConfigs.METRICS_RECORDING_LEVEL_CONFIG</code> (Apache Kafka)</p>"},{"location":"StreamsConfig/#metricreporters","text":"","title":"metric.reporters <p>(Re)Defined in <code>StreamsConfig</code> as <code>CommonClientConfigs.METRIC_REPORTER_CLASSES_CONFIG</code> (Apache Kafka)</p>"},{"location":"StreamsConfig/#metricssamplewindowms","text":"","title":"metrics.sample.window.ms <p>(Re)Defined in <code>StreamsConfig</code> as <code>CommonClientConfigs.METRICS_SAMPLE_WINDOW_MS_CONFIG</code> (Apache Kafka)</p>"},{"location":"StreamsConfig/#pollms","text":"","title":"poll.ms <p>Time (in millis) to block waiting for input</p> <p>Default: <code>100L</code></p> <p>Used when:</p> <ul> <li><code>GlobalStateManagerImpl</code> is created</li> <li><code>GlobalStreamThread</code> is requested to initialize</li> <li><code>StoreChangelogReader</code> is created</li> <li><code>StreamThread</code> is created</li> </ul>"},{"location":"StreamsConfig/#tasktimeoutms","text":"","title":"task.timeout.ms"},{"location":"StreamsConfig/#upgradefrom","text":"","title":"upgrade.from <p>Default: (undefined)</p>"},{"location":"StreamsMetadata/","text":"<p><code>StreamsMetadata</code> is...FIXME</p>","title":"StreamsMetadata"},{"location":"StreamsMetadataState/","text":"<p><code>StreamsMetadataState</code> is used by a KafkaStreams instance to manage global metadata (of all the <code>KafkaStreams</code>s of a Kafka Streams application).</p>","title":"StreamsMetadataState"},{"location":"StreamsMetadataState/#creating-instance","text":"<p><code>StreamsMetadataState</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder <li> User-Defined Endpoint  <p><code>StreamsMetadataState</code> is created\u00a0when:</p> <ul> <li><code>KafkaStreams</code> is created</li> </ul>","title":"Creating Instance"},{"location":"StreamsMetadataState/#global-metadata","text":"","title":"Global Metadata <pre><code>List&lt;StreamsMetadata&gt; allMetadata\n</code></pre> <p><code>StreamsMetadataState</code> uses a <code>allMetadata</code> internal registry for all StreamsMetadatas.</p> <p>The <code>allMetadata</code> registry is initially empty and rebuilt every onChange.</p> <p>The <code>allMetadata</code> registry is available using KafkaStreams.metadataForAllStreamsClients.</p> <p>Used when:</p> <ul> <li>toString</li> <li>getAllMetadataForStore</li> <li>getKeyQueryMetadataForKey</li> </ul>"},{"location":"StreamsMetadataState/#onchange","text":"","title":"onChange <pre><code>void onChange(\n  Map&lt;HostInfo, Set&lt;TopicPartition&gt;&gt; activePartitionHostMap,\n  Map&lt;HostInfo, Set&lt;TopicPartition&gt;&gt; standbyPartitionHostMap,\n  Cluster clusterMetadata)\n</code></pre> <p><code>onChange</code> stores the given <code>Cluster</code> metadata (in the clusterMetadata internal registry) and rebuildMetadata.</p> <p><code>onChange</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to assign and onAssignment</li> </ul>"},{"location":"StreamsMetadataState/#rebuildmetadata","text":"","title":"rebuildMetadata <pre><code>void rebuildMetadata(\n  Map&lt;HostInfo, Set&lt;TopicPartition&gt;&gt; activePartitionHostMap,\n  Map&lt;HostInfo, Set&lt;TopicPartition&gt;&gt; standbyPartitionHostMap)\n</code></pre> <p><code>rebuildMetadata</code>...FIXME</p>"},{"location":"StreamsMetadataState/#getkeyquerymetadataforkey","text":"","title":"getKeyQueryMetadataForKey <pre><code>KeyQueryMetadata getKeyQueryMetadataForKey(\n  String storeName,\n  K key,\n  Serializer&lt;K&gt; keySerializer) // (1)\nKeyQueryMetadata getKeyQueryMetadataForKey(\n  String storeName,\n  K key,\n  StreamPartitioner&lt;? super K, ?&gt; partitioner)\nKeyQueryMetadata getKeyQueryMetadataForKey(\n  String storeName,\n  K key,\n  StreamPartitioner&lt;? super K, ?&gt; partitioner,\n  SourceTopicsInfo sourceTopicsInfo) // (2)\n</code></pre> <ol> <li>Uses <code>DefaultStreamPartitioner</code></li> <li>A private method</li> </ol> <p><code>getKeyQueryMetadataForKey</code>...FIXME</p> <p><code>getKeyQueryMetadataForKey</code>\u00a0is used when:</p> <ul> <li><code>KafkaStreams</code> is requested to queryMetadataForKey</li> </ul>"},{"location":"StreamsPartitionAssignor/","text":"<p><code>StreamsPartitionAssignor</code> is a <code>ConsumerPartitionAssignor</code> (Apache Kafka) and a <code>Configurable</code> (Apache Kafka).</p>","title":"StreamsPartitionAssignor"},{"location":"StreamsPartitionAssignor/#supported-rebalance-protocols","text":"","title":"Supported Rebalance Protocols <pre><code>List&lt;RebalanceProtocol&gt; supportedProtocols()\n</code></pre> <p><code>supportedProtocols</code> returns the following <code>RebalanceProtocol</code>s:</p> <ol> <li><code>RebalanceProtocol.EAGER</code></li> <li><code>RebalanceProtocol.COOPERATIVE</code> (based on upgrade.from)</li> </ol> <p><code>supportedProtocols</code>\u00a0is part of the <code>ConsumerPartitionAssignor</code> (Apache Kafka) abstraction.</p>"},{"location":"StreamsPartitionAssignor/#name","text":"","title":"Name <pre><code>String name()\n</code></pre> <p><code>name</code> is stream.</p> <p><code>name</code>\u00a0is part of the <code>ConsumerPartitionAssignor</code> (Apache Kafka) abstraction.</p>"},{"location":"StreamsPartitionAssignor/#configure","text":"","title":"configure <pre><code>void configure(\n  Map&lt;String, ?&gt; configs)\n</code></pre> <p><code>configure</code> creates a new AssignorConfiguration (with the given <code>configs</code>).</p> <p><code>configure</code>...FIXME</p> <p><code>configure</code>\u00a0is part of the <code>Configurable</code> (Apache Kafka) abstraction.</p>"},{"location":"StreamsPartitionAssignor/#streamsmetadatastate","text":"","title":"StreamsMetadataState <p><code>StreamsPartitionAssignor</code> is given a StreamsMetadataState (from a ReferenceContainer) when requested to configure.</p> <p>The <code>StreamsMetadataState</code> is used (to handle partition assignment change) when:</p> <ul> <li>assign</li> <li>onAssignment</li> </ul>"},{"location":"StreamsPartitionAssignor/#consumer-group-assignment","text":"","title":"Consumer Group Assignment <pre><code>GroupAssignment assign(\n  Cluster metadata,\n  GroupSubscription groupSubscription)\n</code></pre> <p><code>assign</code>...FIXME</p> <p><code>assign</code> prints out the following DEBUG message to the logs:</p> <pre><code>Constructed client metadata [clientMetadata] from the member subscriptions.\n</code></pre> <p><code>assign</code> prepareRepartitionTopics with the given cluster metadata (that gives a <code>Map&lt;TopicPartition, PartitionInfo&gt;</code> as <code>allRepartitionTopicPartitions</code>).</p> <p><code>assign</code> prints out the following DEBUG message to the logs:</p> <pre><code>Created repartition topics [allRepartitionTopicPartitions] from the parsed topology.\n</code></pre> <p><code>assign</code>...FIXME</p> <p><code>assign</code>\u00a0is part of the <code>ConsumerPartitionAssignor</code> (Apache Kafka) abstraction.</p>"},{"location":"StreamsPartitionAssignor/#preparerepartitiontopics","text":"","title":"prepareRepartitionTopics <pre><code>Map&lt;TopicPartition, PartitionInfo&gt; prepareRepartitionTopics(\n  Cluster metadata)\n</code></pre> <p><code>prepareRepartitionTopics</code> creates a new RepartitionTopics that is requested to setup and then for the topicPartitionsInfo.</p>"},{"location":"StreamsPartitionAssignor/#assigntaskstoclients","text":"","title":"assignTasksToClients <pre><code>boolean assignTasksToClients(\n  Cluster fullMetadata,\n  Set&lt;String&gt; allSourceTopics,\n  Map&lt;Subtopology, TopicsInfo&gt; topicGroups,\n  Map&lt;UUID, ClientMetadata&gt; clientMetadataMap,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; partitionsForTask,\n  Set&lt;TaskId&gt; statefulTasks)\n</code></pre> <p><code>assignTasksToClients</code> creates local <code>taskForPartition</code> (<code>Map&lt;TopicPartition, TaskId&gt;</code>) and <code>tasksForTopicGroup</code> (<code>Map&lt;Subtopology, Set&lt;TaskId&gt;&gt;</code>) collections that are used to populate tasks.</p> <p><code>assignTasksToClients</code> creates a ChangelogTopics (with the <code>tasksForTopicGroup</code> local collection) that is in turn requested to setup.</p> <p><code>assignTasksToClients</code> populateClientStatesMap.</p> <p><code>assignTasksToClients</code> prints out the following INFO message to the logs:</p> <pre><code>All members participating in this rebalance:\n [UUID]: [consumers].\n</code></pre> <p><code>assignTasksToClients</code> prints out the following DEBUG message to the logs:</p> <pre><code>Assigning tasks [allTasks] including stateful [statefulTasks] to clients [clientStates] with number of replicas [numStandbyReplicas]\n</code></pre> <p><code>assignTasksToClients</code> creates a TaskAssignor that is in turn requested to assign.</p> <p><code>assignTasksToClients</code> prints out the following INFO message to the logs:</p> <pre><code>Assigned tasks [allTasks] including stateful [statefulTasks] to clients as:\n [UUID]=[currentAssignment].\n</code></pre> <p>In the end, <code>assignTasksToClients</code> returns whether the generated assignment requires a followup probing rebalance (from the TaskAssignor).</p>"},{"location":"StreamsPartitionAssignor/#populatetasksformaps","text":"","title":"populateTasksForMaps <pre><code>void populateTasksForMaps(\n  Map&lt;TopicPartition, TaskId&gt; taskForPartition,\n  Map&lt;Subtopology, Set&lt;TaskId&gt;&gt; tasksForTopicGroup,\n  Set&lt;String&gt; allSourceTopics,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; partitionsForTask,\n  Cluster fullMetadata)\n</code></pre> <p><code>populateTasksForMaps</code>...FIXME</p>"},{"location":"StreamsPartitionAssignor/#populateclientstatesmap","text":"","title":"populateClientStatesMap <pre><code>boolean populateClientStatesMap(\n  Map&lt;UUID, ClientState&gt; clientStates,\n  Map&lt;UUID, ClientMetadata&gt; clientMetadataMap,\n  Map&lt;TopicPartition, TaskId&gt; taskForPartition,\n  ChangelogTopics changelogTopics)\n</code></pre> <p><code>populateClientStatesMap</code>...FIXME</p>"},{"location":"StreamsPartitionAssignor/#createtaskassignor","text":"","title":"createTaskAssignor <pre><code>TaskAssignor createTaskAssignor(\n  boolean lagComputationSuccessful)\n</code></pre> <p><code>createTaskAssignor</code> creates a TaskAssignor (using the taskAssignorSupplier function).</p>"},{"location":"StreamsPartitionAssignor/#handling-task-and-partition-assignment","text":"","title":"Handling Task and Partition Assignment <pre><code>void onAssignment(\n  Assignment assignment,\n  ConsumerGroupMetadata metadata)\n</code></pre> <p><code>onAssignment</code>\u00a0is part of the <code>ConsumerPartitionAssignor</code> (Apache Kafka) abstraction.</p>  <p><code>onAssignment</code> validateActiveTaskEncoding.</p> <p><code>onAssignment</code> gets the active tasks (from the partitions and the <code>AssignmentInfo</code> of the given <code>Assignment</code>).</p> <p><code>onAssignment</code> maybeScheduleFollowupRebalance.</p> <p><code>onAssignment</code> creates an empty (fake) <code>Cluster</code> metadata (with the partitions by host, i.e. <code>Map&lt;HostInfo, Set&lt;TopicPartition&gt;&gt;</code>) and requests the StreamsMetadataState to handle the assignment change.</p> <p>In the end, <code>onAssignment</code> requests the TaskManager to handle the task and partition assignment (with the active and standby tasks).</p>"},{"location":"StreamsPartitionAssignor/#validateactivetaskencoding","text":"","title":"validateActiveTaskEncoding <pre><code>void validateActiveTaskEncoding(\n  List&lt;TopicPartition&gt; partitions,\n  AssignmentInfo info,\n  String logPrefix)\n</code></pre> <p><code>validateActiveTaskEncoding</code> throws a <code>TaskAssignmentException</code> when the number of <code>partitions</code> is not the same as the number of active tasks (of the given <code>AssignmentInfo</code>):</p> <pre><code>[logPrefix]Number of assigned partitions [partitions]\nis not equal to the number of active taskIds [activeTasks], assignmentInfo=[info]\n</code></pre>"},{"location":"StreamsPartitionAssignor/#active-tasks","text":"","title":"Active Tasks <pre><code>Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; getActiveTasks(\n  List&lt;TopicPartition&gt; partitions,\n  AssignmentInfo info)\n</code></pre> <p><code>getActiveTasks</code> returns TaskIds and the associated <code>TopicPartition</code>s (from the <code>partitions</code>).</p> <p><code>getActiveTasks</code> finds the TaskIds among the <code>activeTasks</code> in the given <code>AssignmentInfo</code>.</p> <p><code>getActiveTasks</code> assumes that the position of the <code>TopicPartition</code> in the given <code>partitions</code> is the position of the corresponding <code>TaskId</code> in the <code>activeTasks</code> in the given <code>AssignmentInfo</code>.</p>"},{"location":"StreamsPartitionAssignor/#maybeschedulefollowuprebalance","text":"","title":"maybeScheduleFollowupRebalance <pre><code>void maybeScheduleFollowupRebalance(\n  long encodedNextScheduledRebalanceMs,\n  int receivedAssignmentMetadataVersion,\n  int latestCommonlySupportedVersion,\n  Set&lt;HostInfo&gt; groupHostInfo)\n</code></pre> <p><code>maybeScheduleFollowupRebalance</code>...FIXME</p>"},{"location":"StreamsPartitionAssignor/#subscriptionuserdata","text":"","title":"subscriptionUserData <pre><code>ByteBuffer subscriptionUserData(\n  Set&lt;String&gt; topics)\n</code></pre> <p><code>subscriptionUserData</code>...FIXME</p> <p><code>subscriptionUserData</code>\u00a0is part of the <code>ConsumerPartitionAssignor</code> (Apache Kafka) abstraction.</p>"},{"location":"StreamsPartitionAssignor/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"StreamsProducer/","text":"<p><code>StreamsProducer</code> uses a Kafka Producer to send records for RecordCollectorImpl.</p>","title":"StreamsProducer"},{"location":"StreamsProducer/#creating-instance","text":"<p><code>StreamsProducer</code> takes the following to be created:</p> <ul> <li> StreamsConfig <li> Thread ID <li> KafkaClientSupplier <li> TaskId <li> Process ID <li> <code>LogContext</code>  <p><code>StreamsProducer</code> is created\u00a0when:</p> <ul> <li><code>ActiveTaskCreator</code> is created and requested to createActiveTask</li> </ul>","title":"Creating Instance"},{"location":"StreamsProducer/#kafka-producer","text":"","title":"Kafka Producer <p><code>StreamsProducer</code> requests the KafkaClientSupplier to get a Kafka Producer (<code>Producer&lt;byte[], byte[]&gt;</code>) when created and requested to resetProducer.</p>"},{"location":"StreamsProducer/#sending-record","text":"","title":"Sending Record <pre><code>Future&lt;RecordMetadata&gt; send(\n  ProducerRecord&lt;byte[], byte[]&gt; record,\n  Callback callback)\n</code></pre> <p><code>send</code> maybeBeginTransaction and requests the Producer to send the record (with the given <code>Callback</code>).</p> <p><code>send</code>\u00a0is used when:</p> <ul> <li><code>RecordCollectorImpl</code> is requested to send a record</li> </ul>"},{"location":"StreamsProducer/#maybebegintransaction","text":"","title":"maybeBeginTransaction <pre><code>void maybeBeginTransaction()\n</code></pre> <p><code>maybeBeginTransaction</code>...FIXME</p> <p><code>maybeBeginTransaction</code>\u00a0is used when:</p> <ul> <li><code>StreamsProducer</code> is requested to send a record and commitTransaction</li> </ul>"},{"location":"StreamsProducer/#committransaction","text":"","title":"commitTransaction <pre><code>void commitTransaction(\n  Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,\n  ConsumerGroupMetadata consumerGroupMetadata)\n</code></pre> <p><code>commitTransaction</code>...FIXME</p> <p><code>commitTransaction</code>\u00a0is used when:</p> <ul> <li><code>TaskManager</code> is requested to commitOffsetsOrTransaction</li> <li><code>TopologyTestDriver</code> is requested to commit</li> </ul>"},{"location":"Task/","text":"<p><code>Task</code> is an abstraction of tasks.</p>","title":"Task"},{"location":"Task/#contract","text":"","title":"Contract"},{"location":"Task/#adding-records-to-active-streamtasks","text":"","title":"Adding Records (to Active StreamTasks) <pre><code>void addRecords(\n  TopicPartition partition,\n  Iterable&lt;ConsumerRecord&lt;byte[], byte[]&gt;&gt; records)\n</code></pre> <p>Adds records to StreamTasks (while StandbyTasks throw an <code>IllegalStateException</code>)</p> <p>Used when:</p> <ul> <li><code>TaskManager</code> is requested to add records to active tasks</li> <li><code>TopologyTestDriver</code> is requested to enqueueTaskRecord</li> </ul>"},{"location":"Task/#changelogoffsets","text":"","title":"changelogOffsets <pre><code>Map&lt;TopicPartition, Long&gt; changelogOffsets()\n</code></pre>"},{"location":"Task/#changelogpartitions","text":"","title":"changelogPartitions <pre><code>Collection&lt;TopicPartition&gt; changelogPartitions()\n</code></pre>"},{"location":"Task/#cleartasktimeout","text":"","title":"clearTaskTimeout <pre><code>void clearTaskTimeout()\n</code></pre>"},{"location":"Task/#closeclean","text":"","title":"closeClean <pre><code>void closeClean()\n</code></pre>"},{"location":"Task/#closecleanandrecyclestate","text":"","title":"closeCleanAndRecycleState <pre><code>void closeCleanAndRecycleState()\n</code></pre>"},{"location":"Task/#closedirty","text":"","title":"closeDirty <pre><code>void closeDirty()\n</code></pre>"},{"location":"Task/#commitneeded","text":"","title":"commitNeeded <pre><code>boolean commitNeeded()\n</code></pre>"},{"location":"Task/#committedoffsets","text":"","title":"committedOffsets <pre><code>Map&lt;TopicPartition, Long&gt; committedOffsets()\n</code></pre>"},{"location":"Task/#completerestoration","text":"","title":"completeRestoration <pre><code>void completeRestoration(\n  java.util.function.Consumer&lt;Set&lt;TopicPartition&gt;&gt; offsetResetter)\n</code></pre> <p>Used when:</p> <ul> <li><code>TaskManager</code> is requested to tryToCompleteRestoration</li> <li><code>TopologyTestDriver</code> is requested to setupTask</li> </ul>"},{"location":"Task/#getstore","text":"","title":"getStore <pre><code>StateStore getStore(\n  String name)\n</code></pre> <p>Used when:</p> <ul> <li><code>StreamThreadStateStoreProvider</code> is requested for stores</li> </ul>"},{"location":"Task/#highwatermark","text":"","title":"highWaterMark <pre><code>Map&lt;TopicPartition, Long&gt; highWaterMark()\n</code></pre>"},{"location":"Task/#taskid","text":"","title":"TaskId <pre><code>TaskId id()\n</code></pre>"},{"location":"Task/#initializeifneeded","text":"","title":"initializeIfNeeded <pre><code>void initializeIfNeeded()\n</code></pre>"},{"location":"Task/#inputpartitions","text":"","title":"inputPartitions <pre><code>Set&lt;TopicPartition&gt; inputPartitions()\n</code></pre>"},{"location":"Task/#isactive","text":"","title":"isActive <pre><code>boolean isActive()\n</code></pre>"},{"location":"Task/#markchangelogascorrupted","text":"","title":"markChangelogAsCorrupted <pre><code>void markChangelogAsCorrupted(\n  Collection&lt;TopicPartition&gt; partitions)\n</code></pre>"},{"location":"Task/#maybeinittasktimeoutorthrow","text":"","title":"maybeInitTaskTimeoutOrThrow <pre><code>void maybeInitTaskTimeoutOrThrow(\n  long currentWallClockMs,\n  Exception cause)\n</code></pre>"},{"location":"Task/#maybepunctuatestreamtime","text":"","title":"maybePunctuateStreamTime <pre><code>boolean maybePunctuateStreamTime()\n</code></pre> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li><code>TaskManager</code> is requested to punctuate</li> <li><code>TopologyTestDriver</code> is requested to completeAllProcessableWork</li> </ul>"},{"location":"Task/#maybepunctuatesystemtime","text":"","title":"maybePunctuateSystemTime <pre><code>boolean maybePunctuateSystemTime()\n</code></pre> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li><code>TaskManager</code> is requested to punctuate</li> <li><code>TopologyTestDriver</code> is requested to advanceWallClockTime</li> </ul>"},{"location":"Task/#needsinitializationorrestoration","text":"","title":"needsInitializationOrRestoration <pre><code>boolean needsInitializationOrRestoration()\n</code></pre> <p>Default: Whether this task is in <code>CREATED</code> or <code>RESTORING</code> state</p> <p>Used when:</p> <ul> <li><code>TaskManager</code> is requested for needsInitializationOrRestoration</li> </ul>"},{"location":"Task/#postcommit","text":"","title":"postCommit <pre><code>void postCommit(\n  boolean enforceCheckpoint)\n</code></pre>"},{"location":"Task/#preparecommit","text":"","title":"prepareCommit <pre><code>Map&lt;TopicPartition, OffsetAndMetadata&gt; prepareCommit()\n</code></pre> <p>Used when:</p> <ul> <li><code>TaskManager</code> is requested to closeDirtyAndRevive, handleCloseAndRecycle, prepareCommitAndAddOffsetsToMap, closeTaskDirty, tryCloseCleanAllActiveTasks, tryCloseCleanAllStandbyTasks and commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</li> <li><code>TopologyTestDriver</code> is requested to completeAllProcessableWork, advanceWallClockTime and close</li> </ul>"},{"location":"Task/#processing-record","text":"","title":"Processing Record <pre><code>boolean process(\n  long wallClockTime)\n</code></pre> <p>Default: <code>false</code> (and overriden in StreamTask)</p> <p>Used when:</p> <ul> <li><code>TaskManager</code> is requested to process records</li> <li><code>TopologyTestDriver</code> is requested to completeAllProcessableWork</li> </ul>"},{"location":"Task/#resuming","text":"","title":"Resuming <pre><code>void resume()\n</code></pre>"},{"location":"Task/#revive","text":"","title":"revive <pre><code>void revive()\n</code></pre>"},{"location":"Task/#state","text":"","title":"state <pre><code>State state()\n</code></pre>"},{"location":"Task/#suspend","text":"","title":"suspend <pre><code>void suspend()\n</code></pre>"},{"location":"Task/#timecurrentidlingstarted","text":"","title":"timeCurrentIdlingStarted <pre><code>Optional&lt;Long&gt; timeCurrentIdlingStarted()\n</code></pre>"},{"location":"Task/#updateinputpartitions","text":"","title":"updateInputPartitions <pre><code>void updateInputPartitions(\n  Set&lt;TopicPartition&gt; topicPartitions,\n  Map&lt;String, List&lt;String&gt;&gt; allTopologyNodesToSourceTopics)\n</code></pre>"},{"location":"Task/#implementations","text":"<ul> <li>AbstractTask</li> <li>StandbyTask</li> <li>StreamTask</li> </ul>","title":"Implementations"},{"location":"TaskAssignor/","text":"<p><code>TaskAssignor</code> is...FIXME</p>","title":"TaskAssignor"},{"location":"TaskId/","text":"<p><code>TaskId</code> is...FIXME</p>","title":"TaskId"},{"location":"TaskManager/","text":"","title":"TaskManager"},{"location":"TaskManager/#creating-instance","text":"<p><code>TaskManager</code> takes the following to be created:</p> <ul> <li> <code>Time</code> <li> ChangelogReader <li> Process UUID <li> Log Prefix <li> StreamsMetricsImpl <li>ActiveTaskCreator</li> <li> StandbyTaskCreator <li> InternalTopologyBuilder <li> <code>Admin</code> Client (Apache Kafka) <li> StateDirectory <li> <code>StreamThread.ProcessingMode</code>  <p><code>TaskManager</code> is created\u00a0when:</p> <ul> <li><code>StreamThread</code> utility is used to create a StreamThread</li> </ul>","title":"Creating Instance"},{"location":"TaskManager/#tasks","text":"","title":"Tasks <p><code>TaskManager</code> creates a Tasks when created.</p>"},{"location":"TaskManager/#activetaskcreator","text":"","title":"ActiveTaskCreator <p><code>TaskManager</code> is given an ActiveTaskCreator when created.</p> <p><code>TaskManager</code> uses this <code>ActiveTaskCreator</code> (along with StandbyTaskCreator) merely to create a Tasks.</p>"},{"location":"TaskManager/#punctuating-recurring-actions","text":"","title":"Punctuating Recurring Actions <pre><code>int punctuate()\n</code></pre> <p><code>punctuate</code> requests every active task to maybePunctuateStreamTime and maybePunctuateSystemTime (counting punctuators that were executed).</p> <p><code>punctuate</code> is used when:</p> <ul> <li><code>StreamThread</code> is requested to runOnce</li> </ul>"},{"location":"TaskManager/#committing-active-tasks","text":"","title":"Committing (Active) Tasks <pre><code>int commit(\n  Collection&lt;Task&gt; tasksToCommit)\n</code></pre> <p><code>commit</code> commitAndFillInConsumedOffsetsAndMetadataPerTaskMap the given tasks.</p> <p>In the end, <code>commit</code> returns consumed offsets and metadata per every committed task (<code>Map&lt;Task, Map&lt;TopicPartition, OffsetAndMetadata&gt;&gt;</code>).</p> <p><code>commit</code>\u00a0is used when:</p> <ul> <li><code>StreamThread</code> is requested to maybeCommit</li> <li><code>TaskManager</code> is requested to maybeCommitActiveTasksPerUserRequested</li> </ul>"},{"location":"TaskManager/#handling-active-and-standby-task-assignment","text":"","title":"Handling Active and Standby Task Assignment <pre><code>void handleAssignment(\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; activeTasks,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; standbyTasks)\n</code></pre> <p><code>handleAssignment</code> prints out the following INFO message to the logs:</p> <pre><code>Handle new assignment with:\n  New active tasks: [activeTasks]\n  New standby tasks: [standbyTasks]\n  Existing active tasks: [activeTaskIds]\n  Existing standby tasks: [standbyTaskIds]\n</code></pre> <p><code>handleAssignment</code> requests the TopologyMetadata to addSubscribedTopicsFromAssignment (with the <code>TopicPartition</code>s from the given <code>activeTasks</code>).</p> <p><code>handleAssignment</code> rectifies all the existing tasks.</p> <p><code>handleAssignment</code> determines which existing tasks to close (and remove) or recycle and handleCloseAndRecycle them.</p> <p>In the end, <code>handleAssignment</code> requests the Tasks to handle the new assignment and create active and standby tasks.</p> <p><code>handleAssignment</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to onAssignment</li> </ul>"},{"location":"TaskManager/#handlecloseandrecycle","text":"","title":"handleCloseAndRecycle <pre><code>void handleCloseAndRecycle(\n  Set&lt;Task&gt; tasksToRecycle,\n  Set&lt;Task&gt; tasksToCloseClean,\n  Set&lt;Task&gt; tasksToCloseDirty,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; activeTasksToCreate,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; standbyTasksToCreate,\n  LinkedHashMap&lt;TaskId, RuntimeException&gt; taskCloseExceptions)\n</code></pre> <p><code>handleCloseAndRecycle</code>...FIXME</p>"},{"location":"TaskManager/#handling-taskcorruptedexception","text":"","title":"Handling TaskCorruptedException <pre><code>void handleCorruption(\n  Set&lt;TaskId&gt; corruptedTasks)\n</code></pre> <p><code>handleCorruption</code>...FIXME</p> <p><code>handleCorruption</code> is used when:</p> <ul> <li><code>StreamThread</code> is requested to runLoop (and caught a <code>TaskCorruptedException</code>)</li> </ul>"},{"location":"TaskManager/#maybecommitactivetasksperuserrequested","text":"","title":"maybeCommitActiveTasksPerUserRequested <pre><code>int maybeCommitActiveTasksPerUserRequested()\n</code></pre> <p>With rebalance in progress, <code>maybeCommitActiveTasksPerUserRequested</code> returns <code>-1</code> immediately.</p> <p>Otherwise, <code>maybeCommitActiveTasksPerUserRequested</code> finds tasks (among active tasks) with commitRequested or commitNeeded and, if there is at least one, commits them.</p> <p><code>maybeCommitActiveTasksPerUserRequested</code> is used when:</p> <ul> <li><code>StreamThread</code> is requested to maybeCommit</li> </ul>"},{"location":"TaskManager/#commitandfillinconsumedoffsetsandmetadatapertaskmap","text":"","title":"commitAndFillInConsumedOffsetsAndMetadataPerTaskMap <pre><code>int commitAndFillInConsumedOffsetsAndMetadataPerTaskMap(\n  Collection&lt;Task&gt; tasksToCommit,\n  Map&lt;Task, Map&lt;TopicPartition, OffsetAndMetadata&gt;&gt; consumedOffsetsAndMetadataPerTask)\n</code></pre> <p>With rebalance in progress, <code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> returns <code>-1</code> immediately.</p> <p><code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> requests every Task with commitNeeded (in the given <code>tasksToCommit</code> tasks) to prepareCommit (that gives offsets and metadata per partition). <code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> saves the offsets and metadata per partition for a task (that is active) in the given <code>consumedOffsetsAndMetadataPerTask</code>.</p> <p><code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> commitOffsetsOrTransaction (with the given <code>consumedOffsetsAndMetadataPerTask</code> that may have been updated with some active tasks as described above).</p> <p>Once again, <code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> requests every Task with commitNeeded (in the given <code>tasksToCommit</code> tasks) to clearTaskTimeout and postCommit (with <code>enforceCheckpoint</code> flag disabled).</p> <p>In the end, <code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> returns the number of tasks committed.</p> <p><code>commitAndFillInConsumedOffsetsAndMetadataPerTaskMap</code> is used when:</p> <ul> <li><code>TaskManager</code> is requested to commit and handle a TaskCorruptedException</li> </ul>"},{"location":"TaskManager/#partition-rebalancing","text":"","title":"Partition Rebalancing"},{"location":"TaskManager/#rebalanceinprogress-flag","text":"","title":"rebalanceInProgress Flag <p><code>TaskManager</code> uses <code>rebalanceInProgress</code> internal flag to indicate that it is in the middle of partition rebalancing (which is considered not safe to commit and used to skip commitAndFillInConsumedOffsetsAndMetadataPerTaskMap and maybeCommitActiveTasksPerUserRequested).</p> <p>The <code>rebalanceInProgress</code> flag is disabled (<code>false</code>) initially. It is turned on (<code>true</code>) in handleRebalanceStart and off in handleRebalanceComplete.</p>"},{"location":"TaskManager/#isrebalanceinprogress","text":"","title":"isRebalanceInProgress <pre><code>boolean isRebalanceInProgress()\n</code></pre> <p><code>isRebalanceInProgress</code> returns the value of the internal rebalanceInProgress flag.</p> <p><code>isRebalanceInProgress</code>\u00a0is used when:</p> <ul> <li><code>StreamThread</code> is requested to run</li> </ul>"},{"location":"TaskManager/#handlerebalancestart","text":"","title":"handleRebalanceStart <pre><code>void handleRebalanceStart(\n  Set&lt;String&gt; subscribedTopics)\n</code></pre> <p><code>handleRebalanceStart</code> requests the InternalTopologyBuilder to addSubscribedTopicsFromMetadata with the given <code>subscribedTopics</code>.</p> <p><code>handleRebalanceStart</code> tryToLockAllNonEmptyTaskDirectories and turns the rebalanceInProgress internal flag on (<code>true</code>).</p> <p><code>handleRebalanceStart</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to handleRebalanceStart</li> </ul>"},{"location":"TaskManager/#handlerebalancecomplete","text":"","title":"handleRebalanceComplete <pre><code>void handleRebalanceComplete()\n</code></pre> <p><code>handleRebalanceComplete</code> requests the Consumer to pause (suspend) fetching from the partitions that are assigned to this consumer.</p> <p><code>handleRebalanceComplete</code> releaseLockedUnassignedTaskDirectories and turns the rebalanceInProgress internal flag off (<code>false</code>).</p> <p><code>handleRebalanceComplete</code>\u00a0is used when:</p> <ul> <li><code>StreamsPartitionAssignor</code> is requested to onPartitionsAssigned</li> </ul>"},{"location":"TaskManager/#processing-records-with-active-stream-tasks","text":"","title":"Processing Records (with Active Stream Tasks) <pre><code>int process(\n  int maxNumRecords,\n  Time time)\n</code></pre> <p><code>process</code> requests every active StreamTask to process a record until the number of records processed (across all the active tasks) reaches the given <code>maxNumRecords</code> threshold or there are no more records to process.</p> <p><code>process</code> is used when:</p> <ul> <li><code>StreamThread</code> is requested to runOnce (every iteration)</li> </ul>"},{"location":"TaskManager/#trytocompleterestoration","text":"","title":"tryToCompleteRestoration <pre><code>boolean tryToCompleteRestoration(\n  long now,\n  java.util.function.Consumer&lt;Set&lt;TopicPartition&gt;&gt; offsetResetter)\n</code></pre> <p><code>tryToCompleteRestoration</code>...FIXME</p> <p><code>tryToCompleteRestoration</code> is used when:</p> <ul> <li><code>StreamThread</code> is requested to initializeAndRestorePhase</li> </ul>"},{"location":"TaskManager/#adding-records-to-active-streamtasks","text":"","title":"Adding Records to Active StreamTasks <pre><code>void addRecordsToTasks(\n  ConsumerRecords&lt;byte[], byte[]&gt; records)\n</code></pre> <p>For every partition (in the given <code>records</code> registry of <code>ConsumerRecord</code>s), <code>addRecordsToTasks</code> finds the active StreamTask that handles records of this partition and passes the records (for the partition).</p>  <p>Note</p> <p>A single active Task is responsible for a single <code>TopicPartition</code>.</p>  <p><code>addRecordsToTasks</code> is used when:</p> <ul> <li><code>StreamThread</code> is requested to poll for records</li> </ul>"},{"location":"TaskManager/#needsinitializationorrestoration","text":"","title":"needsInitializationOrRestoration <pre><code>boolean needsInitializationOrRestoration()\n</code></pre> <p><code>needsInitializationOrRestoration</code> is <code>true</code> if any of the Tasks require initialization or restoration.</p> <p><code>needsInitializationOrRestoration</code> is used when:</p> <ul> <li><code>StreamThread</code> is requested to initializeAndRestorePhase</li> </ul>"},{"location":"TaskManager/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.kafka.streams.processor.internals.TaskManager</code> logger to see what happens inside.</p> <p>Add the following line to <code>log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.kafka.streams.processor.internals.TaskManager=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"Tasks/","text":"","title":"Tasks"},{"location":"Tasks/#creating-instance","text":"<p><code>Tasks</code> takes the following to be created:</p> <ul> <li> Log Prefix <li> InternalTopologyBuilder <li> StreamsMetricsImpl <li> ActiveTaskCreator <li> StandbyTaskCreator  <p><code>Tasks</code> is created along with TaskManager.</p>","title":"Creating Instance"},{"location":"Tasks/#main-kafka-consumer","text":"","title":"Main Kafka Consumer <p><code>Tasks</code> is given a Kafka <code>Consumer&lt;byte[], byte[]&gt;</code> (using setMainConsumer method) for the following:</p> <ul> <li>createTasks</li> <li>convertStandbyToActive</li> </ul> <p>The idea is to pass this <code>Consumer</code> on to StreamTasks.</p>"},{"location":"Tasks/#setmainconsumer","text":"","title":"setMainConsumer <pre><code>void setMainConsumer(\n  Consumer&lt;byte[], byte[]&gt; mainConsumer)\n</code></pre> <p><code>setMainConsumer</code> sets the Main Kafka Consumer.</p> <p><code>setMainConsumer</code> is used when:</p> <ul> <li><code>TaskManager</code> is requested to setMainConsumer (when <code>StreamThread</code> utility is used to create a StreamThread)</li> </ul>"},{"location":"Tasks/#handlenewassignmentandcreatetasks","text":"","title":"handleNewAssignmentAndCreateTasks <pre><code>void handleNewAssignmentAndCreateTasks(\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; activeTasksToCreate,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; standbyTasksToCreate,\n  Set&lt;TaskId&gt; assignedActiveTasks,\n  Set&lt;TaskId&gt; assignedStandbyTasks)\n</code></pre> <p><code>handleNewAssignmentAndCreateTasks</code> requests the ActiveTaskCreator to removeRevokedUnknownTasks (from the <code>assignedActiveTasks</code>).</p> <p><code>handleNewAssignmentAndCreateTasks</code> requests the StandbyTaskCreator to removeRevokedUnknownTasks (from the <code>assignedStandbyTasks</code>).</p> <p>In the end, <code>handleNewAssignmentAndCreateTasks</code> createTasks (with the <code>activeTasksToCreate</code> and <code>standbyTasksToCreate</code>).</p> <p><code>handleNewAssignmentAndCreateTasks</code> is used when:</p> <ul> <li><code>TaskManager</code> is requested to handle active and standby task assignment</li> </ul>"},{"location":"Tasks/#createtasks","text":"","title":"createTasks <pre><code>void createTasks(\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; activeTasksToCreate,\n  Map&lt;TaskId, Set&lt;TopicPartition&gt;&gt; standbyTasksToCreate)\n</code></pre> <p><code>createTasks</code> requests the ActiveTaskCreator to create active tasks (in the given <code>activeTasksToCreate</code> collection). <code>createTasks</code> registers the tasks in the activeTasksPerId and allTasksPerId registries. <code>createTasks</code> registers the inputPartitions of the tasks in the activeTasksPerPartition.</p> <p><code>createTasks</code> requests the StandbyTaskCreator to create standby tasks (in the given <code>standbyTasksToCreate</code> collection). <code>createTasks</code> registers the tasks in the standbyTasksPerId and allTasksPerId registries.</p> <p><code>createTasks</code>\u00a0is used when:</p> <ul> <li><code>Tasks</code> is requested to handleNewAssignmentAndCreateTasks and maybeCreateTasksFromNewTopologies</li> </ul>"},{"location":"Tasks/#maybecreatetasksfromnewtopologies","text":"","title":"maybeCreateTasksFromNewTopologies <pre><code>void maybeCreateTasksFromNewTopologies()\n</code></pre> <p><code>maybeCreateTasksFromNewTopologies</code> requests the TopologyMetadata for the names of the named topologies.</p> <p>In the end, <code>maybeCreateTasksFromNewTopologies</code> createTasks with the active and standby tasks (and their assigned partitions from the ActiveTaskCreator and StandbyTaskCreator).</p> <p><code>maybeCreateTasksFromNewTopologies</code>\u00a0is used when:</p> <ul> <li><code>TaskManager</code> is requested to handleTopologyUpdates</li> </ul>"},{"location":"Tasks/#convertstandbytoactive","text":"","title":"convertStandbyToActive <pre><code>void convertStandbyToActive(\n  StandbyTask standbyTask,\n  Set&lt;TopicPartition&gt; partitions)\n</code></pre> <p><code>convertStandbyToActive</code>...FIXME</p> <p><code>convertStandbyToActive</code>\u00a0is used when:</p> <ul> <li><code>TaskManager</code> is requested to handleCloseAndRecycle</li> </ul>"},{"location":"Tasks/#activetasksperpartition","text":"","title":"activeTasksPerPartition <pre><code>Map&lt;TopicPartition, Task&gt; activeTasksPerPartition\n</code></pre> <p><code>Tasks</code> defines an <code>activeTasksPerPartition</code> registry of Tasks that handle (records of) a <code>TopicPartition</code>.</p> <p>A new <code>Task</code> can be added in createTasks, convertStandbyToActive, updateInputPartitionsAndResume</p> <p>One or more <code>Task</code>s can be removed in convertActiveToStandby, updateInputPartitionsAndResume, removeTaskBeforeClosing and clear.</p> <p>A <code>Task</code> can be looked up using activeTasksForInputPartition.</p>"},{"location":"Tasks/#activetasksforinputpartition","text":"","title":"activeTasksForInputPartition <pre><code>Task activeTasksForInputPartition(\n  TopicPartition partition)\n</code></pre> <p><code>activeTasksForInputPartition</code> looks up the Task for the given <code>TopicPartition</code> in the activeTasksPerPartition registry.</p> <p><code>activeTasksForInputPartition</code>\u00a0is used when:</p> <ul> <li><code>TaskManager</code> is requested to add records to active tasks</li> </ul>"},{"location":"TopicsInfo/","text":"","title":"TopicsInfo"},{"location":"TopicsInfo/#creating-instance","text":"<p><code>TopicsInfo</code> takes the following to be created:</p> <ul> <li> Names of the Sink Topics <li> Names of the Source Topics <li> Repartition Source Topics (<code>Map&lt;String, InternalTopicConfig&gt;</code>) <li> State Changelog Topics (<code>Map&lt;String, InternalTopicConfig&gt;</code>)  <p><code>TopicsInfo</code> is created\u00a0when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested for topic groups</li> </ul>","title":"Creating Instance"},{"location":"Topology/","text":"<p><code>Topology</code> is a logical representation of a ProcessorTopology.</p> <p><code>Topology</code> is a facade to InternalTopologyBuilder (with all methods delegating to it).</p>","title":"Topology"},{"location":"Topology/#creating-instance","text":"<p><code>Topology</code> takes no arguments to be created.</p> <p><code>Topology</code> is a part of the public API of Kafka Streams and can be created directly or indirectly for StreamsBuilder.</p>","title":"Creating Instance"},{"location":"Topology/#internaltopologybuilder","text":"","title":"InternalTopologyBuilder <p><code>Topology</code> creates an InternalTopologyBuilder when created.</p>"},{"location":"Topology/#addglobalstore","text":"","title":"addGlobalStore <pre><code>&lt;KIn, VIn&gt; Topology addGlobalStore(\n  StoreBuilder&lt;?&gt; storeBuilder,\n  String sourceName,\n  TimestampExtractor timestampExtractor,\n  Deserializer&lt;KIn&gt; keyDeserializer,\n  Deserializer&lt;VIn&gt; valueDeserializer,\n  String topic,\n  String processorName,\n  ProcessorSupplier&lt;KIn, VIn, Void, Void&gt; stateUpdateSupplier)\n</code></pre> <p><code>addGlobalStore</code> requests the InternalTopologyBuilder to add a global store.</p>"},{"location":"Topology/#addprocessor","text":"","title":"addProcessor <pre><code>Topology addProcessor(\n  String name,\n  ProcessorSupplier&lt;KIn, VIn, KOut, VOut&gt; supplier,\n  String... parentNames)\n</code></pre> <p><code>addProcessor</code> requests the InternalTopologyBuilder to add a processor.</p> <p>If there are any state stores associated with the processor, <code>addProcessor</code> requests the InternalTopologyBuilder to add them.</p>"},{"location":"Topology/#addsource","text":"","title":"addSource <pre><code>Topology addSource(...) // (1)\n</code></pre> <ol> <li>There are over 10 different <code>addSource</code>s</li> </ol> <p><code>addSource</code> requests the InternalTopologyBuilder to add a new source (node) (with the given arguments).</p>"},{"location":"Topology/#demo","text":"","title":"Demo <pre><code>import org.apache.kafka.streams.Topology\nval topology = new Topology\n</code></pre>"},{"location":"TopologyMetadata/","text":"<p><code>TopologyMetadata</code> is...FIXME</p>","title":"TopologyMetadata"},{"location":"TopologyTestDriver/","text":"<p><code>TopologyTestDriver</code> helps writing tests to verify behavior of topologies (created with Topology or StreamsBuilder).</p> <pre><code>import org.apache.kafka.streams.TopologyTestDriver\n</code></pre>","title":"TopologyTestDriver"},{"location":"TopologyTestDriver/#library-dependency","text":"","title":"Library Dependency <p><code>TopologyTestDriver</code> belongs to a separate module and has to be defined as a dependency in a build configuration (e.g. <code>build.sbt</code>).</p> <pre><code>val kafkaVersion = \"3.1.0\"\nlibraryDependencies += \"org.apache.kafka\" % \"kafka-streams-test-utils\" % kafkaVersion % Test\n</code></pre>"},{"location":"global-stores/","text":"<p>StreamsBuilder.addGlobalStore adds a global StateStore to a topology.</p> <p>Such a <code>StateStore</code> sources its data from all partitions of the provided input topic. This store uses the source topic as changelog (and during restore will insert records directly from the source).</p> <p>Global stores should not be added to <code>Processor</code>, <code>Transformer</code>, or <code>ValueTransformer</code> (unlike regular stores). They have read-only access to all global stores by default.</p> <p>There will be exactly one instance of this <code>StateStore</code> per Kafka Streams instance.</p> <p>A SourceNode will be added to consume the data arriving from the partitions of the input topic.</p>","title":"Global Stores"},{"location":"logging/","text":"<p>Kafka Streams uses Simple Logging Facade for Java (SLF4J) for logging.</p> <p>Among the logging frameworks supported by slf4j is Apache Log4j that is used by Apache Kafka by default.</p>","title":"Logging"},{"location":"logging/#library-dependencies","text":"<p>Use <code>slf4j-api</code> and <code>slf4j-log4j12</code> library dependencies in a Kafka Streams application (in <code>build.sbt</code>) for logging.</p> <pre><code>val slf4jVersion = \"2.0.0-alpha5\"\nlibraryDependencies += \"org.slf4j\" % \"slf4j-api\" % slf4jVersion\nlibraryDependencies += \"org.slf4j\" % \"slf4j-log4j12\" % slf4jVersion\n</code></pre>","title":"Library Dependencies"},{"location":"logging/#log4jproperties","text":"","title":"log4j.properties <p>Use the following <code>log4j.properties</code> in <code>src/main/resources</code> in your Kafka Streams application's project.</p> <pre><code>log4j.rootLogger=INFO, stdout\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.target=System.out\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.logger.org.apache.kafka.streams.processor.internals.StreamThread=ALL\n</code></pre>"},{"location":"overview/","text":"<p>Kafka Streams is a library for developing applications for processing records from topics in Apache Kafka.</p> <p>Kafka Streams comes with high-level Streams DSL and low-level Processor API to describe a Topology that eventually is built as a ProcessorTopology.</p> <p>The execution environment of <code>ProcessorTopology</code> is KafkaStreams. Once created, the <code>KafkaStreams</code> instance is supposed to be started to start stream processing.</p> <p>On partition assignment, Kafka Streams validates that the number of active tasks is the same as the number of assigned partitions (or a TaskAssignmentException is thrown).</p>","title":"Kafka Streams\u2009\u2014\u2009Stream Processing Library on Apache Kafka"},{"location":"scala/","text":"<p>Scala API for Kafka Streams is a separate Kafka Streams module (a Scala library) that acts as a wrapper over the existing Java API for Kafka Streams.</p> <p>The Scala API for Kafka Streams defines Scala-friendly types that wrap the corresponding Kafka Streams types and simply delegate all method calls to the underlying Java object with the purpose of making it much more expressive, with less boilerplate and more succinct.</p>","title":"Scala API for Kafka Streams"},{"location":"scala/#scala-package","text":"","title":"scala Package <p>The Scala API is available in the <code>org.apache.kafka.streams.scala</code> package.</p> <pre><code>import org.apache.kafka.streams.scala._\nimport org.apache.kafka.streams.scala.kstream._\n</code></pre>"},{"location":"scala/#library-dependency","text":"","title":"Library Dependency <p>As a separate Scala library Scala API for Kafka Streams has to be defined as a dependency in a build configuration (e.g. <code>build.sbt</code>).</p> <pre><code>val kafkaVersion = \"3.1.0\"\nlibraryDependencies += \"org.apache.kafka\" %% \"kafka-streams-scala\" % kafkaVersion\n</code></pre>"},{"location":"scala/#implicit-conversions","text":"","title":"Implicit Conversions <p>The Scala API for Kafka Streams defines implicit conversions, i.e. <code>Serdes</code>, and <code>ImplicitConversions</code>.</p> <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n</code></pre>"},{"location":"scala/#consumed","text":"","title":"Consumed <p>The Scala API for Kafka Streams comes with Scala objects for creating Consumed, Produced, <code>Materialized</code> and other metadata-related instances with Serdes objects for the key and value types available in implicit scope.</p> <pre><code>import org.apache.kafka.streams.scala.kstream._\n</code></pre>"},{"location":"demo/co-partitioning/","text":"<p>The demo shows what happens when the topics to be joined are not co-partitioned.</p>","title":"Demo: Co-partitioning"},{"location":"demo/co-partitioning/#create-kafka-topics","text":"<p>Kafka Streams requires that all input topics are available before it can be started (or <code>MissingSourceTopicException</code> is thrown).</p> <p>Please note that the topics have different number of partitions.</p> <pre><code>./bin/kafka-topics.sh \\\n  --bootstrap-server :9092 \\\n  --create \\\n  --topic demo-left \\\n  --partitions 1 \\\n  --replication-factor 1\n</code></pre> <pre><code>./bin/kafka-topics.sh \\\n  --bootstrap-server :9092 \\\n  --create \\\n  --topic demo-right \\\n  --partitions 3 \\\n  --replication-factor 1\n</code></pre> <p>Make sure the topics are available.</p> <pre><code>./bin/kafka-topics.sh \\\n  --bootstrap-server :9092 \\\n  --list\n</code></pre>","title":"Create Kafka Topics"},{"location":"demo/co-partitioning/#stream-join","text":"<pre><code>import org.apache.kafka.streams.scala._\nimport org.apache.kafka.streams.scala.kstream._\n\nimport org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nimport org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n\ntype VO = String\ntype VR = String\n\nval left = builder.stream[String, String](topic = \"demo-left\")\nval right = builder.stream[String, VO](topic = \"demo-right\")\n\nval joiner: (String, VO) =&gt; VR = (leftValue, rightValue) =&gt; s\"$leftValue -&gt; $rightValue\"\n\nimport org.apache.kafka.streams.kstream.{JoinWindows, Printed}\nimport java.time.Duration\n\nval windows = JoinWindows.of(Duration.ofSeconds(10))\nval leftRightJoined = left.join(otherStream = right)(joiner, windows)\nleftRightJoined.print(Printed.toSysOut)\n\nimport org.apache.kafka.streams.Topology\nval topology = builder.build()\n</code></pre>","title":"Stream Join"},{"location":"demo/co-partitioning/#review-topology","text":"<pre><code>println(topology.describe)\n</code></pre> <pre><code>Topologies:\n   Sub-topology: 0\n    Source: KSTREAM-SOURCE-0000000000 (topics: [demo-left])\n      --&gt; KSTREAM-WINDOWED-0000000002\n    Source: KSTREAM-SOURCE-0000000001 (topics: [demo-right])\n      --&gt; KSTREAM-WINDOWED-0000000003\n    Processor: KSTREAM-WINDOWED-0000000002 (stores: [KSTREAM-JOINTHIS-0000000004-store])\n      --&gt; KSTREAM-JOINTHIS-0000000004\n      &lt;-- KSTREAM-SOURCE-0000000000\n    Processor: KSTREAM-WINDOWED-0000000003 (stores: [KSTREAM-JOINOTHER-0000000005-store])\n      --&gt; KSTREAM-JOINOTHER-0000000005\n      &lt;-- KSTREAM-SOURCE-0000000001\n    Processor: KSTREAM-JOINOTHER-0000000005 (stores: [KSTREAM-JOINTHIS-0000000004-store])\n      --&gt; KSTREAM-MERGE-0000000006\n      &lt;-- KSTREAM-WINDOWED-0000000003\n    Processor: KSTREAM-JOINTHIS-0000000004 (stores: [KSTREAM-JOINOTHER-0000000005-store])\n      --&gt; KSTREAM-MERGE-0000000006\n      &lt;-- KSTREAM-WINDOWED-0000000002\n    Processor: KSTREAM-MERGE-0000000006 (stores: [])\n      --&gt; KSTREAM-PRINTER-0000000007\n      &lt;-- KSTREAM-JOINTHIS-0000000004, KSTREAM-JOINOTHER-0000000005\n    Processor: KSTREAM-PRINTER-0000000007 (stores: [])\n      --&gt; none\n      &lt;-- KSTREAM-MERGE-0000000006\n</code></pre>","title":"Review Topology"},{"location":"demo/co-partitioning/#kafkastreams","text":"","title":"KafkaStreams <pre><code>import org.apache.kafka.streams.StreamsConfig\nimport scala.jdk.CollectionConverters._\n// Only required configuration properties\n// And one more for demo purposes to slow processing to 15 secs\nimport scala.concurrent.duration._\nval props = Map(\n  StreamsConfig.APPLICATION_ID_CONFIG -&gt; \"demo-join\",\n  StreamsConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; \":9092\",\n  StreamsConfig.POLL_MS_CONFIG -&gt; 15.seconds.toMillis).asJava\nval config = new StreamsConfig(props)\n\nimport org.apache.kafka.streams.KafkaStreams\nval streams = new KafkaStreams(topology, config)\n\nstreams.start\n</code></pre>"},{"location":"demo/co-partitioning/#topologyexception-topics-not-co-partitioned","text":"","title":"TopologyException: Topics not co-partitioned <pre><code>org.apache.kafka.streams.errors.TopologyException: Invalid topology: stream-thread [demo-join-6e1e5363-2e22-4cd6-809b-c73d06d405ac-StreamThread-1-consumer] Topics not co-partitioned: [{demo-left=1, demo-right=3}]\n    at org.apache.kafka.streams.processor.internals.assignment.CopartitionedTopicsEnforcer.getSamePartitions(CopartitionedTopicsEnforcer.java:161)\n    at org.apache.kafka.streams.processor.internals.assignment.CopartitionedTopicsEnforcer.enforce(CopartitionedTopicsEnforcer.java:92)\n    at org.apache.kafka.streams.processor.internals.RepartitionTopics.ensureCopartitioning(RepartitionTopics.java:112)\n    at org.apache.kafka.streams.processor.internals.RepartitionTopics.setup(RepartitionTopics.java:69)\n    at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.prepareRepartitionTopics(StreamsPartitionAssignor.java:485)\n    at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assign(StreamsPartitionAssignor.java:365)\n    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:589)\n    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:690)\n    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1000(AbstractCoordinator.java:112)\n    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:594)\n    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:557)\n    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1184)\n    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1159)\n    at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:206)\n    at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)\n    at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)\n    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:602)\n    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:412)\n    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:297)\n    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)\n    at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1297)\n    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1238)\n    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)\n    at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:932)\n    at org.apache.kafka.streams.processor.internals.StreamThread.pollPhase(StreamThread.java:885)\n    at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:720)\n    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:583)\n    at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:555)\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/","text":"","title":"Demo: Developing Kafka Streams Application"},{"location":"demo/developing-kafka-streams-application/#build-topology-using-streamsbuilder","text":"","title":"Build Topology using StreamsBuilder <p>A Kafka Streams application requires a Topology that can be created directly or described (and built) indirectly using StreamsBuilder.</p> <pre><code>import org.apache.kafka.streams.scala.StreamsBuilder\nval streamBuilder = new StreamsBuilder\n</code></pre> <pre><code>import org.apache.kafka.streams.scala.ImplicitConversions._\nimport org.apache.kafka.streams.scala.serialization.Serdes._\n</code></pre> <pre><code>val records = streamBuilder.stream[String, String](topic = \"streams-demo-input\")\nrecords.to(topic = \"streams-demo-output\")\n</code></pre> <pre><code>import org.apache.kafka.streams.Topology\nval topology = streamBuilder.build()\n</code></pre> <p>A topology can be described.</p> <pre><code>println(topology.describe)\n</code></pre> <pre><code>Topologies:\n   Sub-topology: 0\n    Source: KSTREAM-SOURCE-0000000000 (topics: [streams-demo-input])\n      --&gt; KSTREAM-SINK-0000000001\n    Sink: KSTREAM-SINK-0000000001 (topic: streams-demo-output)\n      &lt;-- KSTREAM-SOURCE-0000000000\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#create-kafka-topics","text":"","title":"Create Kafka Topics <p>Kafka Streams requires that all input topics are available before it can be started (or <code>MissingSourceTopicException</code> is thrown).</p> <pre><code>./bin/kafka-topics.sh \\\n  --bootstrap-server :9092 \\\n  --create \\\n  --topic streams-demo-input \\\n  --partitions 1 \\\n  --replication-factor 1\n</code></pre> <pre><code>./bin/kafka-topics.sh \\\n  --bootstrap-server :9092 \\\n  --create \\\n  --topic streams-demo-output \\\n  --partitions 1 \\\n  --replication-factor 1\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#streamsconfig","text":"","title":"StreamsConfig <p>An execution environment of a Kafka Streams application is configured using StreamsConfig.</p> <pre><code>import org.apache.kafka.streams.StreamsConfig\nimport scala.jdk.CollectionConverters._\n// Only required configuration properties\n// And one more for demo purposes to slow processing to 15 secs\n// import java.util.concurrent.TimeUnit\nimport scala.concurrent.duration._\nval props = Map(\n  StreamsConfig.APPLICATION_ID_CONFIG -&gt; \"kafka-streams-demo\",\n  StreamsConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; \":9092\",\n  StreamsConfig.POLL_MS_CONFIG -&gt; 15.seconds.toMillis).asJava\nval config = new StreamsConfig(props)\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#kafkastreams","text":"","title":"KafkaStreams <p>The execution environment of a Kafka Stream application is KafkaStreams.</p> <pre><code>import org.apache.kafka.streams.KafkaStreams\nval streams = new KafkaStreams(topology, config)\n</code></pre> <p>Eventually, <code>KafkaStreams</code> should be started for the stream processing to be executed.</p> <pre><code>streams.start\n</code></pre>"},{"location":"demo/developing-kafka-streams-application/#kcat","text":"","title":"kcat <pre><code>kcat -P -b localhost -t streams-demo-input\n</code></pre> <pre><code>kcat -C -b localhost -t streams-demo-output\n</code></pre>"},{"location":"kstream/","text":"<p>Streams DSL (KStream DSL) is a high-level API for developers to define streaming topologies in Kafka Streams.</p> <p>The entry point to the KStream DSL is StreamsBuilder.</p> <p>Main abstractions (for Kafka Streams developers):</p> <ul> <li>Consumed</li> <li>GlobalKTable</li> <li>KStream</li> <li>KTable</li> <li>Materialized</li> <li>Produced</li> <li>others</li> </ul> <p>A typical Kafka Streams application (that uses this Streams DSL and Scala API for Kafka Streams) looks as follows:</p> <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nval builder = new StreamsBuilder\n\n// Add a KStream if needed\n// K and V are the types of keys and values, accordingly\nbuilder.stream[K, V](...)\n\n// Add a KTable if needed\nbuilder.table[K, V](...)\n\n// Add a global store if needed\nbuilder.addGlobalStore(...)\n\n// Add a global store if needed\nbuilder.globalTable[K, V](...)\n\n// In the end, build a topology\nval topology = builder.build\n</code></pre>","title":"Streams DSL"},{"location":"kstream/AbstractStream/","text":"<p><code>AbstractStream&lt;K, V&gt;</code> is a base abstraction of KTables and KStreams.</p>","title":"AbstractStream"},{"location":"kstream/AbstractStream/#implementations","text":"<ul> <li><code>CogroupedKStreamImpl</code></li> <li>KGroupedStreamImpl</li> <li><code>KGroupedTableImpl</code></li> <li>KStreamImpl</li> <li>KTableImpl</li> <li><code>SessionWindowedCogroupedKStreamImpl</code></li> <li><code>SessionWindowedKStreamImpl</code></li> <li><code>SlidingWindowedCogroupedKStreamImpl</code></li> <li><code>SlidingWindowedKStreamImpl</code></li> <li><code>TimeWindowedCogroupedKStreamImpl</code></li> <li><code>TimeWindowedKStreamImpl</code></li> </ul>","title":"Implementations"},{"location":"kstream/AbstractStream/#creating-instance","text":"<p><code>AbstractStream</code> takes the following to be created:</p> <ul> <li> Name <li> <code>Serde&lt;K&gt;</code> <li> <code>Serde&lt;V&gt;</code> <li> Names of the Sub-Topology Source Nodes <li> GraphNode <li> InternalStreamsBuilder   <p>Abstract Class</p> <p><code>AbstractStream</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete AbstractStreams.</p>","title":"Creating Instance"},{"location":"kstream/AbstractStream/#ensurecopartitionwith","text":"","title":"ensureCopartitionWith <pre><code>Set&lt;String&gt; ensureCopartitionWith(\n  Collection&lt;? extends AbstractStream&lt;K, ?&gt;&gt; otherStreams)\n</code></pre> <p><code>ensureCopartitionWith</code> requests the InternalStreamsBuilder for the InternalTopologyBuilder that is in turn requested to copartitionSources (with the subTopologySourceNodes and the <code>subTopologySourceNodes</code> of all the other <code>AbstractStream</code>s).</p> <p><code>ensureCopartitionWith</code> returns the subTopologySourceNodes and the <code>subTopologySourceNodes</code> of all the other <code>AbstractStream</code>s.</p> <p><code>ensureCopartitionWith</code>\u00a0is used when:</p> <ul> <li><code>CogroupedStreamAggregateBuilder</code> is requested to <code>processRepartitions</code></li> <li><code>KStreamImpl</code> is requested to doJoin and doStreamTableJoin</li> <li><code>KTableImpl</code> is requested to doJoin</li> </ul>"},{"location":"kstream/Aggregator/","text":"<p><code>Aggregator</code> is...FIXME</p>","title":"Aggregator"},{"location":"kstream/BaseRepartitionNode/","text":"<p><code>BaseRepartitionNode&lt;K, V&gt;</code>\u00a0is an extension of the GraphNode abstraction for repartition nodes.</p>","title":"BaseRepartitionNode"},{"location":"kstream/BaseRepartitionNode/#implementations","text":"<ul> <li>GroupedTableOperationRepartitionNode</li> <li>OptimizableRepartitionNode</li> <li>UnoptimizableRepartitionNode</li> </ul>","title":"Implementations"},{"location":"kstream/BaseRepartitionNode/#creating-instance","text":"<p><code>BaseRepartitionNode</code> takes the following to be created:</p> <ul> <li> Node Name <li> Source Name <li> <code>ProcessorParameters</code> <li> Key <code>Serde</code> (Apache Kafka) <li> Value <code>Serde</code> (Apache Kafka) <li> Sink Name <li> Repartition Topic <li> StreamPartitioner <li> <code>InternalTopicProperties</code>   <p>Abstract Class</p> <p><code>BaseRepartitionNode</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete BaseRepartitionNodes.</p>","title":"Creating Instance"},{"location":"kstream/Consumed/","text":"<p><code>Consumed&lt;K, V&gt;</code> describes how to consume records in a topology in the High-Level KStream DSL for the following StreamsBuilder operators:</p> <ul> <li>StreamsBuilder.stream</li> <li>StreamsBuilder.table</li> <li>StreamsBuilder.globalTable</li> <li>StreamsBuilder.addGlobalStore</li> </ul> <p><code>Consumed&lt;K, V&gt;</code> is a NamedOperation.</p>","title":"Consumed \u2014 Metadata for Consuming Records"},{"location":"kstream/Consumed/#demo","text":"<pre><code>import org.apache.kafka.common.serialization.Serdes\nimport org.apache.kafka.streams.kstream.Consumed\nval consumed = Consumed.`with`(Serdes.Long, Serdes.String)\n</code></pre> <pre><code>scala&gt; :type consumed\norg.apache.kafka.streams.kstream.Consumed[Long,String]\n</code></pre>","title":"Demo"},{"location":"kstream/Consumed/#creating-instance","text":"<p><code>Consumed</code> takes the following to be created:</p> <ul> <li> <code>Serde&lt;K&gt;</code> of keys (Apache Kafka) <li> <code>Serde&lt;V&gt;</code> of values (Apache Kafka) <li> <code>TimestampExtractor</code> <li> Reset Policy (<code>Topology.AutoOffsetReset</code>) <li> Processor Name  <p><code>Consumed</code> is created\u00a0using the factories.</p>","title":"Creating Instance"},{"location":"kstream/Consumed/#creating-consumed","text":"","title":"Creating Consumed"},{"location":"kstream/Consumed/#as","text":"","title":"as <pre><code>Consumed&lt;K, V&gt; as(\n  String processorName)\n</code></pre>"},{"location":"kstream/Consumed/#with","text":"","title":"with <pre><code>Consumed&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde)\nConsumed&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde,\n  TimestampExtractor timestampExtractor,\n  Topology.AutoOffsetReset resetPolicy)\nConsumed&lt;K, V&gt; with(\n  TimestampExtractor timestampExtractor)\nConsumed&lt;K, V&gt; with(\n  Topology.AutoOffsetReset resetPolicy)\n</code></pre>"},{"location":"kstream/Consumed/#scala-api","text":"","title":"Scala API <p>Scala API for Kafka Streams makes the optional <code>Consumed</code> metadata an implicit parameter in the StreamsBuilder API.</p> <p>Moreover, <code>ImplicitConversions</code> object defines <code>consumedFromSerde</code> implicit method that creates a <code>Consumed</code> instance with the key and value <code>Serde</code> objects available in implicit scope.</p> <p>And the last but not least, Scala API for Kafka Streams defines <code>Consumed</code> object with <code>with</code> factory methods that use implicit key and value <code>Serde</code> objects.</p>"},{"location":"kstream/GlobalKTable/","text":"<p><code>GlobalKTable</code> is...FIXME</p>","title":"GlobalKTable"},{"location":"kstream/GraphNode/","text":"<p><code>GraphNode</code> is an abstraction of graph nodes (for InternalStreamsBuilder to build a topology for StreamsBuilder).</p>","title":"GraphNode"},{"location":"kstream/GraphNode/#contract","text":"","title":"Contract"},{"location":"kstream/GraphNode/#writetotopology","text":"","title":"writeToTopology <pre><code>void writeToTopology(\n  InternalTopologyBuilder topologyBuilder,\n  Properties props)\n</code></pre> <p>Used when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to build and optimize a topology</li> </ul>"},{"location":"kstream/GraphNode/#implementations","text":"<ul> <li>BaseJoinProcessorNode</li> <li>BaseRepartitionNode</li> <li>ProcessorGraphNode</li> <li>SourceGraphNode</li> <li>StateStoreNode</li> <li>StreamSinkNode</li> <li>StreamTableJoinNode</li> <li>StreamToTableNode</li> <li>TableProcessorNode</li> </ul>","title":"Implementations"},{"location":"kstream/GraphNode/#creating-instance","text":"<p><code>GraphNode</code> takes the following to be created:</p> <ul> <li> Node Name   <p>Abstract Class</p> <p><code>GraphNode</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete GraphNodes.</p>","title":"Creating Instance"},{"location":"kstream/GraphNode/#mergenode","text":"","title":"mergeNode"},{"location":"kstream/GraphNode/#ismergenode","text":"","title":"isMergeNode <pre><code>boolean isMergeNode()\n</code></pre> <p><code>isMergeNode</code> returns the mergeNode flag.</p> <p><code>isMergeNode</code> is used when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to maybeAddNodeForOptimizationMetadata</li> </ul>"},{"location":"kstream/GraphNode/#setmergenode","text":"","title":"setMergeNode <pre><code>void setMergeNode(\n  boolean mergeNode)\n</code></pre> <p><code>setMergeNode</code> sets the mergeNode flag to the given <code>mergeNode</code>.</p> <p><code>setMergeNode</code> is used when:</p> <ul> <li><code>KStreamImpl</code> is requested to merge</li> </ul>"},{"location":"kstream/GraphNode/#demo","text":"","title":"Demo <pre><code>import org.apache.kafka.streams.StreamsBuilder\n\n// Without this class the following would not be available\n// due to `protected` access level\nclass MyStreamsBuilder extends StreamsBuilder {\n  val root = internalStreamsBuilder.root\n}\n\nval builder = new MyStreamsBuilder\nbuilder.root\n</code></pre> <pre><code>scala&gt; println(builder.root)\nStreamsGraphNode{nodeName='root', buildPriority=null, hasWrittenToTopology=false, keyChangingOperation=false, valueChangingOperation=false, mergeNode=false, parentNodes=[]}\n</code></pre>"},{"location":"kstream/GroupedStreamAggregateBuilder/","text":"<p><code>GroupedStreamAggregateBuilder</code> is...FIXME</p>","title":"GroupedStreamAggregateBuilder"},{"location":"kstream/GroupedTableOperationRepartitionNode/","text":"<p><code>GroupedTableOperationRepartitionNode</code> is...FIXME</p>","title":"GroupedTableOperationRepartitionNode"},{"location":"kstream/Initializer/","text":"<p><code>Initializer</code> is...FIXME</p>","title":"Initializer"},{"location":"kstream/InternalStreamsBuilder/","text":"","title":"InternalStreamsBuilder"},{"location":"kstream/InternalStreamsBuilder/#creating-instance","text":"<p><code>InternalStreamsBuilder</code> takes the following to be created:</p> <ul> <li> InternalTopologyBuilder  <p><code>InternalStreamsBuilder</code> is created\u00a0when:</p> <ul> <li><code>StreamsBuilder</code> is created</li> </ul>","title":"Creating Instance"},{"location":"kstream/InternalStreamsBuilder/#root-node","text":"","title":"Root Node <pre><code>GraphNode root\n</code></pre> <p><code>InternalStreamsBuilder</code> creates a root GraphNode when created.</p> <p>This root node is used to addGraphNode in the following high-level operators:</p> <ul> <li>stream</li> <li>table</li> <li>globalTable</li> <li>addStateStore</li> <li>addGlobalStore</li> </ul> <p>This root node is then used to build and optimize a topology (for StreamsBuilder).</p>"},{"location":"kstream/InternalStreamsBuilder/#buildandoptimizetopology","text":"","title":"buildAndOptimizeTopology <pre><code>void buildAndOptimizeTopology() // (1)\nvoid buildAndOptimizeTopology(\n  Properties props)\n</code></pre> <ol> <li>Used in tests only</li> </ol> <p><code>buildAndOptimizeTopology</code>...FIXME</p> <p><code>buildAndOptimizeTopology</code>\u00a0is used when:</p> <ul> <li><code>StreamsBuilder</code> is requested to build a topology</li> </ul>"},{"location":"kstream/InternalStreamsBuilder/#mergeduplicatesourcenodes","text":"","title":"mergeDuplicateSourceNodes <pre><code>void mergeDuplicateSourceNodes()\n</code></pre> <p><code>mergeDuplicateSourceNodes</code>...FIXME</p>"},{"location":"kstream/InternalStreamsBuilder/#maybeperformoptimizations","text":"","title":"maybePerformOptimizations <pre><code>void maybePerformOptimizations(\n  Properties props)\n</code></pre> <p><code>maybePerformOptimizations</code>...FIXME</p>"},{"location":"kstream/InternalStreamsBuilder/#optimizektablesourcetopics","text":"","title":"optimizeKTableSourceTopics <pre><code>void optimizeKTableSourceTopics()\n</code></pre> <p><code>optimizeKTableSourceTopics</code>...FIXME</p>"},{"location":"kstream/InternalStreamsBuilder/#maybeoptimizerepartitionoperations","text":"","title":"maybeOptimizeRepartitionOperations <pre><code>void maybeOptimizeRepartitionOperations()\n</code></pre> <p><code>maybeOptimizeRepartitionOperations</code>...FIXME</p>"},{"location":"kstream/InternalStreamsBuilder/#maybeupdatekeychangingrepartitionnodemap","text":"","title":"maybeUpdateKeyChangingRepartitionNodeMap <pre><code>void maybeUpdateKeyChangingRepartitionNodeMap()\n</code></pre> <p><code>maybeUpdateKeyChangingRepartitionNodeMap</code>...FIXME</p>"},{"location":"kstream/InternalStreamsBuilder/#createrepartitionnode","text":"","title":"createRepartitionNode <pre><code>OptimizableRepartitionNode&lt;K, V&gt; createRepartitionNode(\n  String repartitionTopicName,\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde)\n</code></pre> <p><code>createRepartitionNode</code>...FIXME</p>"},{"location":"kstream/InternalStreamsBuilder/#adding-statestore-to-topology","text":"","title":"Adding StateStore to Topology <pre><code>void addStateStore(\n  StoreBuilder&lt;?&gt; builder)\n</code></pre> <p><code>addStateStore</code> adds a new StateStoreNode to the root node.</p> <p><code>addStateStore</code> is used when:</p> <ul> <li><code>StreamsBuilder</code> is requested to addStateStore</li> <li><code>KTableImpl</code> is requested to <code>doJoinOnForeignKey</code></li> </ul>"},{"location":"kstream/InternalStreamsBuilder/#stream","text":"","title":"stream <pre><code>KStream&lt;K, V&gt; stream(\n  Collection&lt;String&gt; topics,\n  ConsumedInternal&lt;K, V&gt; consumed)\nKStream&lt;K, V&gt; stream(\n  Pattern topicPattern,\n  ConsumedInternal&lt;K, V&gt; consumed)\n</code></pre> <p><code>stream</code>...FIXME</p> <p><code>stream</code>\u00a0is used when:</p> <ul> <li><code>StreamsBuilder</code> is requested to stream</li> </ul>"},{"location":"kstream/InternalStreamsBuilder/#adding-graphnode","text":"","title":"Adding GraphNode <pre><code>void addGraphNode(\n  Collection&lt;GraphNode&gt; parents,\n  GraphNode child)\n</code></pre> <p><code>addGraphNode</code> adds the given child GraphNode to the children of the given parent <code>GraphNode</code>s.</p> <p>In the end, <code>addGraphNode</code> maybeAddNodeForOptimizationMetadata (with the <code>child</code> node).</p>"},{"location":"kstream/InternalStreamsBuilder/#maybeaddnodeforoptimizationmetadata","text":"","title":"maybeAddNodeForOptimizationMetadata <pre><code>void maybeAddNodeForOptimizationMetadata(\n  GraphNode node)\n</code></pre> <p><code>maybeAddNodeForOptimizationMetadata</code> setBuildPriority of the given GraphNode to the current buildPriorityIndex (and increments it).</p> <p><code>maybeAddNodeForOptimizationMetadata</code> adds the given <code>GraphNode</code> to the following internal registries:</p> <ul> <li>keyChangingOperationsToOptimizableRepartitionNodes when isKeyChangingOperation</li> <li><code>OptimizableRepartitionNode</code> (FIXME)</li> <li>mergeNodes when isMergeNode</li> <li>tableSourceNodes when <code>TableSourceNode</code></li> </ul>"},{"location":"kstream/InternalStreamsBuilder/#merge-graphnodes","text":"","title":"Merge GraphNodes <p><code>InternalStreamsBuilder</code> defines <code>mergeNodes</code> internal registry of GraphNodes that are merge nodes (that are found in maybeAddNodeForOptimizationMetadata while adding a new GraphNode).</p> <p><code>mergeNodes</code> is used for maybeUpdateKeyChangingRepartitionNodeMap.</p>"},{"location":"kstream/KGroupedStream/","text":"<p><code>KGroupedStream</code> is...FIXME</p>","title":"KGroupedStream"},{"location":"kstream/KGroupedStreamImpl/","text":"<p><code>KGroupedStreamImpl</code> is a KGroupedStream (and an AbstractStream).</p>","title":"KGroupedStreamImpl"},{"location":"kstream/KGroupedStreamImpl/#creating-instance","text":"<p><code>KGroupedStreamImpl</code> takes the following to be created:</p> <ul> <li> Name <li> Sub-Topology Source Nodes (Names) <li> <code>GroupedInternal&lt;K, V&gt;</code> <li>repartitionRequired flag</li> <li> GraphNode <li> InternalStreamsBuilder  <p><code>KGroupedStreamImpl</code> is created\u00a0when:</p> <ul> <li><code>KStreamImpl</code> is requested to groupBy and groupByKey</li> </ul>","title":"Creating Instance"},{"location":"kstream/KGroupedStreamImpl/#groupedstreamaggregatebuilder","text":"","title":"GroupedStreamAggregateBuilder <p><code>KGroupedStreamImpl</code> creates a GroupedStreamAggregateBuilder when created.</p>"},{"location":"kstream/KGroupedStreamImpl/#repartitionrequired-flag","text":"","title":"repartitionRequired Flag <p><code>KGroupedStreamImpl</code> is given a <code>repartitionRequired</code> flag when created.</p> <p>The <code>repartitionRequired</code> flag is always <code>true</code> for groupBy.</p>"},{"location":"kstream/KStream/","text":"<p><code>KStream&lt;K, V&gt;</code> is an abstraction of a stream of records (of key-value pairs).</p> <p><code>KStream</code> can be created directly from one or many Kafka topics (using StreamsBuilder.stream operators) or as a result of transformations on an existing <code>KStream</code> instance.</p> <p><code>KStream</code> offers a rich set of operators (KStream API) for building topologies to consume, process and produce (key-value) records.</p>","title":"KStream API \u2014 Stream of Records"},{"location":"kstream/KStream/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"kstream/KStream/#flatmap","text":"","title":"flatMap <pre><code>KStream&lt;KR, VR&gt; flatMap(\n  KeyValueMapper&lt;\n    ? super K,\n    ? super V,\n    ? extends Iterable&lt;? extends KeyValue&lt;? extends KR, ? extends VR&gt;&gt;&gt; mapper)\nKStream&lt;KR, VR&gt; flatMap(\n  KeyValueMapper&lt;\n    ? super K,\n    ? super V,\n    ? extends Iterable&lt;? extends KeyValue&lt;? extends KR, ? extends VR&gt;&gt;&gt; mapper,\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#foreach","text":"","title":"foreach <pre><code>void foreach(\n  ForeachAction&lt;? super K, ? super V&gt; action)\nvoid foreach(\n  ForeachAction&lt;? super K, ? super V&gt; action,\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#groupby","text":"","title":"groupBy <pre><code>KGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector)\nKGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector,\n  Grouped&lt;KR, V&gt; grouped)\n</code></pre>"},{"location":"kstream/KStream/#groupbykey","text":"","title":"groupByKey <pre><code>KGroupedStream&lt;K, V&gt; groupByKey()\nKGroupedStream&lt;K, V&gt; groupByKey(\n  Grouped&lt;K, V&gt; grouped)\n</code></pre>"},{"location":"kstream/KStream/#join","text":"","title":"join <pre><code>KStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoiner&lt;? super V, ? super GV, ? extends RV&gt; joiner)\nKStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoiner&lt;? super V, ? super GV, ? extends RV&gt; joiner,\n  Named named)\nKStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super GV, ? extends RV&gt; joiner)\nKStream&lt;K, RV&gt; join(\n  GlobalKTable&lt;GK, GV&gt; globalTable,\n  KeyValueMapper&lt;? super K, ? super V, ? extends GK&gt; keySelector,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super GV, ? extends RV&gt; joiner,\n  Named named)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoiner&lt;? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoiner&lt;? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K, V, VO&gt; streamJoined)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K, V, VO&gt; streamJoined)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoiner&lt;? super V, ? super VT, ? extends VR&gt; joiner)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoiner&lt;? super V, ? super VT, ? extends VR&gt; joiner,\n  Joined&lt;K, V, VT&gt; joined)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VT, ? extends VR&gt; joiner)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VT&gt; table,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VT, ? extends VR&gt; joiner,\n  Joined&lt;K, V, VT&gt; joined)\n</code></pre>"},{"location":"kstream/KStream/#merging-kstreams","text":"","title":"Merging KStreams <pre><code>KStream&lt;K, V&gt; merge(\n  KStream&lt;K, V&gt; stream,\n  Named named)\n... // (1)\n</code></pre> <ol> <li>There are others. Let's focus on the most important parts.</li> </ol> <p>See KStreamImpl.merge</p>"},{"location":"kstream/KStream/#peek","text":"","title":"peek <pre><code>KStream&lt;K, V&gt; peek(\n  ForeachAction&lt;? super K, ? super V&gt; action)\nKStream&lt;K, V&gt; peek(\n  ForeachAction&lt;? super K, ? super V&gt; action,\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#print","text":"","title":"print <pre><code>void print(\n  Printed&lt;K, V&gt; printed)\n</code></pre>"},{"location":"kstream/KStream/#process","text":"","title":"process <pre><code>void process(\n  ProcessorSupplier&lt;? super K, ? super V, Void, Void&gt; processorSupplier,\n  Named named,\n  String... stateStoreNames)\nvoid process(\n  ProcessorSupplier&lt;? super K, ? super V, Void, Void&gt; processorSupplier,\n  String... stateStoreNames)\n</code></pre>"},{"location":"kstream/KStream/#repartition","text":"","title":"repartition <pre><code>KStream&lt;K, V&gt; repartition()\nKStream&lt;K, V&gt; repartition(\n  Repartitioned&lt;K, V&gt; repartitioned)\n</code></pre> <p>KStreamImpl.repartition</p>"},{"location":"kstream/KStream/#split","text":"","title":"split <pre><code>BranchedKStream&lt;K, V&gt; split()\nBranchedKStream&lt;K, V&gt; split(\n  Named named)\n</code></pre>"},{"location":"kstream/KStream/#to","text":"","title":"to <pre><code>void to(\n  String topic)\nvoid to(\n  String topic,\n  Produced&lt;K, V&gt; produced)\nvoid to(\n  TopicNameExtractor&lt;K, V&gt; topicExtractor)\nvoid to(\n  TopicNameExtractor&lt;K, V&gt; topicExtractor,\n  Produced&lt;K, V&gt; produced)\n</code></pre>"},{"location":"kstream/KStream/#totable","text":"","title":"toTable <pre><code>KTable&lt;K, V&gt; toTable()\nKTable&lt;K, V&gt; toTable(\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\nKTable&lt;K, V&gt; toTable(\n  Named named)\nKTable&lt;K, V&gt; toTable(\n  Named named,\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\n</code></pre>"},{"location":"kstream/KStream/#transform","text":"","title":"transform <pre><code>KStream&lt;K1, V1&gt; transform(\n  TransformerSupplier&lt;? super K, ? super V, KeyValue&lt;K1, V1&gt;&gt; transformerSupplier,\n  Named named,\n  String... stateStoreNames)\nKStream&lt;K1, V1&gt; transform(\n  TransformerSupplier&lt;? super K, ? super V, KeyValue&lt;K1, V1&gt;&gt; transformerSupplier,\n  String... stateStoreNames)\n</code></pre>"},{"location":"kstream/KStream/#implementations","text":"<ul> <li>KStreamImpl</li> </ul>","title":"Implementations"},{"location":"kstream/KStream/#demo","text":"<pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nval builder = new StreamsBuilder\n\n// Use type annotation to describe the stream, i.e. stream[String, String]\n// Else...Scala type inferencer gives us a stream of \"nothing\", i.e. KStream[Nothing, Nothing]\nval input = builder.stream[String, String](\"input\")\n</code></pre> <pre><code>scala&gt; :type input\norg.apache.kafka.streams.scala.kstream.KStream[String,String]\n</code></pre>","title":"Demo"},{"location":"kstream/KStreamAggProcessorSupplier/","text":"<p><code>KStreamAggProcessorSupplier</code> is...FIXME</p>","title":"KStreamAggProcessorSupplier"},{"location":"kstream/KStreamAggregate/","text":"<p><code>KStreamAggregate&lt;K, V, T&gt;</code> is a KStreamAggProcessorSupplier.</p>","title":"KStreamAggregate"},{"location":"kstream/KStreamAggregate/#creating-instance","text":"<p><code>KStreamAggregate</code> takes the following to be created:</p> <ul> <li> Name of a State Store <li> Initializer (of <code>T</code> values) <li> <code>Aggregator&lt;? super K, ? super V, T&gt;</code>  <p><code>KStreamAggregate</code> is created\u00a0when:</p> <ul> <li><code>CogroupedStreamAggregateBuilder</code> is requested to <code>build</code> (a KTable)</li> <li><code>KGroupedStreamImpl</code> is requested to aggregate and doCount</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamAggregateProcessor/","text":"<p><code>KStreamAggregateProcessor</code> is an AbstractProcessor of KStreamAggregate.</p>","title":"KStreamAggregateProcessor"},{"location":"kstream/KStreamAggregateProcessor/#creating-instance","text":"<p><code>KStreamAggregateProcessor</code> takes no arguments to be created.</p> <p><code>KStreamAggregateProcessor</code> is created\u00a0when:</p> <ul> <li><code>KStreamAggregate</code> is requested for a Processor</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamAggregateProcessor/#kstreamaggregate","text":"","title":"KStreamAggregate <p><code>KStreamAggregateProcessor</code> is a <code>private class</code> of KStreamAggregate and so have access to the internal properties (e.g. state name) thereof.</p>"},{"location":"kstream/KStreamAggregateProcessor/#timestampedkeyvaluestore","text":"","title":"TimestampedKeyValueStore <p><code>KStreamAggregateProcessor</code> looks up a TimestampedKeyValueStore by the name given when the owning KStreamAggregate was created.</p> <p>The <code>TimestampedKeyValueStore</code> is used for the following:</p> <ul> <li>Create a TimestampedTupleForwarder (in init)</li> <li>Process a key-value record (using a ValueAndTimestamp)</li> </ul>"},{"location":"kstream/KStreamAggregateProcessor/#timestampedtupleforwarder","text":"","title":"TimestampedTupleForwarder <p><code>KStreamAggregateProcessor</code> creates a new TimestampedTupleForwarder when created.</p> <p>The <code>TimestampedTupleForwarder</code> is used when processing a record.</p>"},{"location":"kstream/KStreamAggregateProcessor/#initializing","text":"","title":"Initializing <pre><code>void init(\n  ProcessorContext context)\n</code></pre> <p><code>init</code>...FIXME</p> <p><code>init</code>\u00a0is part of the AbstractProcessor abstraction.</p>"},{"location":"kstream/KStreamAggregateProcessor/#processing-record","text":"","title":"Processing Record <pre><code>void process(\n  K key, \n  V value)\n</code></pre> <p><code>process</code> requests the TimestampedKeyValueStore for the value for the input key (that gives a ValueAndTimestamp if found).</p> <p>With no previous value found, <code>process</code> requests the parent's Initializer for the initial value and the ProcessorContext for the timestamp.</p> <p><code>process</code> requests the parent's Aggregator for a new aggregate for the input key and value (and the previous or newly-created aggregation).</p> <p><code>process</code> creates a new ValueAndTimestamp with the new aggregate and the timestamp and requests the TimestampedKeyValueStore to store it (for the key).</p> <p>In the end, <code>process</code> requests the TimestampedTupleForwarder to maybeForward.</p>  <p><code>process</code>\u00a0is part of the AbstractProcessor abstraction.</p>"},{"location":"kstream/KStreamFilter/","text":"<p><code>KStreamFilter</code> is a ProcessorSupplier of KStreamFilterProcessors.</p>","title":"KStreamFilter"},{"location":"kstream/KStreamFilter/#creating-instance","text":"<p><code>KStreamFilter</code> takes the following to be created:</p> <ul> <li> <code>Predicate</code> <li> <code>filterNot</code> flag  <p><code>KStreamFilter</code> is created when:</p> <ul> <li><code>KStreamImpl</code> is requested to filter, filterNot and create a repartitioned source</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamFilter/#get","text":"","title":"get <pre><code>Processor&lt;K, V, K, V&gt; get()\n</code></pre> <p><code>get</code> creates a new KStreamFilterProcessor.</p> <p><code>get</code> is part of the ProcessorSupplier abstraction.</p>"},{"location":"kstream/KStreamFilter/#kstreamfilterprocessor","text":"","title":"KStreamFilterProcessor <p><code>KStreamFilterProcessor</code> is a ContextualProcessor</p>"},{"location":"kstream/KStreamFilter/#processing-record","text":"","title":"Processing Record <pre><code>void process(\n  Record&lt;K, V&gt; record)\n</code></pre> <p><code>process</code> requests the ProcessorContext to forward the given record when either the filterNot flag or the Predicate is <code>true</code> but not both (XOR).</p> <p><code>process</code> is part of the Processor abstraction.</p>"},{"location":"kstream/KStreamImpl/","text":"<p><code>KStreamImpl</code> is a KStream.</p>","title":"KStreamImpl"},{"location":"kstream/KStreamImpl/#creating-instance","text":"<p><code>KStreamImpl</code> takes the following to be created:</p> <ul> <li> Name <li> Key <code>Serde</code> (Apache Kafka) <li> Value <code>Serde</code> (Apache Kafka) <li> Sub-Topology Source Nodes (Names) <li>repartitionRequired flag</li> <li> GraphNode <li> InternalStreamsBuilder  <p><code>KStreamImpl</code> is created\u00a0when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to stream</li> <li>others</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamImpl/#repartitionrequired-flag","text":"","title":"repartitionRequired Flag <p><code>KStreamImpl</code> is given a <code>repartitionRequired</code> flag when created.</p> <p>The flag is disabled (<code>false</code>) when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to create a KStream</li> <li><code>KStreamImpl</code> is requested to doRepartition, repartitionForJoin, doStreamTableJoin</li> <li><code>KStreamImplJoin</code> is requested to join</li> <li><code>KTableImpl</code> is requested to toStream</li> </ul> <p>The <code>repartitionRequired</code> flag is left unchanged (and handed over to the child nodes) in most operators (e.g. <code>filter</code>, <code>mapValues</code>, <code>split</code>).</p> <p>The flag is enabled (<code>true</code>) when:</p> <ul> <li><code>KStreamImpl</code> is requested to selectKey, map, flatMap, merge (when either <code>KStream</code> requires so), flatTransform</li> </ul> <p>The <code>repartitionRequired</code> flag is used in the following operators to add an extra (parent) <code>OptimizableRepartitionNode</code> to the InternalStreamsBuilder:</p> <ul> <li>toTable</li> <li>doJoin, join and leftJoin (to repartitionForJoin)</li> </ul>"},{"location":"kstream/KStreamImpl/#merge","text":"","title":"merge <pre><code>KStream&lt;K, V&gt; merge(\n  InternalStreamsBuilder builder,\n  KStream&lt;K, V&gt; stream,\n  NamedInternal named)\n</code></pre> <p><code>merge</code> creates a ProcessorGraphNode and turns mergeNode flag on.</p> <p><code>merge</code> requests the InternalStreamsBuilder to add the new ProcessorGraphNode (with this and the given <code>KStream</code>'s GraphNodes as the parents).</p> <p>In the end, <code>merge</code> creates a new KStreamImpl for the new <code>ProcessorGraphNode</code>.</p> <p><code>merge</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImpl/#join","text":"","title":"join <pre><code>KStream&lt;K, VR&gt; join(\n  GlobalKTable&lt;KG, VG&gt; globalTable,\n  ...) // (1)\nKStream&lt;K, VR&gt; join(\n  KStream&lt;K, VO&gt; otherStream,\n  ...)\nKStream&lt;K, VR&gt; join(\n  KTable&lt;K, VO&gt; table,\n  ...)\n</code></pre> <ol> <li>There are quite a few <code>join</code>s</li> </ol> <p><code>join</code>...FIXME</p> <p><code>join</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImpl/#leftjoin","text":"","title":"leftJoin <pre><code>KStream&lt;K, VR&gt; leftJoin(\n  GlobalKTable&lt;KG, VG&gt; globalTable,\n  ...) // (1)\nKStream&lt;K, VR&gt; leftJoin(\n  KStream&lt;K, VO&gt; otherStream,\n  ...)\nKStream&lt;K, VR&gt; leftJoin(\n  KTable&lt;K, VO&gt; table,\n  ...)\n</code></pre> <ol> <li>There are quite a few <code>leftJoin</code>s</li> </ol> <p><code>leftJoin</code>...FIXME</p> <p><code>leftJoin</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImpl/#outerjoin","text":"","title":"outerJoin <pre><code>KStream&lt;K, VR&gt; outerJoin(\n  KStream&lt;K, VO&gt; otherStream,\n  ...) // (1)\n</code></pre> <ol> <li>There are quite a few <code>outerJoin</code>s</li> </ol> <p><code>outerJoin</code>...FIXME</p> <p><code>outerJoin</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImpl/#dojoin","text":"","title":"doJoin <pre><code>KStream&lt;K, VR&gt; doJoin(\n  KStream&lt;K, VO&gt; otherStream,\n  ValueJoinerWithKey&lt;? super K, ? super V, ? super VO, ? extends VR&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K, V, VO&gt; streamJoined,\n  KStreamImplJoin join)\n</code></pre> <p>In the end, <code>doJoin</code> requests the given KStreamImplJoin to join.</p> <p><code>doJoin</code>\u00a0is used when:</p> <ul> <li><code>KStreamImpl</code> is requested to join, leftJoin and outerJoin</li> </ul>"},{"location":"kstream/KStreamImpl/#repartitionforjoin","text":"","title":"repartitionForJoin <pre><code>KStreamImpl&lt;K, V&gt; repartitionForJoin(\n  String repartitionName,\n  Serde&lt;K&gt; keySerdeOverride,\n  Serde&lt;V&gt; valueSerdeOverride)\n</code></pre> <p><code>repartitionForJoin</code> creates an OptimizableRepartitionNodeBuilder.</p> <p><code>repartitionForJoin</code> creates a repartitioned source.</p> <p>Only when there is no OptimizableRepartitionNode defined already or the name (of this <code>KStreamImpl</code>) is different from the given <code>repartitionName</code>, <code>repartitionForJoin</code> requests the <code>OptimizableRepartitionNodeBuilder</code> to build an <code>OptimizableRepartitionNode</code> and requests the InternalStreamsBuilder to add the repartition node (to the GraphNode).</p> <p>In the end, <code>repartitionForJoin</code> creates a new KStreamImpl (with the repartitionRequired flag off and the OptimizableRepartitionNode as the GraphNode).</p>"},{"location":"kstream/KStreamImpl/#groupby","text":"","title":"groupBy <pre><code>KGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector)\nKGroupedStream&lt;KR, V&gt; groupBy(\n  KeyValueMapper&lt;? super K, ? super V, KR&gt; keySelector,\n  Grouped&lt;KR, V&gt; grouped)\n</code></pre> <p><code>groupBy</code>...FIXME</p> <p>In the end, <code>groupBy</code> creates a KGroupedStreamImpl (with the repartitionRequired flag enabled).</p> <p><code>groupBy</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImpl/#groupbykey","text":"","title":"groupByKey <pre><code>KGroupedStream&lt;K, V&gt; groupByKey()\nKGroupedStream&lt;K, V&gt; groupByKey(\n  Grouped&lt;K, V&gt; grouped)\n</code></pre> <p><code>groupByKey</code> creates a KGroupedStreamImpl.</p> <p><code>groupByKey</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImpl/#repartition","text":"","title":"repartition <pre><code>KStream&lt;K, V&gt; repartition()\nKStream&lt;K, V&gt; repartition(\n  Repartitioned&lt;K, V&gt; repartitioned)\n</code></pre> <p><code>repartition</code> doRepartition</p> <p><code>repartition</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImpl/#dorepartition","text":"","title":"doRepartition <pre><code>KStream&lt;K, V&gt; doRepartition(\n  Repartitioned&lt;K, V&gt; repartitioned)\n</code></pre> <p><code>doRepartition</code> creates a new UnoptimizableRepartitionNodeBuilder that is then used to createRepartitionedSource.</p> <p><code>doRepartition</code> requests the <code>UnoptimizableRepartitionNodeBuilder</code> to build a <code>UnoptimizableRepartitionNode</code>.</p> <p><code>doRepartition</code> requests the InternalStreamsBuilder to add the UnoptimizableRepartitionNode to the GraphNode (as a child node).</p> <p>In the end, <code>doRepartition</code> creates a new KStreamImpl (with the repartitionRequired turned off and the <code>UnoptimizableRepartitionNode</code> as the GraphNode).</p>"},{"location":"kstream/KStreamImpl/#creating-repartitioned-source","text":"","title":"Creating Repartitioned Source <pre><code>String createRepartitionedSource(\n  InternalStreamsBuilder builder,\n  Serde&lt;K1&gt; keySerde,\n  Serde&lt;V1&gt; valueSerde,\n  String repartitionTopicNamePrefix,\n  StreamPartitioner&lt;K1, V1&gt; streamPartitioner,\n  BaseRepartitionNodeBuilder&lt;K1, V1, RN&gt; baseRepartitionNodeBuilder)\n</code></pre> <p><code>createRepartitionedSource</code>...FIXME</p> <p><code>createRepartitionedSource</code>\u00a0is used when:</p> <ul> <li><code>CogroupedStreamAggregateBuilder</code> is requested to <code>createRepartitionSource</code></li> <li><code>GroupedStreamAggregateBuilder</code> is requested to <code>createRepartitionSource</code></li> <li><code>InternalStreamsBuilder</code> is requested to createRepartitionNode</li> <li><code>KStreamImpl</code> is requested to doRepartition, toTable, repartitionForJoin</li> </ul>"},{"location":"kstream/KStreamImpl/#totable","text":"","title":"toTable <pre><code>KTable&lt;K, V&gt; toTable(\n  Named named,\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\nKTable&lt;K, V&gt; toTable(...) // (1)\n</code></pre> <ol> <li>There are other <code>toTable</code>s (of less interest)</li> </ol> <p>Only when the repartitionRequired flag is enabled, <code>toTable</code> creates an OptimizableRepartitionNodeBuilder and a repartitioned source. <code>toTable</code> requests the <code>OptimizableRepartitionNodeBuilder</code> to build a (parent) <code>OptimizableRepartitionNode</code> and requests the InternalStreamsBuilder to add the repartition node (to the GraphNode).</p> <p><code>toTable</code>...FIXME</p> <p><code>toTable</code>\u00a0is part of the KStream abstraction.</p>"},{"location":"kstream/KStreamImplJoin/","text":"","title":"KStreamImplJoin"},{"location":"kstream/KStreamImplJoin/#creating-instance","text":"<p><code>KStreamImplJoin</code> takes the following to be created:</p> <ul> <li> InternalStreamsBuilder <li> <code>leftOuter</code> flag <li> <code>rightOuter</code> flag  <p><code>KStreamImplJoin</code> is created\u00a0when:</p> <ul> <li><code>KStreamImpl</code> is requested to join, leftJoin and outerJoin</li> </ul>","title":"Creating Instance"},{"location":"kstream/KStreamImplJoin/#join","text":"","title":"join <pre><code>KStream&lt;K1, R&gt; join(\n  KStream&lt;K1, V1&gt; lhs,\n  KStream&lt;K1, V2&gt; other,\n  ValueJoinerWithKey&lt;? super K1, ? super V1, ? super V2, ? extends R&gt; joiner,\n  JoinWindows windows,\n  StreamJoined&lt;K1, V1, V2&gt; streamJoined)\n</code></pre> <p><code>join</code>...FIXME</p> <p><code>join</code>\u00a0is used when:</p> <ul> <li><code>KStreamImpl</code> is requested to doJoin</li> </ul>"},{"location":"kstream/KStreamSlidingWindowAggregateProcessor/","text":"<p><code>KStreamSlidingWindowAggregateProcessor</code> is...FIXME</p>","title":"KStreamSlidingWindowAggregateProcessor"},{"location":"kstream/KStreamWindowAggregateProcessor/","text":"<p><code>KStreamWindowAggregateProcessor</code> is...FIXME</p>","title":"KStreamWindowAggregateProcessor"},{"location":"kstream/KTable/","text":"<p><code>KTable</code> is...FIXME</p>","title":"KTable"},{"location":"kstream/KTableImpl/","text":"<p><code>KTableImpl</code> is...FIXME</p>","title":"KTableImpl"},{"location":"kstream/KTableKTableJoinMergeProcessor/","text":"<p><code>KTableKTableJoinMergeProcessor</code> is...FIXME</p>","title":"KTableKTableJoinMergeProcessor"},{"location":"kstream/KTableSource/","text":"<p><code>KTableSource</code> is...FIXME</p>","title":"KTableSource"},{"location":"kstream/Materialized/","text":"<p><code>Materialized</code> is...FIXME</p>","title":"Materialized"},{"location":"kstream/NamedOperation/","text":"<p><code>NamedOperation</code> is an abstraction of metadata with the name of the associated processors (and in turn the names of operations, internal topics and stores).</p>","title":"NamedOperation"},{"location":"kstream/NamedOperation/#contract","text":"","title":"Contract"},{"location":"kstream/NamedOperation/#withname","text":"","title":"withName <pre><code>T withName(\n  String name)\n</code></pre> <p>Processor name</p>"},{"location":"kstream/NamedOperation/#implementations","text":"<ul> <li>Branched</li> <li>Consumed</li> <li>Grouped</li> <li>Joined</li> <li>Named</li> <li>Printed</li> <li>Produced</li> <li>Repartitioned</li> <li>StreamJoined</li> <li>Suppressed</li> </ul>","title":"Implementations"},{"location":"kstream/OptimizableRepartitionNode/","text":"<p><code>OptimizableRepartitionNode&lt;K, V&gt;</code> is a BaseRepartitionNode.</p>","title":"OptimizableRepartitionNode"},{"location":"kstream/OptimizableRepartitionNode/#creating-instance","text":"<p><code>OptimizableRepartitionNode</code> takes the following to be created:</p> <ul> <li> Node Name <li> Source Name <li> <code>ProcessorParameters</code> <li> Key <code>Serde</code> (Apache Kafka) <li> Value <code>Serde</code> (Apache Kafka) <li> Sink Name <li> Repartition Topic <li> StreamPartitioner  <p><code>OptimizableRepartitionNode</code> is created\u00a0when:</p> <ul> <li><code>OptimizableRepartitionNodeBuilder</code> is requested to build</li> </ul>","title":"Creating Instance"},{"location":"kstream/OptimizableRepartitionNode/#writetotopology","text":"","title":"writeToTopology <pre><code>void writeToTopology(\n  InternalTopologyBuilder topologyBuilder,\n  Properties props)\n</code></pre> <p><code>writeToTopology</code> requests the given InternalTopologyBuilder for the following:</p> <ol> <li>addInternalTopic (with the repartitionTopic)</li> <li>addSink (with the sinkName, the repartitionTopic)</li> <li>addSource (with the sourceName, the repartitionTopic)</li> </ol> <p><code>writeToTopology</code>\u00a0is part of the GraphNode abstraction.</p>"},{"location":"kstream/OptimizableRepartitionNode/#creating-optimizablerepartitionnodebuilder","text":"","title":"Creating OptimizableRepartitionNodeBuilder <pre><code>OptimizableRepartitionNodeBuilder&lt;K, V&gt; optimizableRepartitionNodeBuilder()\n</code></pre> <p><code>optimizableRepartitionNodeBuilder</code> creates a new <code>OptimizableRepartitionNodeBuilder</code>.</p> <p><code>optimizableRepartitionNodeBuilder</code>\u00a0is used when:</p> <ul> <li><code>CogroupedStreamAggregateBuilder</code> is requested to <code>processRepartitions</code> (with <code>repartitionRequired</code> flag on among <code>KGroupedStreamImpl</code>s)</li> <li><code>GroupedStreamAggregateBuilder</code> is requested to build a KTable (with <code>repartitionRequired</code> flag on)</li> <li><code>InternalStreamsBuilder</code> is requested to createRepartitionNode</li> <li><code>KStreamImpl</code> is requested to toTable (with <code>repartitionRequired</code> flag on), repartitionForJoin</li> </ul>"},{"location":"kstream/ProcessorGraphNode/","text":"<p><code>ProcessorGraphNode</code> is a GraphNode.</p>","title":"ProcessorGraphNode"},{"location":"kstream/ProcessorGraphNode/#creating-instance","text":"<p><code>ProcessorGraphNode</code> takes the following to be created:</p> <ul> <li> Node Name <li> <code>ProcessorParameters</code>  <p><code>ProcessorGraphNode</code> is created when:</p> <ul> <li><code>BranchedKStreamImpl</code> is created and requested to <code>createBranch</code></li> <li><code>CogroupedStreamAggregateBuilder</code> is requested to <code>createTable</code></li> <li><code>KStreamImpl</code> is requested to filter, filterNot, internalSelectKey, map, mapValues, flatMap, flatMapValues, print, foreach, peek, doBranch, merge</li> <li><code>KStreamImplJoin</code> is requested to join</li> <li><code>KTableImpl</code> is requested to doJoinOnForeignKey, toStream, groupBy</li> </ul>","title":"Creating Instance"},{"location":"kstream/ProcessorGraphNode/#writetotopology","text":"","title":"writeToTopology <pre><code>void writeToTopology(\n  InternalTopologyBuilder topologyBuilder,\n  Properties props)\n</code></pre> <p><code>writeToTopology</code> requests the given InternalTopologyBuilder to addProcessor (with the <code>processorName</code> and <code>processorSupplier</code> of the ProcessorParameters and parentNodeNames)</p> <p><code>writeToTopology</code> is part of the GraphNode abstraction.</p>"},{"location":"kstream/Produced/","text":"<p><code>Produced&lt;K, V&gt;</code> describes how to produce records in a topology in the High-Level KStream DSL for the following high-level operators:</p> <ul> <li>KStream.to</li> </ul> <p><code>Produced&lt;K, V&gt;</code> is a NamedOperation.</p>","title":"Produced \u2014 Metadata for Producing Records"},{"location":"kstream/Produced/#demo","text":"<pre><code>import org.apache.kafka.common.serialization.Serdes\nimport org.apache.kafka.streams.kstream.Produced\nval produced = Produced.`with`(Serdes.Long, Serdes.String)\n</code></pre> <pre><code>scala&gt; :type produced\norg.apache.kafka.streams.kstream.Produced[Long,String]\n</code></pre>","title":"Demo"},{"location":"kstream/Produced/#creating-instance","text":"<p><code>Produced</code> takes the following to be created:</p> <ul> <li> <code>Serde&lt;K&gt;</code> of keys (Apache Kafka) <li> <code>Serde&lt;V&gt;</code> of values (Apache Kafka) <li> StreamPartitioner <li> Processor Name  <p><code>Produced</code> is created\u00a0using the factories.</p>","title":"Creating Instance"},{"location":"kstream/Produced/#creating-consumed","text":"","title":"Creating Consumed"},{"location":"kstream/Produced/#as","text":"","title":"as <pre><code>Produced&lt;K, V&gt; as(\n  String processorName)\n</code></pre>"},{"location":"kstream/Produced/#with","text":"","title":"with <pre><code>Produced&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde)\nProduced&lt;K, V&gt; with(\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde,\n  StreamPartitioner&lt;? super K, ? super V&gt; partitioner)\n</code></pre>"},{"location":"kstream/Produced/#keyserde","text":"","title":"keySerde <pre><code>Produced&lt;K, V&gt; keySerde(\n  Serde&lt;K&gt; keySerde)\n</code></pre>"},{"location":"kstream/Produced/#valueserde","text":"","title":"valueSerde <pre><code>Produced&lt;K, V&gt; valueSerde(\n  Serde&lt;V&gt; valueSerde)\n</code></pre>"},{"location":"kstream/Produced/#streampartitioner","text":"","title":"streamPartitioner <pre><code>Produced&lt;K, V&gt; streamPartitioner(\n  StreamPartitioner&lt;? super K, ? super V&gt; partitioner)\n</code></pre>"},{"location":"kstream/Produced/#scala-api","text":"","title":"Scala API <p>Scala API for Kafka Streams makes the optional <code>Produced</code> metadata an implicit parameter in the KStream API.</p> <p>Moreover, <code>ImplicitConversions</code> object defines <code>producedFromSerde</code> implicit method that creates a <code>Produced</code> instance with the key and value <code>Serde</code> objects available in implicit scope.</p> <p>And the last but not least, Scala API for Kafka Streams defines <code>Produced</code> object with <code>with</code> factory methods that use implicit key and value <code>Serde</code> objects.</p>"},{"location":"kstream/SourceGraphNode/","text":"<p><code>SourceGraphNode</code> is...FIXME</p>","title":"SourceGraphNode"},{"location":"kstream/StateStoreNode/","text":"<p><code>StateStoreNode</code> is a GraphNode.</p>","title":"StateStoreNode"},{"location":"kstream/StateStoreNode/#creating-instance","text":"<p><code>StateStoreNode</code> takes the following to be created:</p> <ul> <li> StoreBuilder  <p><code>StateStoreNode</code> is created\u00a0when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to add a StoreBuilder</li> </ul>","title":"Creating Instance"},{"location":"kstream/StateStoreNode/#writetotopology","text":"","title":"writeToTopology <pre><code>void writeToTopology(\n  InternalTopologyBuilder topologyBuilder,\n  Properties props)\n</code></pre> <p><code>writeToTopology</code> merely requests the given InternalTopologyBuilder to add the storeBuilder.</p> <p><code>writeToTopology</code>\u00a0is part of the GraphNode abstraction.</p>"},{"location":"kstream/StatefulProcessorNode/","text":"<p><code>StatefulProcessorNode</code> is a ProcessorGraphNode.</p>","title":"StatefulProcessorNode"},{"location":"kstream/StatefulProcessorNode/#creating-instance","text":"<p><code>StatefulProcessorNode</code> takes the following to be created:</p> <ul> <li> Name <li> <code>ProcessorParameters</code> <li>State Stores</li> <li> <code>KTableValueGetterSupplier</code>s  <p><code>StatefulProcessorNode</code> is created when:</p> <ul> <li><code>CogroupedStreamAggregateBuilder</code> is requested to <code>getStatefulProcessorNode</code></li> <li><code>GroupedStreamAggregateBuilder</code> is requested to build</li> <li><code>KGroupedTableImpl</code> is requested to <code>doAggregate</code></li> <li><code>KStreamImpl</code> is requested to flatTransform, doTransformValues, doFlatTransformValues and process</li> <li><code>KTableImpl</code> is requested to doJoinOnForeignKey and suppress</li> </ul>","title":"Creating Instance"},{"location":"kstream/StatefulProcessorNode/#state-stores","text":"","title":"State Stores <p><code>StatefulProcessorNode</code> can be given the names of the associated state stores directly or as StoreBuilders (together with the stores of the KTableValueGetterSuppliers) when created.</p>"},{"location":"kstream/StatefulProcessorNode/#state-stores_1","text":"","title":"State Stores <p><code>StatefulProcessorNode</code> can be given a StoreBuilder when created.</p>"},{"location":"kstream/StreamSourceNode/","text":"<p><code>StreamSourceNode&lt;K, V&gt;</code> is a SourceGraphNode.</p>","title":"StreamSourceNode"},{"location":"kstream/StreamSourceNode/#creating-instance","text":"<p><code>StreamSourceNode</code> takes the following to be created:</p> <ul> <li> Node Name <li> Topic Names or Pattern <li> <code>ConsumedInternal&lt;K, V&gt;</code>  <p><code>StreamSourceNode</code> is created\u00a0when:</p> <ul> <li><code>InternalStreamsBuilder</code> is requested to stream</li> <li><code>KTableImpl</code> is requested to doJoinOnForeignKey</li> </ul>","title":"Creating Instance"},{"location":"kstream/StreamSourceNode/#writetotopology","text":"","title":"writeToTopology <pre><code>void writeToTopology(\n  InternalTopologyBuilder topologyBuilder, \n  Properties props)\n</code></pre> <p><code>writeToTopology</code> requests the given InternalTopologyBuilder to addSource.</p> <p><code>writeToTopology</code>\u00a0is part of the GraphNode abstraction.</p>"},{"location":"kstream/StreamsBuilder/","text":"<p><code>StreamsBuilder</code> is the entry point to the High-Level Streams DSL to define and build a stream processing topology.</p> <p>All of the high-level operators use the InternalStreamsBuilder behind the scenes. In other words, <code>StreamsBuilder</code> offers a more developer-friendly high-level API for developing Kafka Streams applications than using the <code>InternalStreamsBuilder</code> API directly (and is a fa\u00e7ade of <code>InternalStreamsBuilder</code>).</p>  <p>Scala API for Kafka Streams</p> <p>Use Scala API for Kafka Streams to make your Kafka Streams development more pleasant with Scala.</p>","title":"StreamsBuilder"},{"location":"kstream/StreamsBuilder/#creating-instance","text":"<p><code>StreamsBuilder</code> takes no arguments to be created.</p> <pre><code>import org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <p>While being created, <code>StreamsBuilder</code> creates an empty Topology that in turn is requested for an InternalTopologyBuilder. In the end, <code>StreamsBuilder</code> creates an InternalStreamsBuilder.</p> <p></p>","title":"Creating Instance"},{"location":"kstream/StreamsBuilder/#topology","text":"","title":"Topology <p><code>StreamsBuilder</code> creates a Topology when created.</p> <p><code>StreamsBuilder</code> uses the <code>Topology</code> to create an InternalTopologyBuilder.</p> <p>The <code>Topology</code> is then optimized and returned when <code>StreamsBuilder</code> is requested to build a topology.</p>"},{"location":"kstream/StreamsBuilder/#building-and-optimizing-topology","text":"","title":"Building and Optimizing Topology <pre><code>Topology build() // (1)\nTopology build(\n  Properties props)\n</code></pre> <ol> <li>Uses undefined properties (<code>null</code>)</li> </ol> <p><code>build</code> requests the InternalStreamsBuilder to build and optimize a topology. In the end, <code>build</code> returns the Topology.</p>"},{"location":"kstream/StreamsBuilder/#globaltable","text":"","title":"globalTable <pre><code>GlobalKTable&lt;K, V&gt; globalTable(\n  String topic)\nGlobalKTable&lt;K, V&gt; globalTable(\n  String topic,\n  Consumed&lt;K, V&gt; consumed)\nGlobalKTable&lt;K, V&gt; globalTable(\n  String topic,\n  Consumed&lt;K, V&gt; consumed,\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\nGlobalKTable&lt;K, V&gt; globalTable(\n  String topic,\n  Materialized&lt;K, V, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized)\n</code></pre> <p><code>globalTable</code> adds a GlobalKTable to a topology.</p>"},{"location":"kstream/StreamsBuilder/#demo-non-queryable-globalktable","text":"","title":"Demo: Non-queryable GlobalKTable <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nimport org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <pre><code>val globalTable = builder.globalTable[String, String](topic = \"demo-global-table\")\n</code></pre> <pre><code>scala&gt; :type globalTable\norg.apache.kafka.streams.kstream.GlobalKTable[String,String]\n</code></pre> <pre><code>assert(globalTable.queryableStoreName == null)\n</code></pre> <pre><code>val topology = builder.build()\n</code></pre> <pre><code>scala&gt; println(topology.describe)\nTopologies:\n   Sub-topology: 0 for global store (will not generate tasks)\n    Source: KSTREAM-SOURCE-0000000001 (topics: [demo-global-table])\n      --&gt; KTABLE-SOURCE-0000000002\n    Processor: KTABLE-SOURCE-0000000002 (stores: [demo-global-table-STATE-STORE-0000000000])\n      --&gt; none\n      &lt;-- KSTREAM-SOURCE-0000000001\n</code></pre>"},{"location":"kstream/StreamsBuilder/#demo-queryable-globalktable","text":"","title":"Demo: Queryable GlobalKTable <pre><code>import org.apache.kafka.streams.scala._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nimport org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <pre><code>import org.apache.kafka.streams.state.Stores\nval supplier = Stores.inMemoryKeyValueStore(\"queryable-store-name\")\n\nimport org.apache.kafka.streams.scala.kstream.Materialized\nval materialized = Materialized.as[String, String](supplier)\nval zipCodes = builder.globalTable[String, String](topic = \"zip-codes\", materialized)\n</code></pre> <pre><code>scala&gt; :type zipCodes\norg.apache.kafka.streams.kstream.GlobalKTable[String,String]\n</code></pre> <pre><code>assert(zipCodes.queryableStoreName == \"queryable-store-name\")\n</code></pre> <pre><code>val topology = builder.build()\n</code></pre> <pre><code>scala&gt; println(topology.describe)\nTopologies:\n   Sub-topology: 0 for global store (will not generate tasks)\n    Source: KSTREAM-SOURCE-0000000000 (topics: [zip-codes])\n      --&gt; KTABLE-SOURCE-0000000001\n    Processor: KTABLE-SOURCE-0000000001 (stores: [queryable-store-name])\n      --&gt; none\n      &lt;-- KSTREAM-SOURCE-0000000000\n</code></pre>"},{"location":"kstream/StreamsBuilder/#stream","text":"","title":"stream <pre><code>KStream&lt;K, V&gt; stream(\n  Collection&lt;String&gt; topics)\nKStream&lt;K, V&gt; stream(\n  Collection&lt;String&gt; topics,\n  Consumed&lt;K, V&gt; consumed)\nKStream&lt;K, V&gt; stream(\n  Pattern topicPattern)\nKStream&lt;K, V&gt; stream(\n  Pattern topicPattern,\n  Consumed&lt;K, V&gt; consumed)\nKStream&lt;K, V&gt; stream(\n  String topic)\nKStream&lt;K, V&gt; stream(\n  String topic,\n  Consumed&lt;K, V&gt; consumed)\n</code></pre> <p><code>stream</code> requests the InternalStreamsBuilder to stream.</p>"},{"location":"kstream/StreamsBuilder/#demo-custom-processor-name","text":"","title":"Demo: Custom Processor Name <pre><code>import org.apache.kafka.streams.scala._\nimport org.apache.kafka.streams.scala.kstream._\nimport ImplicitConversions._\nimport serialization.Serdes._\n\nimport org.apache.kafka.streams.scala.StreamsBuilder\nval builder = new StreamsBuilder\n</code></pre> <pre><code>implicit val consumed = Consumed.`with`[String, String].withName(\"processorName\")\nval demo = builder.stream[String, String](\"demo\")\n</code></pre> <pre><code>scala&gt; println(builder.build().describe)\nTopologies:\n   Sub-topology: 0\n    Source: processorName (topics: [demo])\n      --&gt; none\n</code></pre>"},{"location":"kstream/TimestampedTupleForwarder/","text":"<p><code>TimestampedTupleForwarder</code> is used by processors to determine whether or not to forward records to child nodes (downstream processors) (that happens only with no caching).</p>","title":"TimestampedTupleForwarder"},{"location":"kstream/TimestampedTupleForwarder/#creating-instance","text":"<p><code>TimestampedTupleForwarder</code> takes the following to be created:</p> <ul> <li> StateStore <li> ProcessorContext <li> <code>TimestampedCacheFlushListener</code> <li> <code>sendOldValues</code> flag  <p><code>TimestampedTupleForwarder</code> is created\u00a0when:</p> <ul> <li><code>KStreamAggregateProcessor</code> is requested to initialize</li> <li><code>KStreamSlidingWindowAggregateProcessor</code> is requested to initialize</li> <li><code>KStreamWindowAggregateProcessor</code> is requested to initialize</li> <li><code>KTableSource</code> is requested to initialize</li> <li>others</li> </ul>","title":"Creating Instance"},{"location":"kstream/TimestampedTupleForwarder/#cachingenabled-flag","text":"","title":"cachingEnabled Flag <p><code>TimestampedTupleForwarder</code> requests the StateStore to setFlushListener when created. The returned value is used to initialize <code>cachingEnabled</code> internal flag for maybeForward.</p>"},{"location":"kstream/TimestampedTupleForwarder/#maybeforward","text":"","title":"maybeForward <pre><code>void maybeForward(\n  K key,\n  V newValue,\n  V oldValue) // (1)\nvoid maybeForward(\n  K key,\n  V newValue,\n  V oldValue,\n  long timestamp)\nvoid maybeForward(\n  Record&lt;K, Change&lt;V&gt;&gt; record)\n</code></pre> <p><code>maybeForward</code> requests the InternalProcessorContext to forward a record only with the cachingEnabled flag disabled.</p>"},{"location":"kstream/UnoptimizableRepartitionNode/","text":"<p><code>UnoptimizableRepartitionNode</code> is...FIXME</p>","title":"UnoptimizableRepartitionNode"},{"location":"metrics/","text":"<p>Metrics tracks performance metrics of a KafkaStreams instance using StreamsMetrics abstraction.</p> <p><code>StreamsMetrics</code> is available using ProcessorContext.</p>","title":"Metrics"},{"location":"metrics/ClientMetrics/","text":"<p><code>ClientMetrics</code> registers performance metrics of a KafkaStreams instance (and associated StreamThreads).</p>    Name Description RecordingLevel Value     <code>state</code> The state of the Kafka Streams client <code>INFO</code> state   <code>version</code> The version of the Kafka Streams client <code>INFO</code> version   <code>topology-description</code> The description of the topology executed in the Kafka Streams client <code>INFO</code>    <code>alive-stream-threads</code> The current number of alive stream threads that are running or participating in rebalance <code>INFO</code> getNumLiveStreamThreads    <code>failed-stream-threads</code> The number of failed stream threads since the start of the Kafka Streams client <code>INFO</code> failedStreamThreadSensor    <p><code>ClientMetrics</code> was introduced in KIP-444.</p>","title":"ClientMetrics"},{"location":"metrics/ClientMetrics/#creating-instance","text":"<p><code>ClientMetrics</code> takes no arguments to be created.</p>","title":"Creating Instance"},{"location":"metrics/ClientMetrics/#kafka-streams-versionproperties","text":"","title":"kafka-streams-version.properties <p>While being created, <code>ClientMetrics</code> loads <code>/kafka/kafka-streams-version.properties</code> file (from the CLASSPATH) for the <code>version</code> and <code>commitId</code> values.</p> <p>The values are used when <code>KafkaStreams</code> is created (and prints them out as INFO message to the logs).</p> <pre><code>Kafka Streams version: [version]\nKafka Streams commit ID: [commitId]\n</code></pre>"},{"location":"metrics/ClientMetrics/#addversionmetric","text":"","title":"addVersionMetric <pre><code>void addVersionMetric(\n  StreamsMetricsImpl streamsMetrics)\n</code></pre> <p><code>addVersionMetric</code> requests the given StreamsMetricsImpl to addClientLevelImmutableMetric with the following:</p> <ul> <li>Name: <code>version</code></li> <li>Description: The version of the Kafka Streams client</li> <li>RecordingLevel: <code>INFO</code></li> <li>Value: version</li> </ul> <p><code>addVersionMetric</code> is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> </ul>"},{"location":"metrics/ClientMetrics/#addtopologydescriptionmetric","text":"","title":"addTopologyDescriptionMetric <pre><code>void addTopologyDescriptionMetric(\n  StreamsMetricsImpl streamsMetrics,\n  String topologyDescription)\n</code></pre> <p><code>addTopologyDescriptionMetric</code> requests the given StreamsMetricsImpl to addClientLevelImmutableMetric with the following:</p> <ul> <li>Name: <code>topology-description</code></li> <li>Description: The description of the topology executed in the Kafka Streams client</li> <li>RecordingLevel: <code>INFO</code></li> <li>Value: The given <code>topologyDescription</code></li> </ul> <p><code>addTopologyDescriptionMetric</code> is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> </ul>"},{"location":"metrics/ClientMetrics/#addstatemetric","text":"","title":"addStateMetric <pre><code>void addStateMetric(\n  StreamsMetricsImpl streamsMetrics,\n  Gauge&lt;State&gt; stateProvider)\n</code></pre> <p><code>addStateMetric</code> requests the given StreamsMetricsImpl to addClientLevelMutableMetric with the following:</p> <ul> <li>Name: <code>state</code></li> <li>Description: The state of the Kafka Streams client</li> <li>RecordingLevel: <code>INFO</code></li> <li>Value: The given <code>Gauge&lt;State&gt;</code> (with the state of the owning <code>KafkaStreams</code> instance)</li> </ul> <p><code>addStateMetric</code> is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> </ul>"},{"location":"metrics/ClientMetrics/#addnumalivestreamthreadmetric","text":"","title":"addNumAliveStreamThreadMetric <pre><code>void addNumAliveStreamThreadMetric(\n  StreamsMetricsImpl streamsMetrics,\n  Gauge&lt;Integer&gt; stateProvider)\n</code></pre> <p><code>addNumAliveStreamThreadMetric</code> requests the given StreamsMetricsImpl to addClientLevelMutableMetric with the following:</p> <ul> <li>Name: <code>alive-stream-threads</code></li> <li>Description: The current number of alive stream threads that are running or participating in rebalance</li> <li>RecordingLevel: <code>INFO</code></li> <li>Value: The given <code>Gauge&lt;Integer&gt;</code> (with getNumLiveStreamThreads of the owning <code>KafkaStreams</code> instance)</li> </ul> <p><code>addNumAliveStreamThreadMetric</code> is used when:</p> <ul> <li><code>KafkaStreams</code> is created</li> </ul>"},{"location":"metrics/StateStoreMetrics/","text":"<p><code>StateStoreMetrics</code> is...FIXME</p>","title":"StateStoreMetrics"},{"location":"metrics/StreamsMetrics/","text":"<p><code>StreamsMetrics</code> is an abstraction of performance metric registries (of a KafkaStreams instance).</p> <p><code>StreamsMetrics</code> is available using ProcessorContext.</p>","title":"StreamsMetrics"},{"location":"metrics/StreamsMetrics/#contract","text":"","title":"Contract"},{"location":"metrics/StreamsMetrics/#addlatencyratetotalsensor","text":"","title":"addLatencyRateTotalSensor <pre><code>Sensor addLatencyRateTotalSensor(\n  String scopeName,\n  String entityName,\n  String operationName,\n  Sensor.RecordingLevel recordingLevel,\n  String... tags)\n</code></pre>"},{"location":"metrics/StreamsMetrics/#addratetotalsensor","text":"","title":"addRateTotalSensor <pre><code>Sensor addRateTotalSensor(\n  String scopeName,\n  String entityName,\n  String operationName,\n  Sensor.RecordingLevel recordingLevel,\n  String... tags)\n</code></pre>"},{"location":"metrics/StreamsMetrics/#addsensor","text":"","title":"addSensor <pre><code>Sensor addSensor(\n  String name,\n  Sensor.RecordingLevel recordingLevel)\nSensor addSensor(\n  String name,\n  Sensor.RecordingLevel recordingLevel,\n  Sensor... parents)\n</code></pre>"},{"location":"metrics/StreamsMetrics/#metrics","text":"","title":"metrics <pre><code>Map&lt;MetricName, ? extends Metric&gt; metrics()\n</code></pre>"},{"location":"metrics/StreamsMetrics/#removesensor","text":"","title":"removeSensor <pre><code>void removeSensor(\n  Sensor sensor)\n</code></pre>"},{"location":"metrics/StreamsMetrics/#implementations","text":"<ul> <li>StreamsMetricsImpl</li> </ul>","title":"Implementations"},{"location":"metrics/StreamsMetricsImpl/","text":"<p><code>StreamsMetricsImpl</code> is a concrete StreamsMetrics.</p>","title":"StreamsMetricsImpl"},{"location":"metrics/StreamsMetricsImpl/#creating-instance","text":"<p><code>StreamsMetricsImpl</code> takes the following to be created:</p> <ul> <li> <code>Metrics</code> (Apache Kafka) <li> Client ID <li> Built-in metrics version <li> <code>Time</code>  <p><code>StreamsMetricsImpl</code> is created when:</p> <ul> <li><code>KafkaStreams</code> is created</li> <li><code>TopologyTestDriver</code> is requested to setupMetrics</li> </ul>","title":"Creating Instance"},{"location":"metrics/StreamsMetricsImpl/#rocksdbmetricsrecordingtrigger","text":"","title":"RocksDBMetricsRecordingTrigger <p><code>StreamsMetricsImpl</code> creates a <code>RocksDBMetricsRecordingTrigger</code> when created.</p> <p>The <code>RocksDBMetricsRecordingTrigger</code> is used when:</p> <ul> <li><code>KafkaStreams</code> is requested to start</li> <li><code>RocksDBMetricsRecorder</code> is requested to <code>addValueProviders</code> and <code>removeValueProviders</code></li> </ul>"},{"location":"metrics/StreamsMetricsImpl/#tasklevelsensor","text":"","title":"taskLevelSensor <pre><code>Sensor taskLevelSensor(\n  String threadId,\n  String taskId,\n  String sensorName,\n  RecordingLevel recordingLevel,\n  Sensor... parents)\n</code></pre> <p><code>taskLevelSensor</code> creates a taskSensorPrefix (with the given <code>threadId</code> and <code>taskId</code> identifiers) and getSensors.</p> <p><code>taskLevelSensor</code> is used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"metrics/StreamsMetricsImpl/#getsensors","text":"","title":"getSensors <pre><code>Sensor getSensors(\n  Map&lt;String, Deque&lt;String&gt;&gt; sensors,\n  String sensorName,\n  String key,\n  RecordingLevel recordingLevel,\n  Sensor... parents)\n</code></pre> <p><code>getSensors</code>...FIXME</p> <p><code>getSensors</code> is used when:</p> <ul> <li><code>StreamsMetricsImpl</code> is requested to threadLevelSensor, taskLevelSensor, nodeLevelSensor, cacheLevelSensor, storeLevelSensor</li> </ul>"},{"location":"metrics/StreamsMetricsImpl/#threadlevelsensors","text":"","title":"threadLevelSensors <p><code>StreamsMetricsImpl</code> defines <code>threadLevelSensors</code> registry of sensors (names) per thread (id) for threadLevelSensor.</p> <p>Sensors are removed in removeAllThreadLevelSensors.</p>"},{"location":"metrics/TaskMetrics/","text":"","title":"TaskMetrics"},{"location":"metrics/TaskMetrics/#recordlatenesssensor","text":"","title":"recordLatenessSensor <pre><code>Sensor recordLatenessSensor(\n  String threadId,\n  String taskId,\n  StreamsMetricsImpl streamsMetrics)\n</code></pre> <p><code>recordLatenessSensor</code> creates avg and max metric sensors (to measure record lateness)   with the following:</p>    Metric Name Avg Description Max Description RecordingLevel     <code>record-lateness</code> The observed average lateness of records in milliseconds, measured by comparing the record timestamp with the current stream time The observed maximum lateness of records in milliseconds, measured by comparing the record timestamp with the current stream time <code>DEBUG</code>    <p><code>recordLatenessSensor</code> is used when:</p> <ul> <li><code>StreamTask</code> is created (and creates a PartitionGroup)</li> </ul>"},{"location":"metrics/TaskMetrics/#avgandmaxsensor","text":"","title":"avgAndMaxSensor <pre><code>Sensor avgAndMaxSensor(\n  String threadId,\n  String taskId,\n  String metricName,\n  String descriptionOfAvg,\n  String descriptionOfMax,\n  RecordingLevel recordingLevel,\n  StreamsMetricsImpl streamsMetrics,\n  Sensor... parentSensors)\n</code></pre> <p><code>avgAndMaxSensor</code> requests the given StreamsMetricsImpl to create a taskLevelSensor and a taskLevelTagMap.</p> <p><code>avgAndMaxSensor</code> addAvgAndMaxToSensor.</p> <p><code>avgAndMaxSensor</code> is used when:</p> <ul> <li><code>TaskMetrics</code> is requested for processLatency and recordLateness sensors</li> </ul>"},{"location":"metrics/TaskMetrics/#addavgandmaxtosensor","text":"","title":"addAvgAndMaxToSensor <pre><code>void addAvgAndMaxToSensor(\n  Sensor sensor,\n  String group,\n  Map&lt;String, String&gt; tags,\n  String operation,\n  String descriptionOfAvg,\n  String descriptionOfMax)\n</code></pre> <p><code>addAvgAndMaxToSensor</code>...FIXME</p> <p><code>addAvgAndMaxToSensor</code> is used when:</p> <ul> <li><code>StreamsMetricsImpl</code> is requested to addLatencyRateTotalSensor and addAvgAndMinAndMaxToSensor</li> <li><code>TaskMetrics</code> is requested to avgAndMaxSensor and invocationRateAndCountAndAvgAndMaxLatencySensor</li> <li><code>ThreadMetrics</code> is requested to processLatencySensor, pollRecordsSensor, processRecordsSensor, invocationRateAndCountAndAvgAndMaxLatencySensor</li> <li><code>StateStoreMetrics</code> is requested to prefixScanSensor, sizeOrCountSensor, throughputAndLatencySensor</li> </ul>"},{"location":"metrics/ThreadMetrics/","text":"<p><code>ThreadMetrics</code> is...FIXME</p>","title":"ThreadMetrics"},{"location":"processor/","text":"<p>Processor API is a low-level API for developers to define topologies in Kafka Streams (mostly when High-Level Streams DSL would not meet expectations).</p> <p>Processor API comes with the following low-level stream processing abstractions:</p> <ul> <li>Processor</li> <li>ProcessorContext</li> <li>ProcessorSupplier</li> <li>Punctuator</li> </ul>","title":"Processor API"},{"location":"processor/AbstractProcessor/","text":"<p><code>AbstractProcessor</code> is...FIXME</p>","title":"AbstractProcessor"},{"location":"processor/AbstractProcessorContext/","text":"<p><code>AbstractProcessorContext&lt;KOut, VOut&gt;</code> is an extension of the InternalProcessorContext abstraction for ProcessorContexts with an associated StateManager.</p>","title":"AbstractProcessorContext"},{"location":"processor/AbstractProcessorContext/#contract","text":"","title":"Contract"},{"location":"processor/AbstractProcessorContext/#statemanager","text":"","title":"StateManager <pre><code>StateManager stateManager()\n</code></pre> <p>StateManager</p> <p>Used when:</p> <ul> <li><code>AbstractProcessorContext</code> is requested to stateDir, register a StateStore, taskType, changelogFor</li> <li><code>ProcessorContextImpl</code> is requested to logChange</li> </ul>"},{"location":"processor/AbstractProcessorContext/#implementations","text":"<ul> <li>GlobalProcessorContextImpl</li> <li>ProcessorContextImpl</li> </ul>","title":"Implementations"},{"location":"processor/AbstractProcessorContext/#creating-instance","text":"<p><code>AbstractProcessorContext</code> takes the following to be created:</p> <ul> <li> TaskId <li> StreamsConfig <li> StreamsMetricsImpl <li> ThreadCache   <p>Abstract Class</p> <p><code>AbstractProcessorContext</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete AbstractProcessorContexts.</p>","title":"Creating Instance"},{"location":"processor/AbstractProcessorContext/#current-processornode","text":"","title":"Current ProcessorNode <p><code>AbstractProcessorContext</code> defines <code>currentNode</code> internal registry for a ProcessorNode (that is required by InternalProcessorContext abstraction for currentNode).</p>"},{"location":"processor/ConnectedStoreProvider/","text":"<p><code>ConnectedStoreProvider</code> is an abstraction of providers of StoreBuilders</p>","title":"ConnectedStoreProvider"},{"location":"processor/ConnectedStoreProvider/#contract","text":"","title":"Contract"},{"location":"processor/ConnectedStoreProvider/#stores","text":"","title":"stores <pre><code>Set&lt;StoreBuilder&lt;?&gt;&gt; stores()\n</code></pre> <p>Default: <code>null</code> (uninitialized and so undefined)</p> <p>Used when:</p> <ul> <li><code>AbstractStream</code> is requested to toValueTransformerWithKeySupplier</li> <li><code>Topology</code> is requested to addProcessor</li> <li><code>StatefulProcessorNode</code> is requested to writeToTopology</li> <li>others</li> </ul>"},{"location":"processor/ConnectedStoreProvider/#implementations","text":"<ul> <li>ProcessorSupplier</li> <li>TransformerSupplier</li> <li><code>ValueTransformerSupplier</code></li> <li><code>ValueTransformerWithKeySupplier</code></li> </ul>","title":"Implementations"},{"location":"processor/ContextualProcessor/","text":"<p><code>ContextualProcessor</code> is...FIXME</p>","title":"ContextualProcessor"},{"location":"processor/ExtractRecordMetadataTimestamp/","text":"<p><code>ExtractRecordMetadataTimestamp</code> is an extension of the TimestampExtractor abstraction for timestamp extractors that use ConsumerRecords for timestamps and can handle invalid (negative) timestamps.</p>","title":"ExtractRecordMetadataTimestamp"},{"location":"processor/ExtractRecordMetadataTimestamp/#contract","text":"","title":"Contract"},{"location":"processor/ExtractRecordMetadataTimestamp/#handling-invalid-negative-timestamp","text":"","title":"Handling Invalid (Negative) Timestamp <pre><code>long onInvalidTimestamp(\n  ConsumerRecord&lt;Object, Object&gt; record,\n  long recordTimestamp,\n  long partitionTime)\n</code></pre> <p>Used when:</p> <ul> <li><code>ExtractRecordMetadataTimestamp</code> is requested to extract a timestamp</li> </ul>"},{"location":"processor/ExtractRecordMetadataTimestamp/#implementations","text":"<ul> <li><code>FailOnInvalidTimestamp</code></li> <li><code>LogAndSkipOnInvalidTimestamp</code></li> <li><code>UsePartitionTimeOnInvalidTimestamp</code></li> </ul>","title":"Implementations"},{"location":"processor/ExtractRecordMetadataTimestamp/#extracting-timestamp","text":"","title":"Extracting Timestamp <pre><code>long extract(\n  ConsumerRecord&lt;Object, Object&gt; record,\n  long partitionTime)\n</code></pre> <p><code>extract</code> requests the given <code>ConsumerRecord</code> for the <code>timestamp</code>.</p> <p>In case the (extracted) timestamp is negative, <code>extract</code> onInvalidTimestamp.</p> <p><code>extract</code> is part of the TimestampExtractor abstraction.</p>"},{"location":"processor/FailOnInvalidTimestamp/","text":"<p><code>FailOnInvalidTimestamp</code> is...FIXME</p>","title":"FailOnInvalidTimestamp"},{"location":"processor/GlobalProcessorContextImpl/","text":"<p><code>GlobalProcessorContextImpl</code> is...FIXME</p>","title":"GlobalProcessorContextImpl"},{"location":"processor/GlobalStateManager/","text":"<p><code>GlobalStateManager</code> is...FIXME</p>","title":"GlobalStateManager"},{"location":"processor/GlobalStateManagerImpl/","text":"<p><code>GlobalStateManagerImpl</code> is...FIXME</p>","title":"GlobalStateManagerImpl"},{"location":"processor/GlobalStateUpdateTask/","text":"<p><code>GlobalStateUpdateTask</code> is...FIXME</p>","title":"GlobalStateUpdateTask"},{"location":"processor/GlobalStreamThread/","text":"<p><code>GlobalStreamThread</code> is...FIXME</p>","title":"GlobalStreamThread"},{"location":"processor/InternalProcessorContext/","text":"<p><code>InternalProcessorContext</code> is an extension of ProcessorContext and <code>StateStoreContext</code> abstractions.</p>","title":"InternalProcessorContext"},{"location":"processor/InternalProcessorContext/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"processor/InternalProcessorContext/#currentnode","text":"","title":"currentNode <pre><code>ProcessorNode&lt;?, ?, ?, ?&gt; currentNode()\n</code></pre> <p>Current ProcessorNode (as set by setCurrentNode)</p>"},{"location":"processor/InternalProcessorContext/#initialize","text":"","title":"initialize <pre><code>void initialize()\n</code></pre> <p>Used when:</p> <ul> <li><code>GlobalStateUpdateTask</code> is requested to initialize</li> <li><code>StandbyTask</code> is requested to initializeIfNeeded</li> <li><code>StreamTask</code> is requested to completeRestoration</li> </ul>"},{"location":"processor/InternalProcessorContext/#setcurrentnode","text":"","title":"setCurrentNode <pre><code>void setCurrentNode(\n  ProcessorNode&lt;?, ?, ?, ?&gt; currentNode)\n</code></pre> <p>Sets the given ProcessorNode as the current node</p>"},{"location":"processor/InternalProcessorContext/#implementations","text":"<ul> <li>AbstractProcessorContext</li> </ul>","title":"Implementations"},{"location":"processor/NodeFactory/","text":"<p><code>NodeFactory&lt;KIn, VIn, KOut, VOut&gt;</code> is an abstraction of named node factories (with predecessors).</p> <p><code>NodeFactory</code> is a <code>private static abstract class</code> of InternalTopologyBuilder.</p>","title":"NodeFactory"},{"location":"processor/NodeFactory/#contract","text":"","title":"Contract"},{"location":"processor/NodeFactory/#build","text":"","title":"build <pre><code>ProcessorNode&lt;KIn, VIn, KOut, VOut&gt; build()\n</code></pre> <p>Used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to build a ProcessorTopology</li> </ul>"},{"location":"processor/NodeFactory/#describe","text":"","title":"describe <pre><code>AbstractNode describe()\n</code></pre> <p>Used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to describeSubtopology</li> </ul>"},{"location":"processor/NodeFactory/#implementations","text":"<ul> <li>ProcessorNodeFactory</li> <li>SinkNodeFactory</li> <li>SourceNodeFactory</li> </ul>","title":"Implementations"},{"location":"processor/NodeFactory/#creating-instance","text":"<p><code>NodeFactory</code> takes the following to be created:</p> <ul> <li> Name <li> Predecessors   <p>Abstract Class</p> <p><code>NodeFactory</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete NodeFactory'ies.</p>","title":"Creating Instance"},{"location":"processor/Processor/","text":"<p><code>Processor&lt;KIn, VIn, KOut, VOut&gt;</code> is an abstraction of processing nodes (in a stream processing topology).</p>","title":"Processor"},{"location":"processor/Processor/#contract","text":"","title":"Contract"},{"location":"processor/Processor/#close","text":"","title":"close <pre><code>void close()\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorNode</code> is requested to close</li> </ul>"},{"location":"processor/Processor/#init","text":"","title":"init <pre><code>void init(\n  ProcessorContext&lt;KOut, VOut&gt; context)\n</code></pre> <p>Initializes this <code>Processor</code> (passing in a ProcessorContext)</p> <p>Used when:</p> <ul> <li><code>ProcessorNode</code> is requested to init</li> </ul>"},{"location":"processor/Processor/#process","text":"","title":"process <pre><code>void process(\n  Record&lt;KIn, VIn&gt; record)\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorNode</code> is requested to process</li> </ul>"},{"location":"processor/Processor/#implementations","text":"<ul> <li><code>ContextualProcessor</code></li> <li><code>ForeachProcessor</code></li> </ul>","title":"Implementations"},{"location":"processor/ProcessorContext/","text":"<p><code>ProcessorContext&lt;KForward, VForward&gt;</code> is an abstraction of contexts for Processors.</p>","title":"ProcessorContext"},{"location":"processor/ProcessorContext/#contract-subset","text":"","title":"Contract (Subset)"},{"location":"processor/ProcessorContext/#forwarding-record","text":"","title":"Forwarding Record <pre><code>void forward(\n  Record&lt;K, V&gt; record)\nvoid forward(\n  Record&lt;K, V&gt; record,\n  String childName)\n</code></pre> <p>Forwards a <code>Record</code> (to all or the specified child processor)</p> <p>See ProcessorContextImpl</p>"},{"location":"processor/ProcessorContext/#scheduling-recurring-action","text":"","title":"Scheduling Recurring Action <pre><code>Cancellable schedule(\n  Duration interval,\n  PunctuationType type,\n  Punctuator callback)\n</code></pre> <p>Schedules an recurring action (Punctuator) to be executed every <code>interval</code> ms</p> <p>See ProcessorContextImpl</p>"},{"location":"processor/ProcessorContext/#implementations","text":"<ul> <li>AbstractProcessorContext</li> <li>InternalProcessorContext</li> </ul>","title":"Implementations"},{"location":"processor/ProcessorContextImpl/","text":"<p><code>ProcessorContextImpl</code> is an AbstractProcessorContext.</p>","title":"ProcessorContextImpl"},{"location":"processor/ProcessorContextImpl/#creating-instance","text":"<p><code>ProcessorContextImpl</code> takes the following to be created:</p> <ul> <li> TaskId <li> StreamsConfig <li>ProcessorStateManager</li> <li> StreamsMetricsImpl <li> ThreadCache  <p><code>ProcessorContextImpl</code> is created when:</p> <ul> <li><code>ActiveTaskCreator</code> is requested to createTasks</li> <li><code>StandbyTaskCreator</code> is requested to createTasks</li> </ul>","title":"Creating Instance"},{"location":"processor/ProcessorContextImpl/#processorstatemanager","text":"","title":"ProcessorStateManager <p><code>ProcessorContextImpl</code> is given a ProcessorStateManager when created.</p>"},{"location":"processor/ProcessorContextImpl/#scheduling-recurring-action","text":"","title":"Scheduling Recurring Action <pre><code>Cancellable schedule(\n  Duration interval,\n  PunctuationType type,\n  Punctuator callback)\n</code></pre> <p><code>schedule</code> converts the <code>interval</code> to milliseconds and requests the StreamTask to schedule the given Punctuator.</p> <p><code>schedule</code> makes sure that the <code>interval</code> is at least 1 ms or throws an <code>IllegalArgumentException</code>:</p> <pre><code>The minimum supported scheduling interval is 1 millisecond.\n</code></pre> <p><code>schedule</code> is part of the ProcessorContext abstraction.</p>"},{"location":"processor/ProcessorContextImpl/#forwarding-record-downstream","text":"","title":"Forwarding Record Downstream <p><code>ProcessorContextImpl</code> is associated with a ProcessorNode known as the current node.</p> <p>forward (and forwardInternal in particular) uses the current ProcessorNode to process a given record.</p> <p>ProcessorNodes are associated with child <code>ProcessorNode</code>s known as children.</p>"},{"location":"processor/ProcessorContextImpl/#forward","text":"","title":"forward <pre><code>void forward(\n  K key,\n  V value)\nvoid forward(\n  K key,\n  V value,\n  To to)\nvoid forward(\n  Record&lt;K, V&gt; record)\nvoid forward(\n  Record&lt;K, V&gt; record, \n  String childName)\n</code></pre> <p><code>forward</code> forwardInternal to the child or all the children nodes of the current ProcessorNode.</p>  <p><code>forward</code> throws an <code>UnsupportedOperationException</code> for a <code>TaskType.STANDBY</code> task:</p> <pre><code>this should not happen: forward() is not supported in standby tasks.\n</code></pre>  <p><code>forward</code> is part of the ProcessorContext abstraction.</p>"},{"location":"processor/ProcessorContextImpl/#forwardinternal","text":"","title":"forwardInternal <pre><code>void forwardInternal(\n  ProcessorNode&lt;K, V, ?, ?&gt; child,\n  Record&lt;K, V&gt; record)\n</code></pre> <p><code>forwardInternal</code> sets the current node to be the given child ProcessorNode that is in turn requested to process the record.</p> <p>If the child node is terminal (no children), <code>forwardInternal</code> requests the StreamTask to maybeRecordE2ELatency.</p>"},{"location":"processor/ProcessorNode/","text":"<p><code>ProcessorNode</code> is a \"hosting environment\" of a Processor in a processor topology.</p>","title":"ProcessorNode"},{"location":"processor/ProcessorNode/#creating-instance","text":"<p><code>ProcessorNode</code> takes the following to be created:</p> <ul> <li> Name <li> Processor <li> Names of the state stores  <p><code>ProcessorNode</code> is created\u00a0when:</p> <ul> <li><code>ProcessorNodeFactory</code> is requested to build a processor</li> </ul>","title":"Creating Instance"},{"location":"processor/ProcessorNode/#specialized-processornodes","text":"<p>SourceNode and SinkNode are specialized <code>ProcessorNode</code>s.</p>","title":"Specialized ProcessorNodes"},{"location":"processor/ProcessorNode/#child-processornodes","text":"","title":"Child ProcessorNodes <p><code>ProcessorNode</code> defines <code>children</code> internal registry of child <code>ProcessorNode</code>s.</p> <p>The <code>children</code> is empty when <code>ProcessorNode</code> is created.</p> <p>A new <code>ProcessorNode</code> is added in addChild.</p>"},{"location":"processor/ProcessorNode/#addchild","text":"","title":"addChild <pre><code>void addChild(\n  ProcessorNode&lt;KOut, VOut, ?, ?&gt; child)\n</code></pre> <p><code>addChild</code> adds a new <code>ProcessorNode</code> to the children and the childByName internal registries.</p> <p><code>addChild</code> is used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to buildSinkNode and buildProcessorNode</li> </ul>"},{"location":"processor/ProcessorNode/#children","text":"","title":"children <pre><code>List&lt;ProcessorNode&lt;KOut, VOut, ?, ?&gt;&gt; children()\n</code></pre> <p><code>children</code> is used when:</p> <ul> <li><code>GlobalProcessorContextImpl</code> is requested to forward a record</li> <li><code>ProcessorContextImpl</code> is requested to forward a record</li> <li><code>ProcessorTopology</code> is requested for text representation</li> </ul>"},{"location":"processor/ProcessorNode/#terminal-node","text":"","title":"Terminal Node <p><code>ProcessorNode</code> is terminal when has got no children.</p>"},{"location":"processor/ProcessorNode/#punctuate","text":"","title":"punctuate <pre><code>void punctuate(\n  long timestamp, \n  Punctuator punctuator)\n</code></pre> <p><code>punctuate</code> requests the given Punctuator to punctuate (with the given <code>timestamp</code>).</p> <p><code>punctuate</code> is used when:</p> <ul> <li><code>StreamTask</code> is requested to punctuate</li> </ul>"},{"location":"processor/ProcessorNodeFactory/","text":"<p><code>ProcessorNodeFactory</code> is a NodeFactory.</p>","title":"ProcessorNodeFactory"},{"location":"processor/ProcessorNodeFactory/#creating-instance","text":"<p><code>ProcessorNodeFactory</code> takes the following to be created:</p> <ul> <li> Name <li> Predecessor Nodes <li> ProcessorSupplier  <p><code>ProcessorNodeFactory</code> is created\u00a0when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to addProcessor and addGlobalStore</li> </ul>","title":"Creating Instance"},{"location":"processor/ProcessorNodePunctuator/","text":"<p><code>ProcessorNodePunctuator</code> is an abstraction of processor punctuators (that PunctuationQueues use to mayPunctuate).</p>","title":"ProcessorNodePunctuator"},{"location":"processor/ProcessorNodePunctuator/#contract","text":"","title":"Contract"},{"location":"processor/ProcessorNodePunctuator/#punctuate","text":"","title":"punctuate <pre><code>void punctuate(\n  ProcessorNode&lt;?, ?, ?, ?&gt; node,\n  long timestamp,\n  PunctuationType type,\n  Punctuator punctuator)\n</code></pre> <p>Used when:</p> <ul> <li><code>PunctuationQueue</code> is requested to mayPunctuate</li> </ul>"},{"location":"processor/ProcessorNodePunctuator/#implementations","text":"<ul> <li>StreamTask</li> </ul>","title":"Implementations"},{"location":"processor/ProcessorSupplier/","text":"<p><code>ProcessorSupplier</code>\u00a0is an extension of the ConnectedStoreProvider and <code>Supplier</code> (Java) abstractions for processor suppliers (factories).</p> <p><code>ProcessorSupplier</code>\u00a0is marked with <code>FunctionalInterface</code> (Java) annotation.</p> <p>The design idea of <code>ProcessorSupplier</code> is to be a Single Abstract Method (SAM) interface and let lambda expressions make code easier to type (and hopefully to read and maintain, too).</p>","title":"ProcessorSupplier"},{"location":"processor/ProcessorSupplier/#contract","text":"","title":"Contract"},{"location":"processor/ProcessorSupplier/#creating-processor","text":"","title":"Creating Processor <pre><code>Processor&lt;KIn, VIn, KOut, VOut&gt; get()\n</code></pre> <p>Creates a Processor</p>"},{"location":"processor/ProcessorTopology/","text":"","title":"ProcessorTopology"},{"location":"processor/ProcessorTopology/#creating-instance","text":"<p><code>ProcessorTopology</code> takes the following to be created:</p> <ul> <li> ProcessorNodes <li> SourceNodes by topic <li> SinkNodes by topic <li> StateStores <li> Global StateStores <li> Store names by topic <li> Repartition topics  <p><code>ProcessorTopology</code> is created\u00a0when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to build a ProcessorTopology</li> </ul>","title":"Creating Instance"},{"location":"processor/ProcessorTopology/#text-representation","text":"","title":"Text Representation <pre><code>String toString() // (1)\nString toString(\n  String indent)\n</code></pre> <ol> <li>Uses an empty indent (to start recursion)</li> </ol> <p><code>toString</code>...FIXME</p> <p><code>toString</code> is part of the <code>Object</code> (Java) abstraction.</p>"},{"location":"processor/ProcessorTopology/#childrentostring","text":"","title":"childrenToString <pre><code>String childrenToString(\n  String indent, \n  List&lt;? extends ProcessorNode&lt;?, ?, ?, ?&gt;&gt; children)\n</code></pre> <p><code>childrenToString</code>...FIXME</p>"},{"location":"processor/Punctuator/","text":"<p><code>Punctuator</code> is a functional abstraction of recurring actions.</p>","title":"Punctuator"},{"location":"processor/Punctuator/#contract","text":"","title":"Contract"},{"location":"processor/Punctuator/#punctuate","text":"","title":"punctuate <pre><code>void punctuate(\n  long timestamp)\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorNode</code> is requested to punctuate</li> </ul>"},{"location":"processor/SinkNode/","text":"<p><code>SinkNode</code> is...FIXME</p>","title":"SinkNode"},{"location":"processor/SinkNodeFactory/","text":"<p><code>SinkNodeFactory</code> is...FIXME</p>","title":"SinkNodeFactory"},{"location":"processor/SourceNode/","text":"<p><code>SourceNode</code> is a ProcessorNode.</p>","title":"SourceNode"},{"location":"processor/SourceNode/#creating-instance","text":"<p><code>SourceNode</code> takes the following to be created:</p> <ul> <li> Node Name <li> TimestampExtractor <li> Key <code>Deserializer</code> <li> Value <code>Deserializer</code>  <p><code>SourceNode</code> is created when:</p> <ul> <li><code>SourceNodeFactory</code> is requested to build a ProcessorNode</li> </ul>","title":"Creating Instance"},{"location":"processor/SourceNodeFactory/","text":"<p><code>SourceNodeFactory</code> is a NodeFactory with no predecessors.</p>","title":"SourceNodeFactory"},{"location":"processor/SourceNodeFactory/#creating-instance","text":"<p><code>SourceNodeFactory</code> takes the following to be created:</p> <ul> <li> Node Name <li> Topics <li> Topic Pattern <li> TimestampExtractor <li> Key <code>Deserializer</code> <li> Value <code>Deserializer</code>  <p><code>SourceNodeFactory</code> is created when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to addSource and addGlobalStore</li> </ul>","title":"Creating Instance"},{"location":"processor/StateStore/","text":"<p><code>StateStore</code> is an abstraction of storage engines (for the state of a stream processor).</p>","title":"StateStore"},{"location":"processor/StateStore/#contract","text":"","title":"Contract"},{"location":"processor/StateStore/#closing","text":"","title":"Closing <pre><code>void close()\n</code></pre>"},{"location":"processor/StateStore/#flushing-cached-data","text":"","title":"Flushing Cached Data <pre><code>void flush()\n</code></pre> <p>Flushing cached data of this <code>StateStore</code></p> <p>Used when:</p> <ul> <li><code>GlobalStateManagerImpl</code> is requested to flush all global state stores</li> <li><code>ProcessorStateManager</code> is requested to flush all state stores and flushCache</li> </ul>"},{"location":"processor/StateStore/#initializing","text":"","title":"Initializing <pre><code>void init(\n  StateStoreContext context,\n  StateStore root)\n</code></pre>"},{"location":"processor/StateStore/#isopen","text":"","title":"isOpen <pre><code>boolean isOpen()\n</code></pre>"},{"location":"processor/StateStore/#name","text":"","title":"Name <pre><code>String name()\n</code></pre>"},{"location":"processor/StateStore/#persistent","text":"","title":"persistent <pre><code>boolean persistent()\n</code></pre> <p>Used when:</p> <ul> <li><code>GlobalStateManagerImpl</code> is created (and finds global non-persistent state stores)</li> <li><code>ProcessorStateManager</code> is requested to initializeStoreOffsetsFromCheckpoint and checkpoint</li> <li><code>ProcessorTopology</code> is requested to hasPersistentLocalStore and hasPersistentGlobalStore</li> <li><code>TimestampedKeyValueStoreBuilder</code> is requested to build a <code>TimestampedKeyValueStore</code></li> <li><code>TimestampedWindowStoreBuilder</code> is requested to build a <code>TimestampedWindowStore</code></li> <li>others</li> </ul>"},{"location":"processor/StateStore/#implementations","text":"<ul> <li>KeyValueStore</li> <li><code>SegmentedBytesStore</code></li> <li><code>SessionStore</code></li> <li><code>TimeOrderedKeyValueBuffer</code></li> <li>WindowStore</li> <li>WrappedStateStore</li> </ul>","title":"Implementations"},{"location":"processor/StateStoreFactory/","text":"<p><code>StateStoreFactory</code> is a factory of StateStores.</p> <pre><code>StateStoreFactory&lt;S extends StateStore&gt;\n</code></pre> <p><code>StateStoreFactory</code> is a <code>public static class</code> of InternalTopologyBuilder.</p>","title":"StateStoreFactory"},{"location":"processor/StateStoreFactory/#creating-instance","text":"<p><code>StateStoreFactory</code> takes the following to be created:</p> <ul> <li> StoreBuilder (of <code>S</code> StateStores)  <p><code>StateStoreFactory</code> is created\u00a0when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to addStateStore</li> </ul>","title":"Creating Instance"},{"location":"processor/StoreChangelogReader/","text":"<p><code>StoreChangelogReader</code> is...FIXME</p>","title":"StoreChangelogReader"},{"location":"processor/StreamPartitioner/","text":"<p><code>StreamPartitioner&lt;K, V&gt;</code> is an abstraction of partitioners that determine how records are distributed among the partitions in a Kafka topic.</p>","title":"StreamPartitioner"},{"location":"processor/StreamPartitioner/#contract","text":"","title":"Contract"},{"location":"processor/StreamPartitioner/#partition","text":"","title":"partition <pre><code>Integer partition(\n  String topic,\n  K key,\n  V value,\n  int numPartitions)\n</code></pre> <p>Used when:</p> <ul> <li><code>RecordCollectorImpl</code> is requested to send a record</li> <li><code>StreamsMetadataState</code> is requested to getKeyQueryMetadataForKey</li> </ul>"},{"location":"processor/StreamPartitioner/#implementations","text":"<ul> <li><code>DefaultStreamPartitioner</code></li> <li><code>WindowedStreamPartitioner</code></li> </ul>","title":"Implementations"},{"location":"processor/TimestampExtractor/","text":"<p><code>TimestampExtractor</code> is an abstraction of time extractors that Kafka Streams uses to extract a timestamp from a record.</p> <p><code>TimestampExtractor</code> can be configured as follows:</p> <ol> <li>For SourceNodes using Topology.addSource and Topology.addGlobalStore</li> <li>Application-wide using default.timestamp.extractor configuration property</li> </ol>","title":"TimestampExtractor"},{"location":"processor/TimestampExtractor/#contract","text":"","title":"Contract"},{"location":"processor/TimestampExtractor/#extracting-timestamp","text":"","title":"Extracting Timestamp <pre><code>long extract(\n  ConsumerRecord&lt;Object, Object&gt; record,\n  long partitionTime)\n</code></pre> <p>Extracts a timestamp from the given <code>ConsumerRecord</code> (Apache Kafka)</p> <p>Used when:</p> <ul> <li><code>RecordQueue</code> is requested to update the head record</li> </ul>"},{"location":"processor/TimestampExtractor/#implementations","text":"<ul> <li>ExtractRecordMetadataTimestamp</li> <li><code>WallclockTimestampExtractor</code></li> </ul>","title":"Implementations"},{"location":"processor/TransformerSupplier/","text":"<p><code>TransformerSupplier</code> is...FIXME</p>","title":"TransformerSupplier"},{"location":"state/AbstractStoreBuilder/","text":"<p><code>AbstractStoreBuilder</code>\u00a0is a base abstraction of the StoreBuilder abstraction.</p> <pre><code>AbstractStoreBuilder&lt;K, V, T extends StateStore&gt;\n</code></pre>","title":"AbstractStoreBuilder"},{"location":"state/AbstractStoreBuilder/#implementations","text":"<ul> <li>KeyValueStoreBuilder</li> <li>SessionStoreBuilder</li> <li>TimeOrderedWindowStoreBuilder</li> <li>TimestampedKeyValueStoreBuilder</li> <li>TimestampedWindowStoreBuilder</li> <li>WindowStoreBuilder</li> </ul>","title":"Implementations"},{"location":"state/AbstractStoreBuilder/#creating-instance","text":"<p><code>AbstractStoreBuilder</code> takes the following to be created:</p> <ul> <li> Name <li> Key <code>Serde&lt;K&gt;</code> <li> Value <code>Serde&lt;V&gt;</code> <li> <code>Time</code>   <p>Abstract Class</p> <p><code>AbstractStoreBuilder</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete AbstractStoreBuilders.</p>","title":"Creating Instance"},{"location":"state/CachedStateStore/","text":"<p><code>CachedStateStore</code> is an abstraction of cached state stores.</p>","title":"CachedStateStore"},{"location":"state/CachedStateStore/#contract","text":"","title":"Contract"},{"location":"state/CachedStateStore/#flushcache","text":"","title":"flushCache <pre><code>void flushCache()\n</code></pre> <p>Used when:</p> <ul> <li><code>ProcessorStateManager</code> is requested to flush store caches</li> </ul>"},{"location":"state/CachedStateStore/#setflushlistener","text":"","title":"setFlushListener <pre><code>boolean setFlushListener(\n  CacheFlushListener&lt;K, V&gt; listener,\n  boolean sendOldValues)\n</code></pre>"},{"location":"state/CachedStateStore/#implementations","text":"<ul> <li><code>CachingKeyValueStore</code></li> <li><code>CachingSessionStore</code></li> <li><code>CachingWindowStore</code></li> <li>WrappedStateStore</li> </ul>","title":"Implementations"},{"location":"state/ChangeLoggingKeyValueBytesStore/","text":"<p><code>ChangeLoggingKeyValueBytesStore</code> is...FIXME</p>","title":"ChangeLoggingKeyValueBytesStore"},{"location":"state/InMemoryWindowBytesStoreSupplier/","text":"<p><code>InMemoryWindowBytesStoreSupplier</code> is a WindowBytesStoreSupplier.</p>","title":"InMemoryWindowBytesStoreSupplier"},{"location":"state/InMemoryWindowBytesStoreSupplier/#creating-instance","text":"<p><code>InMemoryWindowBytesStoreSupplier</code> takes the following to be created:</p> <ul> <li> Name <li> <code>retentionPeriod</code> <li> <code>windowSize</code> <li> <code>retainDuplicates</code>  <p><code>InMemoryWindowBytesStoreSupplier</code> is created\u00a0when:</p> <ul> <li><code>Stores</code> is requested for in-memory window store</li> </ul>","title":"Creating Instance"},{"location":"state/KeyValueBytesStoreSupplier/","text":"<p><code>KeyValueBytesStoreSupplier</code> is...FIXME</p>","title":"KeyValueBytesStoreSupplier"},{"location":"state/KeyValueStore/","text":"<p><code>KeyValueStore&lt;K, V&gt;</code>\u00a0is an extension of the StateStore and ReadOnlyKeyValueStore abstractions for read-only key-value stores.</p>","title":"KeyValueStore"},{"location":"state/KeyValueStore/#contract","text":"","title":"Contract"},{"location":"state/KeyValueStore/#delete","text":"","title":"delete <pre><code>V delete(\n  K key)\n</code></pre>"},{"location":"state/KeyValueStore/#put","text":"","title":"put <pre><code>void put(\n  K key,\n  V value)\n</code></pre>"},{"location":"state/KeyValueStore/#putall","text":"","title":"putAll <pre><code>void putAll(\n  List&lt;KeyValue&lt;K, V&gt;&gt; entries)\n</code></pre>"},{"location":"state/KeyValueStore/#putifabsent","text":"","title":"putIfAbsent <pre><code>V putIfAbsent(\n  K key,\n  V value)\n</code></pre>"},{"location":"state/KeyValueStore/#implementations","text":"<ul> <li><code>CachingKeyValueStore</code></li> <li><code>ChangeLoggingKeyValueBytesStore</code></li> <li><code>InMemoryKeyValueStore</code></li> <li><code>InMemoryTimestampedKeyValueStoreMarker</code></li> <li><code>KeyValueStoreFacade</code></li> <li><code>KeyValueStoreReadOnlyDecorator</code></li> <li><code>KeyValueStoreReadWriteDecorator</code></li> <li><code>KeyValueToTimestampedKeyValueByteStoreAdapter</code></li> <li><code>MemoryLRUCache</code></li> <li><code>MeteredKeyValueStore</code></li> <li><code>RocksDBStore</code></li> <li><code>Segment</code></li> <li><code>TimestampedKeyValueStore</code></li> </ul>","title":"Implementations"},{"location":"state/KeyValueStoreBuilder/","text":"<p><code>KeyValueStoreBuilder</code> is a concrete AbstractStoreBuilder of KeyValueStores.</p>","title":"KeyValueStoreBuilder"},{"location":"state/KeyValueStoreBuilder/#creating-instance","text":"<p><code>KeyValueStoreBuilder</code> takes the following to be created:</p> <ul> <li> KeyValueBytesStoreSupplier <li> Key <code>Serde&lt;K&gt;</code> <li> Value <code>Serde&lt;V&gt;</code> <li> <code>Time</code>  <p><code>KeyValueStoreBuilder</code> is created\u00a0when:</p> <ul> <li><code>Stores</code> utility is used to create a StoreBuilder of KeyValueStores</li> </ul>","title":"Creating Instance"},{"location":"state/KeyValueStoreBuilder/#building-statestore","text":"","title":"Building StateStore <pre><code>KeyValueStore&lt;K, V&gt; build()\n</code></pre> <p><code>build</code> creates a MeteredKeyValueStore.</p> <p><code>build</code>\u00a0is part of the StoreBuilder abstraction.</p>"},{"location":"state/MeteredKeyValueStore/","text":"<p><code>MeteredKeyValueStore</code> is...FIXME</p>","title":"MeteredKeyValueStore"},{"location":"state/OffsetCheckpoint/","text":"<p><code>OffsetCheckpoint</code> is...FIXME</p>","title":"OffsetCheckpoint"},{"location":"state/QueryableStoreProvider/","text":"<p><code>QueryableStoreProvider</code> is...FIXME</p>","title":"QueryableStoreProvider"},{"location":"state/ReadOnlyKeyValueStore/","text":"<p><code>ReadOnlyKeyValueStore</code> is...FIXME</p>","title":"ReadOnlyKeyValueStore"},{"location":"state/StateSerdes/","text":"<p><code>StateSerdes&lt;K, V&gt;</code> is a factory for creating serializers and deserializers for state stores in Kafka Streams.</p>","title":"StateSerdes"},{"location":"state/StateSerdes/#demo","text":"<pre><code>import org.apache.kafka.streams.state.StateSerdes\nimport java.lang.{Long =&gt; JLong}\nval stateSerdes = StateSerdes.withBuiltinTypes[JLong, String](\"topicName\", classOf[JLong], classOf[String])\n</code></pre> <pre><code>scala&gt; :type stateSerdes\norg.apache.kafka.streams.state.StateSerdes[Long,String]\n</code></pre>","title":"Demo"},{"location":"state/StateSerdes/#creating-instance","text":"<p><code>StateSerdes</code> takes the following to be created:</p> <ul> <li> Topic Name <li> Key <code>Serde</code> <li> Value <code>Serde</code>  <p><code>StateSerdes</code> is created\u00a0when:</p> <ul> <li><code>CachingWindowStore</code> is requested to <code>initInternal</code></li> <li><code>MeteredKeyValueStore</code> is requested to <code>initStoreSerde</code></li> <li><code>MeteredSessionStore</code> is requested to <code>initStoreSerde</code></li> <li><code>MeteredWindowStore</code> is requested to <code>initStoreSerde</code></li> <li>withBuiltinTypes</li> </ul>","title":"Creating Instance"},{"location":"state/StateSerdes/#withbuiltintypes","text":"","title":"withBuiltinTypes <pre><code>StateSerdes&lt;K, V&gt; withBuiltinTypes(\n  String topic,\n  Class&lt;K&gt; keyClass,\n  Class&lt;V&gt; valueClass)\n</code></pre> <p><code>withBuiltinTypes</code> creates a StateSerdes using <code>Serdes.serdeFrom</code> utility with the given key and value classes.</p>"},{"location":"state/StoreBuilder/","text":"<p><code>StoreBuilder</code> is an abstraction of builders of StateStores (with optional caching and logging).</p> <pre><code>StoreBuilder&lt;T extends StateStore&gt;\n</code></pre>","title":"StoreBuilder"},{"location":"state/StoreBuilder/#contract","text":"","title":"Contract"},{"location":"state/StoreBuilder/#building-statestore","text":"","title":"Building StateStore <pre><code>T build()\n</code></pre> <p>Used when:</p> <ul> <li><code>InternalTopologyBuilder</code> is requested to rewriteTopology (and build global state stores)</li> <li><code>StateStoreFactory</code> is requested to build a StateStore</li> </ul>"},{"location":"state/StoreBuilder/#logconfig","text":"","title":"logConfig <pre><code>Map&lt;String, String&gt; logConfig()\n</code></pre>"},{"location":"state/StoreBuilder/#loggingenabled","text":"","title":"loggingEnabled <pre><code>boolean loggingEnabled()\n</code></pre>"},{"location":"state/StoreBuilder/#name","text":"","title":"name <pre><code>String name()\n</code></pre>"},{"location":"state/StoreBuilder/#withcachingdisabled","text":"","title":"withCachingDisabled <pre><code>StoreBuilder&lt;T&gt; withCachingDisabled()\n</code></pre>"},{"location":"state/StoreBuilder/#withcachingenabled","text":"","title":"withCachingEnabled <pre><code>StoreBuilder&lt;T&gt; withCachingEnabled()\n</code></pre>"},{"location":"state/StoreBuilder/#withloggingdisabled","text":"","title":"withLoggingDisabled <pre><code>StoreBuilder&lt;T&gt; withLoggingDisabled()\n</code></pre>"},{"location":"state/StoreBuilder/#withloggingenabled","text":"","title":"withLoggingEnabled <pre><code>StoreBuilder&lt;T&gt; withLoggingEnabled(\n  Map&lt;String, String&gt; config)\n</code></pre>"},{"location":"state/StoreBuilder/#implementations","text":"<ul> <li>AbstractStoreBuilder</li> </ul>","title":"Implementations"},{"location":"state/StoreSupplier/","text":"<p><code>StoreSupplier</code> is an abstraction of suppliers of StateStores.</p>","title":"StoreSupplier"},{"location":"state/StoreSupplier/#contract","text":"","title":"Contract"},{"location":"state/StoreSupplier/#creating-statestore","text":"","title":"Creating StateStore <pre><code>T get()\n</code></pre> <p>Used when:</p> <ul> <li><code>KStreamImplJoin</code> is requested to <code>sharedOuterJoinWindowStoreBuilder</code></li> <li><code>KeyValueStoreBuilder</code> is requested to <code>build</code></li> <li><code>SessionStoreBuilder</code> is requested to <code>build</code></li> <li><code>TimeOrderedWindowStoreBuilder</code> is requested to <code>build</code></li> <li><code>TimestampedKeyValueStoreBuilder</code> is requested to <code>build</code></li> <li><code>TimestampedWindowStoreBuilder</code> is requested to <code>build</code></li> <li><code>WindowStoreBuilder</code> is requested to <code>build</code></li> </ul>"},{"location":"state/StoreSupplier/#metricsscope","text":"","title":"metricsScope <pre><code>String metricsScope()\n</code></pre>"},{"location":"state/StoreSupplier/#name","text":"","title":"name <pre><code>String name()\n</code></pre>"},{"location":"state/StoreSupplier/#implementations","text":"<ul> <li>KeyValueBytesStoreSupplier</li> <li>SessionBytesStoreSupplier</li> <li>WindowBytesStoreSupplier</li> </ul>","title":"Implementations"},{"location":"state/Stores/","text":"<p><code>Stores</code> utility is a factory for creating state stores in Kafka Streams.</p>","title":"Stores"},{"location":"state/Stores/#inmemorywindowstore","text":"","title":"inMemoryWindowStore <pre><code>WindowBytesStoreSupplier inMemoryWindowStore(\n  String name,\n  Duration retentionPeriod,\n  Duration windowSize,\n  boolean retainDuplicates)\n</code></pre> <p><code>inMemoryWindowStore</code>...FIXME</p> <p><code>inMemoryWindowStore</code>\u00a0is used when:</p> <ul> <li><code>KStreamImplJoin</code> is requested to <code>sharedOuterJoinWindowStoreBuilder</code> (for left outer join)</li> </ul>"},{"location":"state/Stores/#keyvaluestorebuilder","text":"","title":"keyValueStoreBuilder <pre><code>StoreBuilder&lt;KeyValueStore&lt;K, V&gt;&gt; keyValueStoreBuilder(\n  KeyValueBytesStoreSupplier supplier,\n  Serde&lt;K&gt; keySerde,\n  Serde&lt;V&gt; valueSerde)\n</code></pre> <p><code>keyValueStoreBuilder</code> creates a KeyValueStoreBuilder (with the given arguments and <code>Time.SYSTEM</code>).</p>"},{"location":"state/Stores/#demo","text":"","title":"Demo <p>This demo uses the Processor API to add a StoreBuilder to a Topology. Once created with the Stores utility, the <code>StoreBuilder</code> is then attached to a Processor using Topology.addStateStore.</p>"},{"location":"state/Stores/#create-topology","text":"<pre><code>import org.apache.kafka.streams.Topology\nval builder = new Topology()\n</code></pre>","title":"Create Topology"},{"location":"state/Stores/#add-processor","text":"<pre><code>val processorName = \"my-custom-processor\"\ntopology.addProcessor(processorName, ...);\n</code></pre>","title":"Add Processor"},{"location":"state/Stores/#create-storebuilder","text":"<pre><code>import org.apache.kafka.streams.state.Stores\nval storeBuilder = Stores.keyValueStoreBuilder(...)\n</code></pre>","title":"Create StoreBuilder"},{"location":"state/Stores/#attach-processor-to-statestorebuilder","text":"<pre><code>builder.addStateStore(storeBuilder, processorName)\n</code></pre>","title":"Attach Processor to StateStore(Builder)"},{"location":"state/StreamThreadStateStoreProvider/","text":"<p><code>StreamThreadStateStoreProvider</code> is...FIXME</p>","title":"StreamThreadStateStoreProvider"},{"location":"state/ThreadCache/","text":"","title":"ThreadCache"},{"location":"state/ThreadCache/#creating-instance","text":"<p><code>ThreadCache</code> takes the following to be created:</p> <ul> <li> <code>LogContext</code> <li>Maximum Cache Size</li> <li> StreamsMetricsImpl  <p><code>ThreadCache</code> is created when:</p> <ul> <li><code>GlobalStreamThread</code> is created</li> <li><code>StandbyTaskCreator</code> is created</li> <li><code>StreamThread</code> utility is used to create a StreamThread (and creates a ActiveTaskCreator)</li> </ul>","title":"Creating Instance"},{"location":"state/ThreadCache/#maximum-cache-size","text":"","title":"Maximum Cache Size <p><code>ThreadCache</code> is given the maximum cache size (in bytes) when created.</p> <p>The cache size is determined using cache.max.bytes.buffering configuration property when <code>KafkaStreams</code> is created and requested to addStreamThread.</p>"},{"location":"state/ThreadCache/#getorcreatecache","text":"","title":"getOrCreateCache <pre><code>NamedCache getOrCreateCache(\n  String name)\n</code></pre> <p><code>getOrCreateCache</code>...FIXME</p> <p><code>getOrCreateCache</code> is used when:</p> <ul> <li><code>ThreadCache</code> is requested to addDirtyEntryFlushListener, put, putIfAbsent, maybeEvict</li> </ul>"},{"location":"state/ThreadCache/#put","text":"","title":"put <pre><code>void put(\n  String namespace,\n  Bytes key,\n  LRUCacheEntry value)\n</code></pre> <p><code>put</code>...FIXME</p> <p><code>put</code> is used when:</p> <ul> <li><code>CachingKeyValueStore</code> is requested to <code>putInternal</code> and <code>getInternal</code></li> <li><code>CachingSessionStore</code> is requested to <code>put</code></li> <li><code>CachingWindowStore</code> is requested to <code>put</code></li> </ul>"},{"location":"state/ThreadCache/#flush","text":"","title":"flush <pre><code>void flush(\n  String namespace)\n</code></pre> <p><code>flush</code>...FIXME</p> <p><code>flush</code> is used when:</p> <ul> <li><code>CachingKeyValueStore</code> is requested to <code>flush</code>, <code>flushCache</code>, <code>close</code></li> <li><code>CachingSessionStore</code> is requested to <code>flush</code>, <code>flushCache</code>, <code>close</code></li> <li><code>CachingWindowStore</code> is requested to <code>flush</code>, <code>flushCache</code>, <code>close</code></li> </ul>"},{"location":"state/ThreadCache/#sizebytes","text":"","title":"sizeBytes <pre><code>long sizeBytes()\n</code></pre> <p><code>sizeBytes</code> is the sum of all the sizes of the NamedCaches.</p> <p><code>sizeBytes</code> is used when:</p> <ul> <li><code>ThreadCache</code> is requested to resize and maybeEvict</li> </ul>"},{"location":"state/TimestampedKeyValueStore/","text":"<p><code>TimestampedKeyValueStore</code> is...FIXME</p>","title":"TimestampedKeyValueStore"},{"location":"state/ValueAndTimestamp/","text":"<p><code>ValueAndTimestamp</code> is...FIXME</p>","title":"ValueAndTimestamp"},{"location":"state/WindowBytesStoreSupplier/","text":"<p><code>WindowBytesStoreSupplier</code>\u00a0is an extension of the StoreSupplier abstraction for state store suppliers of WindowStores (<code>WindowStore&lt;Bytes, byte[]&gt;</code>s).</p>","title":"WindowBytesStoreSupplier"},{"location":"state/WindowBytesStoreSupplier/#contract","text":"","title":"Contract"},{"location":"state/WindowBytesStoreSupplier/#retainduplicates","text":"","title":"retainDuplicates <pre><code>boolean retainDuplicates()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#retentionperiod","text":"","title":"retentionPeriod <pre><code>long retentionPeriod()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#segmentintervalms","text":"","title":"segmentIntervalMs <pre><code>long segmentIntervalMs()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#windowsize","text":"","title":"windowSize <pre><code>long windowSize()\n</code></pre>"},{"location":"state/WindowBytesStoreSupplier/#implementations","text":"<ul> <li>RocksDbWindowBytesStoreSupplier</li> <li>InMemoryWindowBytesStoreSupplier</li> </ul>","title":"Implementations"},{"location":"state/WindowStore/","text":"<p><code>WindowStore</code> is...FIXME</p>","title":"WindowStore"},{"location":"state/WindowStoreBuilder/","text":"<p><code>WindowStoreBuilder</code> is...FIXME</p>","title":"WindowStoreBuilder"},{"location":"state/WrappedStateStore/","text":"<p><code>WrappedStateStore</code> is an extension of the StateStore and CachedStateStore abstractions for state stores that hold (wrap) another state store.</p>","title":"WrappedStateStore"},{"location":"state/WrappedStateStore/#implementations","text":"<ul> <li><code>AbstractReadOnlyDecorator</code></li> <li><code>AbstractReadWriteDecorator</code></li> <li><code>CachingKeyValueStore</code></li> <li><code>CachingSessionStore</code></li> <li><code>CachingWindowStore</code></li> <li><code>ChangeLoggingKeyValueBytesStore</code></li> <li><code>ChangeLoggingSessionBytesStore</code></li> <li><code>ChangeLoggingWindowBytesStore</code></li> <li><code>MeteredKeyValueStore</code></li> <li><code>MeteredSessionStore</code></li> <li><code>MeteredWindowStore</code></li> <li><code>RocksDBSessionStore</code></li> <li><code>RocksDBTimeOrderedWindowStore</code></li> <li><code>RocksDBWindowStore</code></li> </ul>","title":"Implementations"},{"location":"state/WrappedStateStore/#creating-instance","text":"<p><code>WrappedStateStore</code> takes the following to be created:</p> <ul> <li> StateStore   Abstract Class <p><code>WrappedStateStore</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete WrappedStateStores.</p>","title":"Creating Instance"},{"location":"state/WrappedStateStore/#setflushlistener","text":"","title":"setFlushListener <pre><code>boolean setFlushListener(\n  CacheFlushListener&lt;K, V&gt; listener,\n  boolean sendOldValues)\n</code></pre> <p><code>setFlushListener</code> returns <code>false</code> for the wrapped state store being of any type but a CachedStateStore.</p> <p>Otherwise, <code>setFlushListener</code> returns the value of requesting the CachedStateStore to setFlushListener.</p> <p><code>setFlushListener</code>\u00a0is part of the CachedStateStore abstraction.</p>"}]}